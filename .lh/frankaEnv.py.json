{
    "sourceFile": "frankaEnv.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 407,
            "patches": [
                {
                    "date": 1691482703701,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1691627801238,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -26,10 +26,10 @@\n         # self.reward_arr = np.zeros(2) #shape=(2,), dtype=np.float64\n         self.reward_arr = [0,0,0]\n         self.cnt = 1\n         self.temp_action = np.zeros(12)\n-        self._max_joint_position = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n-        self._min_joint_position = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n+        self._max_joint_position = [0,0,0,0,0,0,0]\n+        self._min_joint_position = [0,0,0,0,0,0,0]\n         self.timestep = 0\n \n         self._max_joint_position[0] = 0.15\n         self._min_joint_position[0] = -0.15\n@@ -44,24 +44,8 @@\n         self._max_joint_position[5] = 170.0 * DEG2RAD\n         self._min_joint_position[5] = -170.0 * DEG2RAD\n         self._max_joint_position[6] = 90.0 * DEG2RAD\n         self._min_joint_position[6] = -90.0 * DEG2RAD\n-        self._max_joint_position[7] = 45.0 * DEG2RAD\n-        self._min_joint_position[7] = -45.0 * DEG2RAD\n-        self._max_joint_position[8] = 160.0 * DEG2RAD\n-        self._min_joint_position[8] = -50.0 * DEG2RAD\n-        self._max_joint_position[9] = 30.0 * DEG2RAD\n-        self._min_joint_position[9] = -150.0 * DEG2RAD\n-        self._max_joint_position[10] = 150.0 * DEG2RAD # 몸 바깥쪽 회전\n-        self._min_joint_position[10] = -150.0 * DEG2RAD # 몸 안쪽 회전\n-        self._max_joint_position[11] = 30.0 * DEG2RAD\n-        self._min_joint_position[11] = -160.0 * DEG2RAD\n-        self._max_joint_position[12] = 170.0 * DEG2RAD\n-        self._min_joint_position[12] = -170.0 * DEG2RAD\n-        self._max_joint_position[13] = 90.0 * DEG2RAD\n-        self._min_joint_position[13] = -90.0 * DEG2RAD\n-        self._max_joint_position[14] = 45.0 * DEG2RAD\n-        self._min_joint_position[14] = -45.0 * DEG2RAD\n \n         self._max_joint_position_hat = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n         self._min_joint_position_hat = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n         for i in range(15):\n"
                },
                {
                    "date": 1691627855990,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -45,10 +45,10 @@\n         self._min_joint_position[5] = -170.0 * DEG2RAD\n         self._max_joint_position[6] = 90.0 * DEG2RAD\n         self._min_joint_position[6] = -90.0 * DEG2RAD\n \n-        self._max_joint_position_hat = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n-        self._min_joint_position_hat = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n+        self._max_joint_position_hat = [0,0,0,0,0,0,0]\n+        self._min_joint_position_hat = [0,0,0,0,0,0,0]\n         for i in range(15):\n             self._max_joint_position_hat[i] = self._max_joint_position[i] - 0.1*(self._max_joint_position[i] - self._min_joint_position[i])\n             self._min_joint_position_hat[i] = self._min_joint_position[i] + 0.1*(self._max_joint_position[i] - self._min_joint_position[i])    \n         \n"
                },
                {
                    "date": 1691627861745,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -47,9 +47,9 @@\n         self._min_joint_position[6] = -90.0 * DEG2RAD\n \n         self._max_joint_position_hat = [0,0,0,0,0,0,0]\n         self._min_joint_position_hat = [0,0,0,0,0,0,0]\n-        for i in range(15):\n+        for i in range(7):\n             self._max_joint_position_hat[i] = self._max_joint_position[i] - 0.1*(self._max_joint_position[i] - self._min_joint_position[i])\n             self._min_joint_position_hat[i] = self._min_joint_position[i] + 0.1*(self._max_joint_position[i] - self._min_joint_position[i])    \n         \n         self.observation_space = self._construct_observation_space()\n"
                },
                {
                    "date": 1691627874773,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -21,9 +21,9 @@\n         self.data = self.sim.data\n         self.viewer = None\n         self._viewers = {}\n         self.controller = controller.CController()\n-        self.torque = np.ones(15)\n+        self.torque = np.ones(7)\n         # self.reward_arr = np.zeros(2) #shape=(2,), dtype=np.float64\n         self.reward_arr = [0,0,0]\n         self.cnt = 1\n         self.temp_action = np.zeros(12)\n"
                },
                {
                    "date": 1691627912740,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -25,9 +25,9 @@\n         self.torque = np.ones(7)\n         # self.reward_arr = np.zeros(2) #shape=(2,), dtype=np.float64\n         self.reward_arr = [0,0,0]\n         self.cnt = 1\n-        self.temp_action = np.zeros(12)\n+        self.temp_action = np.zeros(20)\n         self._max_joint_position = [0,0,0,0,0,0,0]\n         self._min_joint_position = [0,0,0,0,0,0,0]\n         self.timestep = 0\n \n"
                },
                {
                    "date": 1691627948874,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -69,10 +69,11 @@\n         sum_reward = 0\n         done = False\n         motion_cnt = self.controller.cnt()\n         # print(action)\n-        self.controller.read(self.sim_state.time, self.sim_state.qpos, self.sim_state.qvel)\n-        self.temp_action = action\n+        # self.temp_action = action\n+        self.controller.read(self.sim_state.time, self.sim_state.qpos, self.sim_state.qvel, action)\n+        \n         # self.controller.slack(action)\n         self.controller.slack(action)\n         # print(self.temp_action)\n         # print(self.sim_state.time)\n"
                },
                {
                    "date": 1691627956478,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -72,13 +72,8 @@\n         # print(action)\n         # self.temp_action = action\n         self.controller.read(self.sim_state.time, self.sim_state.qpos, self.sim_state.qvel, action)\n         \n-        # self.controller.slack(action)\n-        self.controller.slack(action)\n-        # print(self.temp_action)\n-        # print(self.sim_state.time)\n-        \n         self.controller.control_mujoco()\n         self.torque = self.controller.write()\n         self.do_simulation(self.torque, self.frame_skip)\n         obs =self.get_observation()\n"
                },
                {
                    "date": 1691627995595,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,10 +1,11 @@\n from tokenize import Double\n import gym\n import numpy as np\n \n-from mujoco_py import load_model_from_path, MjSim, MjViewer\n-import mujoco_py\n+# from mujoco_py import load_model_from_path, MjSim, MjViewer\n+# import mujoco_py\n+import mujoco\n \n from gym import ObservationWrapper, spaces\n import controller\n \n@@ -67,9 +68,9 @@\n         \n         # print('step')\n         sum_reward = 0\n         done = False\n-        motion_cnt = self.controller.cnt()\n+        # motion_cnt = self.controller.cnt()\n         # print(action)\n         # self.temp_action = action\n         self.controller.read(self.sim_state.time, self.sim_state.qpos, self.sim_state.qvel, action)\n         \n"
                },
                {
                    "date": 1691628008542,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -13,11 +13,12 @@\n DEG2RAD = 3.14 / 180.0\n \n class FrankaEnv(gym.Env):\n     def __init__(self):\n+        # self.frame_skip = 1\n         self.model_path = 'model/fr3.xml'\n-        self.frame_skip = 1\n-        self.model = load_model_from_path(self.model_path)\n+        \n+        self.model = mujoco.MjModel.from_xml_path(self.model_path)\n         self.sim = mujoco_py.MjSim(self.model)\n          \n         self.data = self.sim.data\n         self.viewer = None\n"
                },
                {
                    "date": 1691628017419,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -15,11 +15,10 @@\n class FrankaEnv(gym.Env):\n     def __init__(self):\n         # self.frame_skip = 1\n         self.model_path = 'model/fr3.xml'\n-        \n         self.model = mujoco.MjModel.from_xml_path(self.model_path)\n-        self.sim = mujoco_py.MjSim(self.model)\n+        self.data = mujoco.MjData(self.model)\n          \n         self.data = self.sim.data\n         self.viewer = None\n         self._viewers = {}\n"
                },
                {
                    "date": 1691628023431,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -17,10 +17,8 @@\n         # self.frame_skip = 1\n         self.model_path = 'model/fr3.xml'\n         self.model = mujoco.MjModel.from_xml_path(self.model_path)\n         self.data = mujoco.MjData(self.model)\n-         \n-        self.data = self.sim.data\n         self.viewer = None\n         self._viewers = {}\n         self.controller = controller.CController()\n         self.torque = np.ones(7)\n"
                },
                {
                    "date": 1691628035882,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -6,8 +6,9 @@\n # import mujoco_py\n import mujoco\n \n from gym import ObservationWrapper, spaces\n+from mujoco import viewer\n import controller\n \n DEFAULT_SIZE = 300\n DEG2RAD = 3.14 / 180.0\n@@ -17,8 +18,9 @@\n         # self.frame_skip = 1\n         self.model_path = 'model/fr3.xml'\n         self.model = mujoco.MjModel.from_xml_path(self.model_path)\n         self.data = mujoco.MjData(self.model)\n+\n         self.viewer = None\n         self._viewers = {}\n         self.controller = controller.CController()\n         self.torque = np.ones(7)\n"
                },
                {
                    "date": 1691628044058,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -19,10 +19,9 @@\n         self.model_path = 'model/fr3.xml'\n         self.model = mujoco.MjModel.from_xml_path(self.model_path)\n         self.data = mujoco.MjData(self.model)\n \n-        self.viewer = None\n-        self._viewers = {}\n+        self.viewer = viewer.launch_passive(model=self.model, data=self.data)\n         self.controller = controller.CController()\n         self.torque = np.ones(7)\n         # self.reward_arr = np.zeros(2) #shape=(2,), dtype=np.float64\n         self.reward_arr = [0,0,0]\n"
                },
                {
                    "date": 1691628103787,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -20,8 +20,9 @@\n         self.model = mujoco.MjModel.from_xml_path(self.model_path)\n         self.data = mujoco.MjData(self.model)\n \n         self.viewer = viewer.launch_passive(model=self.model, data=self.data)\n+\n         self.controller = controller.CController()\n         self.torque = np.ones(7)\n         # self.reward_arr = np.zeros(2) #shape=(2,), dtype=np.float64\n         self.reward_arr = [0,0,0]\n@@ -74,9 +75,11 @@\n         self.controller.read(self.sim_state.time, self.sim_state.qpos, self.sim_state.qvel, action)\n         \n         self.controller.control_mujoco()\n         self.torque = self.controller.write()\n-        self.do_simulation(self.torque, self.frame_skip)\n+        for i in range(self.k):\n+                self.data.ctrl[i] = self._torque[i]\n+        # self.do_simulation(self.torque, self.frame_skip)\n         obs =self.get_observation()\n         # print(obs)\n         sum_reward = self.reward()\n         if sum_reward == 21498098 :\n"
                },
                {
                    "date": 1691628120382,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -15,8 +15,9 @@\n \n class FrankaEnv(gym.Env):\n     def __init__(self):\n         # self.frame_skip = 1\n+        self.k = 7  # for jacobian calculation\n         self.model_path = 'model/fr3.xml'\n         self.model = mujoco.MjModel.from_xml_path(self.model_path)\n         self.data = mujoco.MjData(self.model)\n \n@@ -76,9 +77,9 @@\n         \n         self.controller.control_mujoco()\n         self.torque = self.controller.write()\n         for i in range(self.k):\n-                self.data.ctrl[i] = self._torque[i]\n+                self.data.ctrl[i] = self.torque[i]\n         # self.do_simulation(self.torque, self.frame_skip)\n         obs =self.get_observation()\n         # print(obs)\n         sum_reward = self.reward()\n"
                },
                {
                    "date": 1691628136717,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -23,9 +23,9 @@\n \n         self.viewer = viewer.launch_passive(model=self.model, data=self.data)\n \n         self.controller = controller.CController()\n-        self.torque = np.ones(7)\n+        self.torque = np.ones(self.k)\n         # self.reward_arr = np.zeros(2) #shape=(2,), dtype=np.float64\n         self.reward_arr = [0,0,0]\n         self.cnt = 1\n         self.temp_action = np.zeros(20)\n@@ -49,9 +49,9 @@\n         self._min_joint_position[6] = -90.0 * DEG2RAD\n \n         self._max_joint_position_hat = [0,0,0,0,0,0,0]\n         self._min_joint_position_hat = [0,0,0,0,0,0,0]\n-        for i in range(7):\n+        for i in range(self.k):\n             self._max_joint_position_hat[i] = self._max_joint_position[i] - 0.1*(self._max_joint_position[i] - self._min_joint_position[i])\n             self._min_joint_position_hat[i] = self._min_joint_position[i] + 0.1*(self._max_joint_position[i] - self._min_joint_position[i])    \n         \n         self.observation_space = self._construct_observation_space()\n"
                },
                {
                    "date": 1691628161942,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -23,15 +23,15 @@\n \n         self.viewer = viewer.launch_passive(model=self.model, data=self.data)\n \n         self.controller = controller.CController()\n-        self.torque = np.ones(self.k)\n+        self.torque = np.zeros(self.k)\n         # self.reward_arr = np.zeros(2) #shape=(2,), dtype=np.float64\n         self.reward_arr = [0,0,0]\n         self.cnt = 1\n         self.temp_action = np.zeros(20)\n-        self._max_joint_position = [0,0,0,0,0,0,0]\n-        self._min_joint_position = [0,0,0,0,0,0,0]\n+        self._max_joint_position = np.zeros(self.k)\n+        self._min_joint_position = np.zeros(self.k)\n         self.timestep = 0\n \n         self._max_joint_position[0] = 0.15\n         self._min_joint_position[0] = -0.15\n@@ -49,8 +49,9 @@\n         self._min_joint_position[6] = -90.0 * DEG2RAD\n \n         self._max_joint_position_hat = [0,0,0,0,0,0,0]\n         self._min_joint_position_hat = [0,0,0,0,0,0,0]\n+\n         for i in range(self.k):\n             self._max_joint_position_hat[i] = self._max_joint_position[i] - 0.1*(self._max_joint_position[i] - self._min_joint_position[i])\n             self._min_joint_position_hat[i] = self._min_joint_position[i] + 0.1*(self._max_joint_position[i] - self._min_joint_position[i])    \n         \n"
                },
                {
                    "date": 1691628171126,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -47,10 +47,10 @@\n         self._min_joint_position[5] = -170.0 * DEG2RAD\n         self._max_joint_position[6] = 90.0 * DEG2RAD\n         self._min_joint_position[6] = -90.0 * DEG2RAD\n \n-        self._max_joint_position_hat = [0,0,0,0,0,0,0]\n-        self._min_joint_position_hat = [0,0,0,0,0,0,0]\n+        self._max_joint_position_hat = np.zeros(self.k)\n+        self._min_joint_position_hat = np.zeros(self.k)\n \n         for i in range(self.k):\n             self._max_joint_position_hat[i] = self._max_joint_position[i] - 0.1*(self._max_joint_position[i] - self._min_joint_position[i])\n             self._min_joint_position_hat[i] = self._min_joint_position[i] + 0.1*(self._max_joint_position[i] - self._min_joint_position[i])    \n"
                },
                {
                    "date": 1691628210554,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -73,9 +73,9 @@\n         done = False\n         # motion_cnt = self.controller.cnt()\n         # print(action)\n         # self.temp_action = action\n-        self.controller.read(self.sim_state.time, self.sim_state.qpos, self.sim_state.qvel, action)\n+        self.controller.read(self.sim_state.time, self.sim_state.qpos[0:self.k], self.sim_state.qvel[0:self.k], action)\n         \n         self.controller.control_mujoco()\n         self.torque = self.controller.write()\n         for i in range(self.k):\n"
                },
                {
                    "date": 1691628220805,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -77,19 +77,22 @@\n         self.controller.read(self.sim_state.time, self.sim_state.qpos[0:self.k], self.sim_state.qvel[0:self.k], action)\n         \n         self.controller.control_mujoco()\n         self.torque = self.controller.write()\n+\n         for i in range(self.k):\n                 self.data.ctrl[i] = self.torque[i]\n         # self.do_simulation(self.torque, self.frame_skip)\n+\n         obs =self.get_observation()\n         # print(obs)\n         sum_reward = self.reward()\n-        if sum_reward == 21498098 :\n-            done = True\n-            self.controller.init_controller()\n-            # print(action)\n-            return obs, 0, done, dict()\n+\n+        # if sum_reward == 21498098 :\n+        #     done = True\n+        #     self.controller.init_controller()\n+        #     # print(action)\n+        #     return obs, 0, done, dict()\n         # print(\"sum_reward : \")\n         # print(sum_reward)\n \n         if render:\n"
                },
                {
                    "date": 1691628234001,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -97,41 +97,28 @@\n \n         if render:\n             self.render()\n \n-        for i in range(15):\n-            # print(i)\n-            # print(\":\")\n-            # print(self.sim_state.qpos[i])\n-            # print(self._min_joint_position[i])\n-            if self.sim_state.qpos[i] >=  self._max_joint_position[i] :\n-                done = True\n-                sum_reward = sum_reward - 10000\n-                self.controller.init_controller()\n-                # print('joint{%.1i} > max val'% (i))\n-                # print(action)\n-                return obs, sum_reward, done, dict()\n+        # for i in range(self.k):\n+        #     # print(i)\n+        #     # print(\":\")\n+        #     # print(self.sim_state.qpos[i])\n+        #     # print(self._min_joint_position[i])\n+        #     if self.sim_state.qpos[i] >=  self._max_joint_position[i] :\n+        #         done = True\n+        #         sum_reward = sum_reward - 10000\n+        #         self.controller.init_controller()\n+        #         # print('joint{%.1i} > max val'% (i))\n+        #         # print(action)\n+        #         return obs, sum_reward, done, dict()\n             \n-            elif self.sim_state.qpos[i] <= self._min_joint_position[i] :\n-                done = True\n-                sum_reward = sum_reward - 10000\n-                self.controller.init_controller()\n-                # print('joint{%.1i} < min val'% (i))\n-                # print(action)\n-                return obs, sum_reward, done, dict()\n-\n-        # if abs(self.sim_state.qpos[0]) > 0.1 :\n-        #     done = True\n-        #     sum_reward = sum_reward -100000\n-        #     self.controller.init_controller()\n-        #     return obs, sum_reward, done, dict()\n-        \n-        # if motion_cnt[0] > 6:\n-        #     done = True\n-        #     self.controller.init_controller()\n-        #     return obs, sum_reward, done, dict()\n-\n-\n+        #     elif self.sim_state.qpos[i] <= self._min_joint_position[i] :\n+        #         done = True\n+        #         sum_reward = sum_reward - 10000\n+        #         self.controller.init_controller()\n+        #         # print('joint{%.1i} < min val'% (i))\n+        #         # print(action)\n+        #         return obs, sum_reward, done, dict()\n         return obs, sum_reward, done, dict()\n     \n         \n     def _construct_action_space(self):\n"
                },
                {
                    "date": 1691628244432,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -122,10 +122,10 @@\n     \n         \n     def _construct_action_space(self):\n         # action_low = -1 * np.ones(12)\n-        action_low =  np.zeros(15)\n-        action_high = np.ones(15) \n+        action_low =  np.zeros(20)\n+        action_high = np.ones(20) \n         return gym.spaces.Box(low=action_low, high=action_high, dtype=np.float64)\n     \n     \n     def _construct_observation_space(self):\n"
                },
                {
                    "date": 1691628281697,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -128,10 +128,10 @@\n         return gym.spaces.Box(low=action_low, high=action_high, dtype=np.float64)\n     \n     \n     def _construct_observation_space(self):\n-        obs_low = -1 * np.ones(39)\n-        obs_high = 1 * np.ones(39)\n+        obs_low = -1 * np.ones(24)\n+        obs_high = 1 * np.ones(24)\n         # obs_low = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0,0,0, -1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1]\n         # obs_high = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0,0,0, 1,1,1,1,1,1,1,1,1,1,1,1]\n         return gym.spaces.Box(obs_low, obs_high)\n     \n"
                },
                {
                    "date": 1691628307165,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -128,9 +128,9 @@\n         return gym.spaces.Box(low=action_low, high=action_high, dtype=np.float64)\n     \n     \n     def _construct_observation_space(self):\n-        obs_low = -1 * np.ones(24)\n+        obs_low = -1 * np.ones(24) # X , Xdot , X_des, Xdot_des\n         obs_high = 1 * np.ones(24)\n         # obs_low = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0,0,0, -1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1]\n         # obs_high = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0,0,0, 1,1,1,1,1,1,1,1,1,1,1,1]\n         return gym.spaces.Box(obs_low, obs_high)\n"
                },
                {
                    "date": 1691628333486,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -139,9 +139,10 @@\n     \n     def get_observation(self):\n         self.sim_state = self.sim.get_state()\n         \n-        joint_position = self.sim_state.qpos\n+        joint_position = self.sim_state.qpos[0:self.k]\n+        joint_velocity = self.sim_state.qvel[0:self.k]\n         \n         # joint_velocity = self.sim_state.qvel\n         xr_des = self.controller.xr_des()\n         # print(xr_des)\n"
                },
                {
                    "date": 1691628347653,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -139,10 +139,10 @@\n     \n     def get_observation(self):\n         self.sim_state = self.sim.get_state()\n         \n-        joint_position = self.sim_state.qpos[0:self.k]\n-        joint_velocity = self.sim_state.qvel[0:self.k]\n+        # joint_position = self.sim_state.qpos[0:self.k]\n+        # joint_velocity = self.sim_state.qvel[0:self.k]\n         \n         # joint_velocity = self.sim_state.qvel\n         xr_des = self.controller.xr_des()\n         # print(xr_des)\n"
                },
                {
                    "date": 1691628421219,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -138,12 +138,13 @@\n      \n     \n     def get_observation(self):\n         self.sim_state = self.sim.get_state()\n+        position_goal = # controller로부터 받은 X\n+        xdot_goal = # controller로부터 받은 X\n+        position = # controller로부터 받은 X\n+        velocity = # controller로부터 받은 Xdot\n         \n-        # joint_position = self.sim_state.qpos[0:self.k]\n-        # joint_velocity = self.sim_state.qvel[0:self.k]\n-        \n         # joint_velocity = self.sim_state.qvel\n         xr_des = self.controller.xr_des()\n         # print(xr_des)\n         # joint_velocity = joint_velocity / 15.0\n"
                },
                {
                    "date": 1691628430619,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -139,11 +139,11 @@\n     \n     def get_observation(self):\n         self.sim_state = self.sim.get_state()\n         position_goal = # controller로부터 받은 X\n-        xdot_goal = # controller로부터 받은 X\n+        # xdot_goal = # controller로부터 받은 X\n         position = # controller로부터 받은 X\n-        velocity = # controller로부터 받은 Xdot\n+        # velocity = # controller로부터 받은 Xdot\n         \n         # joint_velocity = self.sim_state.qvel\n         xr_des = self.controller.xr_des()\n         # print(xr_des)\n"
                },
                {
                    "date": 1691628445630,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -128,10 +128,10 @@\n         return gym.spaces.Box(low=action_low, high=action_high, dtype=np.float64)\n     \n     \n     def _construct_observation_space(self):\n-        obs_low = -1 * np.ones(24) # X , Xdot , X_des, Xdot_des\n-        obs_high = 1 * np.ones(24)\n+        obs_low = -1 * np.ones(12) # X , X_goal\n+        obs_high = 1 * np.ones(12)\n         # obs_low = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0,0,0, -1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1]\n         # obs_high = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0,0,0, 1,1,1,1,1,1,1,1,1,1,1,1]\n         return gym.spaces.Box(obs_low, obs_high)\n     \n"
                },
                {
                    "date": 1691628451366,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -128,10 +128,10 @@\n         return gym.spaces.Box(low=action_low, high=action_high, dtype=np.float64)\n     \n     \n     def _construct_observation_space(self):\n-        obs_low = -1 * np.ones(12) # X , X_goal\n-        obs_high = 1 * np.ones(12)\n+        obs_low = -1 * np.ones(6) # X , X_goal\n+        obs_high = 1 * np.ones(6)\n         # obs_low = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0,0,0, -1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1]\n         # obs_high = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0,0,0, 1,1,1,1,1,1,1,1,1,1,1,1]\n         return gym.spaces.Box(obs_low, obs_high)\n     \n"
                },
                {
                    "date": 1691628457763,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -128,9 +128,9 @@\n         return gym.spaces.Box(low=action_low, high=action_high, dtype=np.float64)\n     \n     \n     def _construct_observation_space(self):\n-        obs_low = -1 * np.ones(6) # X , X_goal\n+        obs_low = -1 * np.ones(6) # X , Xdot\n         obs_high = 1 * np.ones(6)\n         # obs_low = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0,0,0, -1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1]\n         # obs_high = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0,0,0, 1,1,1,1,1,1,1,1,1,1,1,1]\n         return gym.spaces.Box(obs_low, obs_high)\n"
                },
                {
                    "date": 1691628468393,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -128,9 +128,9 @@\n         return gym.spaces.Box(low=action_low, high=action_high, dtype=np.float64)\n     \n     \n     def _construct_observation_space(self):\n-        obs_low = -1 * np.ones(6) # X , Xdot\n+        obs_low = -1 * np.ones(6) # X\n         obs_high = 1 * np.ones(6)\n         # obs_low = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0,0,0, -1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1]\n         # obs_high = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0,0,0, 1,1,1,1,1,1,1,1,1,1,1,1]\n         return gym.spaces.Box(obs_low, obs_high)\n"
                },
                {
                    "date": 1691628473625,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -138,10 +138,9 @@\n      \n     \n     def get_observation(self):\n         self.sim_state = self.sim.get_state()\n-        position_goal = # controller로부터 받은 X\n-        # xdot_goal = # controller로부터 받은 X\n+        # position_goal = # controller로부터 받은 X\n         position = # controller로부터 받은 X\n         # velocity = # controller로부터 받은 Xdot\n         \n         # joint_velocity = self.sim_state.qvel\n"
                },
                {
                    "date": 1691628528258,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -138,43 +138,12 @@\n      \n     \n     def get_observation(self):\n         self.sim_state = self.sim.get_state()\n-        # position_goal = # controller로부터 받은 X\n+        \n         position = # controller로부터 받은 X\n-        # velocity = # controller로부터 받은 Xdot\n         \n-        # joint_velocity = self.sim_state.qvel\n-        xr_des = self.controller.xr_des()\n-        # print(xr_des)\n-        # joint_velocity = joint_velocity / 15.0\n-        # for i in range(15) :    \n-        #     if joint_velocity[i]>=1 :\n-        #         print(\"joint_velocity[%.1i] >= 1\"%(i))\n-        #         print(joint_velocity[i])\n-        #     elif joint_velocity[i]<= -1 :\n-        #         print(\"joint_velocity[%.1i] <= -1\"%(i))\n-        #         print(joint_velocity[i])\n-        \n \n-        joint_position = self.sim_state.qpos / 3.141592\n-        joint_position[0] = joint_position[0] * 3.141592\n-        xr_des[0] = xr_des[0] / 0.9\n-        xr_des[1] = xr_des[1] / 0.9\n-        xr_des[2] = xr_des[2] / 1.7\n-        \n-        xr_des[3] = xr_des[3] / 0.9\n-        xr_des[4] = xr_des[4] / 0.9\n-        xr_des[5] = xr_des[5] / 1.7\n-        \n-        xr_des[6] = xr_des[6] / 3.141592\n-        xr_des[7] = xr_des[7] / 3.141592\n-        xr_des[8] = xr_des[8] / 3.141592\n-        \n-        xr_des[9] = xr_des[9] / 3.141592\n-        xr_des[10] = xr_des[10] / 3.141592\n-        xr_des[11] = xr_des[11] / 3.141592\n-\n         self.temp_action = self.temp_action * 2\n         self.temp_action = self.temp_action - 1\n         return np.concatenate(\n         (\n"
                },
                {
                    "date": 1691628535674,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -140,11 +140,10 @@\n     def get_observation(self):\n         self.sim_state = self.sim.get_state()\n         \n         position = # controller로부터 받은 X\n-        \n \n-        self.temp_action = self.temp_action * 2\n+        self.temp_action = self.temp_action * 10\n         self.temp_action = self.temp_action - 1\n         return np.concatenate(\n         (\n             joint_position, #15\n"
                },
                {
                    "date": 1691628554223,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -142,12 +142,12 @@\n         \n         position = # controller로부터 받은 X\n \n         self.temp_action = self.temp_action * 10\n-        self.temp_action = self.temp_action - 1\n+        # self.temp_action = self.temp_action - 1\n         return np.concatenate(\n         (\n-            joint_position, #15\n+            position, #6\n             # joint_velocity, #15\n             xr_des, # 12\n             self.temp_action, # 12\n             # self.get_body_com(\"endEffector\"),# 3\n"
                },
                {
                    "date": 1691628562605,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -147,10 +147,10 @@\n         return np.concatenate(\n         (\n             position, #6\n             # joint_velocity, #15\n-            xr_des, # 12\n-            self.temp_action, # 12\n+            # xr_des, # 12\n+            self.temp_action, # 20\n             # self.get_body_com(\"endEffector\"),# 3\n         )\n         )\n         \n"
                },
                {
                    "date": 1691628595934,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -141,8 +141,10 @@\n         self.sim_state = self.sim.get_state()\n         \n         position = # controller로부터 받은 X\n \n+        goal = # controller로부터 받은 goal\n+\n         self.temp_action = self.temp_action * 10\n         # self.temp_action = self.temp_action - 1\n         return np.concatenate(\n         (\n"
                },
                {
                    "date": 1691628601046,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -148,8 +148,9 @@\n         # self.temp_action = self.temp_action - 1\n         return np.concatenate(\n         (\n             position, #6\n+            goal, #6\n             # joint_velocity, #15\n             # xr_des, # 12\n             self.temp_action, # 20\n             # self.get_body_com(\"endEffector\"),# 3\n"
                },
                {
                    "date": 1691628645173,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -128,10 +128,10 @@\n         return gym.spaces.Box(low=action_low, high=action_high, dtype=np.float64)\n     \n     \n     def _construct_observation_space(self):\n-        obs_low = -1 * np.ones(6) # X\n-        obs_high = 1 * np.ones(6)\n+        obs_low = -1 * np.ones(32) # X\n+        obs_high = 1 * np.ones(32)\n         # obs_low = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0,0,0, -1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1]\n         # obs_high = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0,0,0, 1,1,1,1,1,1,1,1,1,1,1,1]\n         return gym.spaces.Box(obs_low, obs_high)\n     \n@@ -149,12 +149,9 @@\n         return np.concatenate(\n         (\n             position, #6\n             goal, #6\n-            # joint_velocity, #15\n-            # xr_des, # 12\n             self.temp_action, # 20\n-            # self.get_body_com(\"endEffector\"),# 3\n         )\n         )\n         \n     def reset(self):\n"
                },
                {
                    "date": 1691628663395,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -128,9 +128,9 @@\n         return gym.spaces.Box(low=action_low, high=action_high, dtype=np.float64)\n     \n     \n     def _construct_observation_space(self):\n-        obs_low = -1 * np.ones(32) # X\n+        obs_low = -1 * np.ones(32) # X(6), X_goal(6), action(20) \n         obs_high = 1 * np.ones(32)\n         # obs_low = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0,0,0, -1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1]\n         # obs_high = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0,0,0, 1,1,1,1,1,1,1,1,1,1,1,1]\n         return gym.spaces.Box(obs_low, obs_high)\n"
                },
                {
                    "date": 1691628673073,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -160,9 +160,8 @@\n         self.set_state(qpos, qvel)\n         self.timestep = 0\n         self.temp_action = np.zeros(12)\n         \n-        \n         return self.get_observation()\n     \n     \n     \n"
                },
                {
                    "date": 1691628689676,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -74,9 +74,8 @@\n         # motion_cnt = self.controller.cnt()\n         # print(action)\n         # self.temp_action = action\n         self.controller.read(self.sim_state.time, self.sim_state.qpos[0:self.k], self.sim_state.qvel[0:self.k], action)\n-        \n         self.controller.control_mujoco()\n         self.torque = self.controller.write()\n \n         for i in range(self.k):\n@@ -140,11 +139,9 @@\n     def get_observation(self):\n         self.sim_state = self.sim.get_state()\n         \n         position = # controller로부터 받은 X\n-\n         goal = # controller로부터 받은 goal\n-\n         self.temp_action = self.temp_action * 10\n         # self.temp_action = self.temp_action - 1\n         return np.concatenate(\n         (\n"
                },
                {
                    "date": 1691628694964,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -70,11 +70,9 @@\n         \n         # print('step')\n         sum_reward = 0\n         done = False\n-        # motion_cnt = self.controller.cnt()\n-        # print(action)\n-        # self.temp_action = action\n+        \n         self.controller.read(self.sim_state.time, self.sim_state.qpos[0:self.k], self.sim_state.qvel[0:self.k], action)\n         self.controller.control_mujoco()\n         self.torque = self.controller.write()\n \n"
                },
                {
                    "date": 1691628729939,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -70,19 +70,16 @@\n         \n         # print('step')\n         sum_reward = 0\n         done = False\n-        \n+\n         self.controller.read(self.sim_state.time, self.sim_state.qpos[0:self.k], self.sim_state.qvel[0:self.k], action)\n         self.controller.control_mujoco()\n         self.torque = self.controller.write()\n \n         for i in range(self.k):\n                 self.data.ctrl[i] = self.torque[i]\n-        # self.do_simulation(self.torque, self.frame_skip)\n-\n         obs =self.get_observation()\n-        # print(obs)\n         sum_reward = self.reward()\n \n         # if sum_reward == 21498098 :\n         #     done = True\n"
                },
                {
                    "date": 1691628760788,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -148,8 +148,9 @@\n         \n     def reset(self):\n         qpos = np.zeros(15)\n         qvel = np.zeros(15)\n+        # self.sim_state.qpos[0:self.k], self.sim_state.qvel[0:self.k]\n         self.set_state(qpos, qvel)\n         self.timestep = 0\n         self.temp_action = np.zeros(12)\n         \n"
                },
                {
                    "date": 1691628776630,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -146,10 +146,10 @@\n         )\n         )\n         \n     def reset(self):\n-        qpos = np.zeros(15)\n-        qvel = np.zeros(15)\n+        qpos = np.zeros(self.k)\n+        qvel = np.zeros(self.k)\n         # self.sim_state.qpos[0:self.k], self.sim_state.qvel[0:self.k]\n         self.set_state(qpos, qvel)\n         self.timestep = 0\n         self.temp_action = np.zeros(12)\n"
                },
                {
                    "date": 1691628819528,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -151,17 +151,17 @@\n         qvel = np.zeros(self.k)\n         # self.sim_state.qpos[0:self.k], self.sim_state.qvel[0:self.k]\n         self.set_state(qpos, qvel)\n         self.timestep = 0\n-        self.temp_action = np.zeros(12)\n+        self.temp_action = np.zeros(20)\n         \n         return self.get_observation()\n     \n     \n     \n     def set_state(self, qpos, qvel):\n         old_state = self.sim.get_state()\n-        new_state = mujoco_py.MjSimState(\n+        new_state = mujoco.MjSimState(\n             old_state.time, qpos, qvel, old_state.act, old_state.udd_state\n         )\n         self.sim.set_state(new_state)\n         self.sim.forward()\n"
                },
                {
                    "date": 1691628847322,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -169,13 +169,13 @@\n     @property\n     def dt(self):\n         return self.model.opt.timestep * self.frame_skip\n \n-    def do_simulation(self, ctrl, n_frames):\n-        self.sim.data.ctrl[:] = ctrl\n-        self.sim.step()\n-        # for _ in range(n_frames):\n-        #     self.sim.step()\n+    # def do_simulation(self, ctrl, n_frames):\n+    #     self.sim.data.ctrl[:] = ctrl\n+    #     self.sim.step()\n+    #     # for _ in range(n_frames):\n+    #     #     self.sim.step()\n     \n     \n     def render(\n         self,\n"
                },
                {
                    "date": 1691628869705,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -242,11 +242,11 @@\n     def _get_viewer(self, mode):\n         self.viewer = self._viewers.get(mode)\n         if self.viewer is None:\n             if mode == \"human\":\n-                self.viewer = mujoco_py.MjViewer(self.sim)\n+                self.viewer = mujoco.MjViewer(self.sim)\n             elif mode == \"rgb_array\" or mode == \"depth_array\":\n-                self.viewer = mujoco_py.MjRenderContextOffscreen(self.sim, -1)\n+                self.viewer = mujoco.MjRenderContextOffscreen(self.sim, -1)\n \n             self.viewer_setup()\n             self._viewers[mode] = self.viewer\n         return self.viewer\n"
                },
                {
                    "date": 1691628886156,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -281,26 +281,14 @@\n                     reward2 = reward2 - abs(self._max_joint_position_hat[i] - self.sim_state.qpos[i]) * alpha2\n                 else :\n                     reward2 = reward2 - abs(self._max_joint_position_hat[i] - self.sim_state.qpos[i])\n         reward2 = reward2 * alpha2\n-        #Avoiding Cartesian Local Minima (Cost Function)\n-        # reward3 = self.reward_arr[2]\n-        # if reward3 != 0 :\n-        #     reward3 = np.sqrt(reward3)            \n-        # alpha3 =  -1.0\n-        # if reward3 < 0.01 :\n-        #     reward3 = 1 - reward3\n-        #     reward3 = reward3 * alpha3 \n-            # reward3 = reward3 * 5.0\n-        # else :\n-        #     reward3 = 0.0\n+\n         # print(\"reward1 : %.3f, reward2 : %.3f, reward3 : %.3f\"%(reward1,reward2,reward3))\n         # print(reward1)\n         # print(\"reward2 : %.3f\"%reward2)\n         # print(reward2)\n         # print(\"reward3 : %.3f\"%reward3)\n         # print(reward3)\n         # reward = reward1 + reward2 + reward3 + 2\n         reward = reward1 + reward2 + 1\n-        # print(\"reward : %.3f\"%reward)\n-        # print(reward)\n         return reward\n\\ No newline at end of file\n"
                },
                {
                    "date": 1691629430358,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -73,8 +73,9 @@\n         done = False\n \n         self.controller.read(self.sim_state.time, self.sim_state.qpos[0:self.k], self.sim_state.qvel[0:self.k], action)\n         self.controller.control_mujoco()\n+        self.controller.state_controller()\n         self.torque = self.controller.write()\n \n         for i in range(self.k):\n                 self.data.ctrl[i] = self.torque[i]\n"
                },
                {
                    "date": 1691629561371,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -24,8 +24,9 @@\n         self.viewer = viewer.launch_passive(model=self.model, data=self.data)\n \n         self.controller = controller.CController()\n         self.torque = np.zeros(self.k)\n+        self.tmp_X = np.zeros(12)\n         # self.reward_arr = np.zeros(2) #shape=(2,), dtype=np.float64\n         self.reward_arr = [0,0,0]\n         self.cnt = 1\n         self.temp_action = np.zeros(20)\n@@ -73,9 +74,9 @@\n         done = False\n \n         self.controller.read(self.sim_state.time, self.sim_state.qpos[0:self.k], self.sim_state.qvel[0:self.k], action)\n         self.controller.control_mujoco()\n-        self.controller.state_controller()\n+        self.self.controller.state_controller()\n         self.torque = self.controller.write()\n \n         for i in range(self.k):\n                 self.data.ctrl[i] = self.torque[i]\n"
                },
                {
                    "date": 1691629576471,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -25,8 +25,11 @@\n \n         self.controller = controller.CController()\n         self.torque = np.zeros(self.k)\n         self.tmp_X = np.zeros(12)\n+        self.X_goal = np.zeros(12)\n+        self.X = np.zeros(12)\n+\n         # self.reward_arr = np.zeros(2) #shape=(2,), dtype=np.float64\n         self.reward_arr = [0,0,0]\n         self.cnt = 1\n         self.temp_action = np.zeros(20)\n"
                },
                {
                    "date": 1691629587929,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -25,10 +25,10 @@\n \n         self.controller = controller.CController()\n         self.torque = np.zeros(self.k)\n         self.tmp_X = np.zeros(12)\n-        self.X_goal = np.zeros(12)\n-        self.X = np.zeros(12)\n+        self.X_goal = np.zeros(6)\n+        self.X = np.zeros(6)\n \n         # self.reward_arr = np.zeros(2) #shape=(2,), dtype=np.float64\n         self.reward_arr = [0,0,0]\n         self.cnt = 1\n@@ -77,9 +77,9 @@\n         done = False\n \n         self.controller.read(self.sim_state.time, self.sim_state.qpos[0:self.k], self.sim_state.qvel[0:self.k], action)\n         self.controller.control_mujoco()\n-        self.self.controller.state_controller()\n+        self.tmpX = self.controller.state_controller()\n         self.torque = self.controller.write()\n \n         for i in range(self.k):\n                 self.data.ctrl[i] = self.torque[i]\n"
                },
                {
                    "date": 1691629628038,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -77,9 +77,13 @@\n         done = False\n \n         self.controller.read(self.sim_state.time, self.sim_state.qpos[0:self.k], self.sim_state.qvel[0:self.k], action)\n         self.controller.control_mujoco()\n-        self.tmpX = self.controller.state_controller()\n+        self.tmp_X = self.controller.state_controller()\n+        self.X = self.tmp_X\n+        self.X_goal = self.tmp_X\n+        for i in range(6):\n+            self.X[i] = self.tmp_X[i]\n         self.torque = self.controller.write()\n \n         for i in range(self.k):\n                 self.data.ctrl[i] = self.torque[i]\n"
                },
                {
                    "date": 1691629641385,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -81,9 +81,11 @@\n         self.tmp_X = self.controller.state_controller()\n         self.X = self.tmp_X\n         self.X_goal = self.tmp_X\n         for i in range(6):\n-            self.X[i] = self.tmp_X[i]\n+            self.X_goal[i] = self.tmp_X[i]\n+        for i in range(6):\n+            self.X[i] = self.tmp_X[i+6]\n         self.torque = self.controller.write()\n \n         for i in range(self.k):\n                 self.data.ctrl[i] = self.torque[i]\n"
                },
                {
                    "date": 1691629648358,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -78,14 +78,14 @@\n \n         self.controller.read(self.sim_state.time, self.sim_state.qpos[0:self.k], self.sim_state.qvel[0:self.k], action)\n         self.controller.control_mujoco()\n         self.tmp_X = self.controller.state_controller()\n-        self.X = self.tmp_X\n-        self.X_goal = self.tmp_X\n+        \n         for i in range(6):\n             self.X_goal[i] = self.tmp_X[i]\n         for i in range(6):\n             self.X[i] = self.tmp_X[i+6]\n+\n         self.torque = self.controller.write()\n \n         for i in range(self.k):\n                 self.data.ctrl[i] = self.torque[i]\n"
                },
                {
                    "date": 1691629666796,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -78,18 +78,17 @@\n \n         self.controller.read(self.sim_state.time, self.sim_state.qpos[0:self.k], self.sim_state.qvel[0:self.k], action)\n         self.controller.control_mujoco()\n         self.tmp_X = self.controller.state_controller()\n-        \n+        self.torque = self.controller.write()\n+        for i in range(self.k):\n+                self.data.ctrl[i] = self.torque[i]\n+\n         for i in range(6):\n             self.X_goal[i] = self.tmp_X[i]\n         for i in range(6):\n             self.X[i] = self.tmp_X[i+6]\n \n-        self.torque = self.controller.write()\n-\n-        for i in range(self.k):\n-                self.data.ctrl[i] = self.torque[i]\n         obs =self.get_observation()\n         sum_reward = self.reward()\n \n         # if sum_reward == 21498098 :\n"
                },
                {
                    "date": 1691629683605,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -143,10 +143,10 @@\n     \n     def get_observation(self):\n         self.sim_state = self.sim.get_state()\n         \n-        position = # controller로부터 받은 X\n-        goal = # controller로부터 받은 goal\n+        # position = self.# controller로부터 받은 X\n+        # goal = # controller로부터 받은 goal\n         self.temp_action = self.temp_action * 10\n         # self.temp_action = self.temp_action - 1\n         return np.concatenate(\n         (\n"
                },
                {
                    "date": 1691629692752,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -149,10 +149,10 @@\n         self.temp_action = self.temp_action * 10\n         # self.temp_action = self.temp_action - 1\n         return np.concatenate(\n         (\n-            position, #6\n-            goal, #6\n+            self.X, #6\n+            self.X_goal, #6\n             self.temp_action, # 20\n         )\n         )\n         \n"
                },
                {
                    "date": 1691630342074,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -89,17 +89,9 @@\n             self.X[i] = self.tmp_X[i+6]\n \n         obs =self.get_observation()\n         sum_reward = self.reward()\n-\n-        # if sum_reward == 21498098 :\n-        #     done = True\n-        #     self.controller.init_controller()\n-        #     # print(action)\n-        #     return obs, 0, done, dict()\n-        # print(\"sum_reward : \")\n-        # print(sum_reward)\n-\n+        \n         if render:\n             self.render()\n \n         # for i in range(self.k):\n"
                },
                {
                    "date": 1691630359862,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -89,9 +89,9 @@\n             self.X[i] = self.tmp_X[i+6]\n \n         obs =self.get_observation()\n         sum_reward = self.reward()\n-        \n+\n         if render:\n             self.render()\n \n         # for i in range(self.k):\n@@ -113,8 +113,9 @@\n         #         self.controller.init_controller()\n         #         # print('joint{%.1i} < min val'% (i))\n         #         # print(action)\n         #         return obs, sum_reward, done, dict()\n+        \n         return obs, sum_reward, done, dict()\n     \n         \n     def _construct_action_space(self):\n"
                },
                {
                    "date": 1691630501718,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -87,8 +87,10 @@\n             self.X_goal[i] = self.tmp_X[i]\n         for i in range(6):\n             self.X[i] = self.tmp_X[i+6]\n \n+        print(self.X)\n+        print(self.X_goal)\n         obs =self.get_observation()\n         sum_reward = self.reward()\n \n         if render:\n@@ -113,9 +115,9 @@\n         #         self.controller.init_controller()\n         #         # print('joint{%.1i} < min val'% (i))\n         #         # print(action)\n         #         return obs, sum_reward, done, dict()\n-        \n+\n         return obs, sum_reward, done, dict()\n     \n         \n     def _construct_action_space(self):\n"
                },
                {
                    "date": 1691630665072,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -21,11 +21,14 @@\n         self.model = mujoco.MjModel.from_xml_path(self.model_path)\n         self.data = mujoco.MjData(self.model)\n \n         self.viewer = viewer.launch_passive(model=self.model, data=self.data)\n-\n+        self.duration = 380  # (seconds)\n+        self.framerate = 10  # (Hz)\n+        \n         self.controller = controller.CController()\n         self.torque = np.zeros(self.k)\n+        print(\"1\")\n         self.tmp_X = np.zeros(12)\n         self.X_goal = np.zeros(6)\n         self.X = np.zeros(6)\n \n"
                },
                {
                    "date": 1691630682602,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -23,12 +23,12 @@\n \n         self.viewer = viewer.launch_passive(model=self.model, data=self.data)\n         self.duration = 380  # (seconds)\n         self.framerate = 10  # (Hz)\n-        \n+\n         self.controller = controller.CController()\n         self.torque = np.zeros(self.k)\n-        print(\"1\")\n+        \n         self.tmp_X = np.zeros(12)\n         self.X_goal = np.zeros(6)\n         self.X = np.zeros(6)\n \n"
                },
                {
                    "date": 1691630694241,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -70,8 +70,9 @@\n             # \"video.frames_per_second\": int(1000),\n         }\n         \n         self.sim_state = self.sim.get_state()\n+        print(\"1\")\n         \n         \n     def step(self, action, render=True):\n         \n"
                },
                {
                    "date": 1691630708399,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -70,9 +70,8 @@\n             # \"video.frames_per_second\": int(1000),\n         }\n         \n         self.sim_state = self.sim.get_state()\n-        print(\"1\")\n         \n         \n     def step(self, action, render=True):\n         \n"
                },
                {
                    "date": 1691630755671,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -26,20 +26,21 @@\n         self.framerate = 10  # (Hz)\n \n         self.controller = controller.CController()\n         self.torque = np.zeros(self.k)\n-        \n+        print(\"1\")\n         self.tmp_X = np.zeros(12)\n         self.X_goal = np.zeros(6)\n         self.X = np.zeros(6)\n-\n+        print(\"2\")\n         # self.reward_arr = np.zeros(2) #shape=(2,), dtype=np.float64\n         self.reward_arr = [0,0,0]\n         self.cnt = 1\n         self.temp_action = np.zeros(20)\n         self._max_joint_position = np.zeros(self.k)\n         self._min_joint_position = np.zeros(self.k)\n         self.timestep = 0\n+        print(\"3\")\n \n         self._max_joint_position[0] = 0.15\n         self._min_joint_position[0] = -0.15\n         self._max_joint_position[1] = 50.0 * DEG2RAD\n@@ -56,20 +57,23 @@\n         self._min_joint_position[6] = -90.0 * DEG2RAD\n \n         self._max_joint_position_hat = np.zeros(self.k)\n         self._min_joint_position_hat = np.zeros(self.k)\n+        print(\"4\")\n \n         for i in range(self.k):\n             self._max_joint_position_hat[i] = self._max_joint_position[i] - 0.1*(self._max_joint_position[i] - self._min_joint_position[i])\n             self._min_joint_position_hat[i] = self._min_joint_position[i] + 0.1*(self._max_joint_position[i] - self._min_joint_position[i])    \n         \n+        print(\"5\")\n         self.observation_space = self._construct_observation_space()\n         self.action_space = self._construct_action_space()\n         self.metadata = {\n             \"render.modes\": [\"human\", \"rgb_array\", \"depth_array\"],\n             \"video.frames_per_second\": int(np.round(1.0 / self.dt)),\n             # \"video.frames_per_second\": int(1000),\n         }\n+        print(\"6\")\n         \n         self.sim_state = self.sim.get_state()\n         \n         \n"
                },
                {
                    "date": 1691630860134,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -65,15 +65,16 @@\n             self._min_joint_position_hat[i] = self._min_joint_position[i] + 0.1*(self._max_joint_position[i] - self._min_joint_position[i])    \n         \n         print(\"5\")\n         self.observation_space = self._construct_observation_space()\n+        print(\"6\")\n         self.action_space = self._construct_action_space()\n         self.metadata = {\n             \"render.modes\": [\"human\", \"rgb_array\", \"depth_array\"],\n             \"video.frames_per_second\": int(np.round(1.0 / self.dt)),\n             # \"video.frames_per_second\": int(1000),\n         }\n-        print(\"6\")\n+        print(\"7\")\n         \n         self.sim_state = self.sim.get_state()\n         \n         \n"
                },
                {
                    "date": 1691630919726,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -139,9 +139,9 @@\n         obs_low = -1 * np.ones(32) # X(6), X_goal(6), action(20) \n         obs_high = 1 * np.ones(32)\n         # obs_low = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0,0,0, -1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1]\n         # obs_high = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0,0,0, 1,1,1,1,1,1,1,1,1,1,1,1]\n-        return gym.spaces.Box(obs_low, obs_high)\n+        return gym.spaces.Box(obs_low, obs_high, dtype=np.float64)\n     \n      \n     \n     def get_observation(self):\n"
                },
                {
                    "date": 1691630937825,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -67,14 +67,15 @@\n         print(\"5\")\n         self.observation_space = self._construct_observation_space()\n         print(\"6\")\n         self.action_space = self._construct_action_space()\n+        print(\"7\")\n         self.metadata = {\n             \"render.modes\": [\"human\", \"rgb_array\", \"depth_array\"],\n             \"video.frames_per_second\": int(np.round(1.0 / self.dt)),\n             # \"video.frames_per_second\": int(1000),\n         }\n-        print(\"7\")\n+        print(\"8\")\n         \n         self.sim_state = self.sim.get_state()\n         \n         \n"
                },
                {
                    "date": 1691630978737,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -68,13 +68,13 @@\n         self.observation_space = self._construct_observation_space()\n         print(\"6\")\n         self.action_space = self._construct_action_space()\n         print(\"7\")\n-        self.metadata = {\n-            \"render.modes\": [\"human\", \"rgb_array\", \"depth_array\"],\n-            \"video.frames_per_second\": int(np.round(1.0 / self.dt)),\n-            # \"video.frames_per_second\": int(1000),\n-        }\n+        # self.metadata = {\n+        #     \"render.modes\": [\"human\", \"rgb_array\", \"depth_array\"],\n+        #     \"video.frames_per_second\": int(np.round(1.0 / self.dt)),\n+        #     # \"video.frames_per_second\": int(1000),\n+        # }\n         print(\"8\")\n         \n         self.sim_state = self.sim.get_state()\n         \n"
                },
                {
                    "date": 1691631046166,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -75,18 +75,18 @@\n         #     # \"video.frames_per_second\": int(1000),\n         # }\n         print(\"8\")\n         \n-        self.sim_state = self.sim.get_state()\n+        # self.data = self.sim.get_state()\n         \n         \n     def step(self, action, render=True):\n         \n         # print('step')\n         sum_reward = 0\n         done = False\n \n-        self.controller.read(self.sim_state.time, self.sim_state.qpos[0:self.k], self.sim_state.qvel[0:self.k], action)\n+        self.controller.read(self.data.time, self.data.qpos[0:self.k], self.data.qvel[0:self.k], action)\n         self.controller.control_mujoco()\n         self.tmp_X = self.controller.state_controller()\n         self.torque = self.controller.write()\n         for i in range(self.k):\n@@ -107,19 +107,19 @@\n \n         # for i in range(self.k):\n         #     # print(i)\n         #     # print(\":\")\n-        #     # print(self.sim_state.qpos[i])\n+        #     # print(self.data.qpos[i])\n         #     # print(self._min_joint_position[i])\n-        #     if self.sim_state.qpos[i] >=  self._max_joint_position[i] :\n+        #     if self.data.qpos[i] >=  self._max_joint_position[i] :\n         #         done = True\n         #         sum_reward = sum_reward - 10000\n         #         self.controller.init_controller()\n         #         # print('joint{%.1i} > max val'% (i))\n         #         # print(action)\n         #         return obs, sum_reward, done, dict()\n             \n-        #     elif self.sim_state.qpos[i] <= self._min_joint_position[i] :\n+        #     elif self.data.qpos[i] <= self._min_joint_position[i] :\n         #         done = True\n         #         sum_reward = sum_reward - 10000\n         #         self.controller.init_controller()\n         #         # print('joint{%.1i} < min val'% (i))\n@@ -145,9 +145,9 @@\n     \n      \n     \n     def get_observation(self):\n-        self.sim_state = self.sim.get_state()\n+        self.data = self.sim.get_state()\n         \n         # position = self.# controller로부터 받은 X\n         # goal = # controller로부터 받은 goal\n         self.temp_action = self.temp_action * 10\n@@ -162,9 +162,9 @@\n         \n     def reset(self):\n         qpos = np.zeros(self.k)\n         qvel = np.zeros(self.k)\n-        # self.sim_state.qpos[0:self.k], self.sim_state.qvel[0:self.k]\n+        # self.data.qpos[0:self.k], self.data.qvel[0:self.k]\n         self.set_state(qpos, qvel)\n         self.timestep = 0\n         self.temp_action = np.zeros(20)\n         \n@@ -284,18 +284,18 @@\n         #Joint Limit Avoidance (Cost Function)\n         reward2 = 0.0\n         alpha2 = 1.0\n         for i in range(15) :\n-            if self._min_joint_position_hat[i] > self.sim_state.qpos[i]:\n+            if self._min_joint_position_hat[i] > self.data.qpos[i]:\n                 if i == 0:\n-                    reward2 = reward2 - abs(self.sim_state.qpos[i] - self._min_joint_position_hat[i]) * alpha2\n+                    reward2 = reward2 - abs(self.data.qpos[i] - self._min_joint_position_hat[i]) * alpha2\n                 else :\n-                    reward2 = reward2 - abs(self.sim_state.qpos[i] - self._min_joint_position_hat[i])\n-            if self._max_joint_position_hat[i] < self.sim_state.qpos[i]:\n+                    reward2 = reward2 - abs(self.data.qpos[i] - self._min_joint_position_hat[i])\n+            if self._max_joint_position_hat[i] < self.data.qpos[i]:\n                 if i == 0:\n-                    reward2 = reward2 - abs(self._max_joint_position_hat[i] - self.sim_state.qpos[i]) * alpha2\n+                    reward2 = reward2 - abs(self._max_joint_position_hat[i] - self.data.qpos[i]) * alpha2\n                 else :\n-                    reward2 = reward2 - abs(self._max_joint_position_hat[i] - self.sim_state.qpos[i])\n+                    reward2 = reward2 - abs(self._max_joint_position_hat[i] - self.data.qpos[i])\n         reward2 = reward2 * alpha2\n \n         # print(\"reward1 : %.3f, reward2 : %.3f, reward3 : %.3f\"%(reward1,reward2,reward3))\n         # print(reward1)\n"
                },
                {
                    "date": 1691631114678,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -75,18 +75,18 @@\n         #     # \"video.frames_per_second\": int(1000),\n         # }\n         print(\"8\")\n         \n-        # self.data = self.sim.get_state()\n+        # self.sim_state = self.sim.get_state()\n         \n         \n     def step(self, action, render=True):\n         \n         # print('step')\n         sum_reward = 0\n         done = False\n \n-        self.controller.read(self.data.time, self.data.qpos[0:self.k], self.data.qvel[0:self.k], action)\n+        self.controller.read(self.sim_state.time, self.sim_state.qpos[0:self.k], self.sim_state.qvel[0:self.k], action)\n         self.controller.control_mujoco()\n         self.tmp_X = self.controller.state_controller()\n         self.torque = self.controller.write()\n         for i in range(self.k):\n@@ -107,19 +107,19 @@\n \n         # for i in range(self.k):\n         #     # print(i)\n         #     # print(\":\")\n-        #     # print(self.data.qpos[i])\n+        #     # print(self.sim_state.qpos[i])\n         #     # print(self._min_joint_position[i])\n-        #     if self.data.qpos[i] >=  self._max_joint_position[i] :\n+        #     if self.sim_state.qpos[i] >=  self._max_joint_position[i] :\n         #         done = True\n         #         sum_reward = sum_reward - 10000\n         #         self.controller.init_controller()\n         #         # print('joint{%.1i} > max val'% (i))\n         #         # print(action)\n         #         return obs, sum_reward, done, dict()\n             \n-        #     elif self.data.qpos[i] <= self._min_joint_position[i] :\n+        #     elif self.sim_state.qpos[i] <= self._min_joint_position[i] :\n         #         done = True\n         #         sum_reward = sum_reward - 10000\n         #         self.controller.init_controller()\n         #         # print('joint{%.1i} < min val'% (i))\n@@ -145,9 +145,9 @@\n     \n      \n     \n     def get_observation(self):\n-        self.data = self.sim.get_state()\n+        self.sim_state = self.sim.get_state()\n         \n         # position = self.# controller로부터 받은 X\n         # goal = # controller로부터 받은 goal\n         self.temp_action = self.temp_action * 10\n@@ -162,9 +162,9 @@\n         \n     def reset(self):\n         qpos = np.zeros(self.k)\n         qvel = np.zeros(self.k)\n-        # self.data.qpos[0:self.k], self.data.qvel[0:self.k]\n+        # self.sim_state.qpos[0:self.k], self.sim_state.qvel[0:self.k]\n         self.set_state(qpos, qvel)\n         self.timestep = 0\n         self.temp_action = np.zeros(20)\n         \n@@ -284,18 +284,18 @@\n         #Joint Limit Avoidance (Cost Function)\n         reward2 = 0.0\n         alpha2 = 1.0\n         for i in range(15) :\n-            if self._min_joint_position_hat[i] > self.data.qpos[i]:\n+            if self._min_joint_position_hat[i] > self.sim_state.qpos[i]:\n                 if i == 0:\n-                    reward2 = reward2 - abs(self.data.qpos[i] - self._min_joint_position_hat[i]) * alpha2\n+                    reward2 = reward2 - abs(self.sim_state.qpos[i] - self._min_joint_position_hat[i]) * alpha2\n                 else :\n-                    reward2 = reward2 - abs(self.data.qpos[i] - self._min_joint_position_hat[i])\n-            if self._max_joint_position_hat[i] < self.data.qpos[i]:\n+                    reward2 = reward2 - abs(self.sim_state.qpos[i] - self._min_joint_position_hat[i])\n+            if self._max_joint_position_hat[i] < self.sim_state.qpos[i]:\n                 if i == 0:\n-                    reward2 = reward2 - abs(self._max_joint_position_hat[i] - self.data.qpos[i]) * alpha2\n+                    reward2 = reward2 - abs(self._max_joint_position_hat[i] - self.sim_state.qpos[i]) * alpha2\n                 else :\n-                    reward2 = reward2 - abs(self._max_joint_position_hat[i] - self.data.qpos[i])\n+                    reward2 = reward2 - abs(self._max_joint_position_hat[i] - self.sim_state.qpos[i])\n         reward2 = reward2 * alpha2\n \n         # print(\"reward1 : %.3f, reward2 : %.3f, reward3 : %.3f\"%(reward1,reward2,reward3))\n         # print(reward1)\n"
                },
                {
                    "date": 1691631134628,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -84,9 +84,9 @@\n         # print('step')\n         sum_reward = 0\n         done = False\n \n-        self.controller.read(self.sim_state.time, self.sim_state.qpos[0:self.k], self.sim_state.qvel[0:self.k], action)\n+        self.controller.read(self.data.time, self.data.qpos[0:self.k], self.data.qvel[0:self.k], action)\n         self.controller.control_mujoco()\n         self.tmp_X = self.controller.state_controller()\n         self.torque = self.controller.write()\n         for i in range(self.k):\n"
                },
                {
                    "date": 1691631145056,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -145,9 +145,9 @@\n     \n      \n     \n     def get_observation(self):\n-        self.sim_state = self.sim.get_state()\n+        # self.sim_state = self.sim.get_state()\n         \n         # position = self.# controller로부터 받은 X\n         # goal = # controller로부터 받은 goal\n         self.temp_action = self.temp_action * 10\n"
                },
                {
                    "date": 1691631317189,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -163,9 +163,12 @@\n     def reset(self):\n         qpos = np.zeros(self.k)\n         qvel = np.zeros(self.k)\n         # self.sim_state.qpos[0:self.k], self.sim_state.qvel[0:self.k]\n-        self.set_state(qpos, qvel)\n+\n+        for i in range(self.k):\n+            self.data.qpos[i] = self.qpos[i]\n+\n         self.timestep = 0\n         self.temp_action = np.zeros(20)\n         \n         return self.get_observation()\n"
                },
                {
                    "date": 1691631323003,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -166,8 +166,9 @@\n         # self.sim_state.qpos[0:self.k], self.sim_state.qvel[0:self.k]\n \n         for i in range(self.k):\n             self.data.qpos[i] = self.qpos[i]\n+            self.data.qvel[i] = self.qvel[i]\n \n         self.timestep = 0\n         self.temp_action = np.zeros(20)\n         \n"
                },
                {
                    "date": 1691631362458,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -26,21 +26,18 @@\n         self.framerate = 10  # (Hz)\n \n         self.controller = controller.CController()\n         self.torque = np.zeros(self.k)\n-        print(\"1\")\n         self.tmp_X = np.zeros(12)\n         self.X_goal = np.zeros(6)\n         self.X = np.zeros(6)\n-        print(\"2\")\n         # self.reward_arr = np.zeros(2) #shape=(2,), dtype=np.float64\n         self.reward_arr = [0,0,0]\n         self.cnt = 1\n         self.temp_action = np.zeros(20)\n         self._max_joint_position = np.zeros(self.k)\n         self._min_joint_position = np.zeros(self.k)\n         self.timestep = 0\n-        print(\"3\")\n \n         self._max_joint_position[0] = 0.15\n         self._min_joint_position[0] = -0.15\n         self._max_joint_position[1] = 50.0 * DEG2RAD\n@@ -57,25 +54,20 @@\n         self._min_joint_position[6] = -90.0 * DEG2RAD\n \n         self._max_joint_position_hat = np.zeros(self.k)\n         self._min_joint_position_hat = np.zeros(self.k)\n-        print(\"4\")\n \n         for i in range(self.k):\n             self._max_joint_position_hat[i] = self._max_joint_position[i] - 0.1*(self._max_joint_position[i] - self._min_joint_position[i])\n             self._min_joint_position_hat[i] = self._min_joint_position[i] + 0.1*(self._max_joint_position[i] - self._min_joint_position[i])    \n         \n-        print(\"5\")\n         self.observation_space = self._construct_observation_space()\n-        print(\"6\")\n         self.action_space = self._construct_action_space()\n-        print(\"7\")\n         # self.metadata = {\n         #     \"render.modes\": [\"human\", \"rgb_array\", \"depth_array\"],\n         #     \"video.frames_per_second\": int(np.round(1.0 / self.dt)),\n         #     # \"video.frames_per_second\": int(1000),\n         # }\n-        print(\"8\")\n         \n         # self.sim_state = self.sim.get_state()\n         \n         \n"
                },
                {
                    "date": 1691631423700,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -152,15 +152,15 @@\n         )\n         )\n         \n     def reset(self):\n-        qpos = np.zeros(self.k)\n-        qvel = np.zeros(self.k)\n+        # self.qpos = np.zeros(self.k)\n+        # qvel = np.zeros(self.k)\n         # self.sim_state.qpos[0:self.k], self.sim_state.qvel[0:self.k]\n \n         for i in range(self.k):\n-            self.data.qpos[i] = self.qpos[i]\n-            self.data.qvel[i] = self.qvel[i]\n+            self.data.qpos[i] = 0.0\n+            self.data.qvel[i] = 0.0\n \n         self.timestep = 0\n         self.temp_action = np.zeros(20)\n         \n"
                },
                {
                    "date": 1691631535065,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -32,9 +32,9 @@\n         self.X = np.zeros(6)\n         # self.reward_arr = np.zeros(2) #shape=(2,), dtype=np.float64\n         self.reward_arr = [0,0,0]\n         self.cnt = 1\n-        self.temp_action = np.zeros(20)\n+        self.temp_action = np.zeros(1)\n         self._max_joint_position = np.zeros(self.k)\n         self._min_joint_position = np.zeros(self.k)\n         self.timestep = 0\n \n@@ -122,16 +122,16 @@\n     \n         \n     def _construct_action_space(self):\n         # action_low = -1 * np.ones(12)\n-        action_low =  np.zeros(20)\n-        action_high = np.ones(20) \n+        action_low =  np.zeros(1)\n+        action_high = np.ones(1) \n         return gym.spaces.Box(low=action_low, high=action_high, dtype=np.float64)\n     \n     \n     def _construct_observation_space(self):\n-        obs_low = -1 * np.ones(32) # X(6), X_goal(6), action(20) \n-        obs_high = 1 * np.ones(32)\n+        obs_low = -1 * np.ones(13) # X(6), X_goal(6), action(1) \n+        obs_high = 1 * np.ones(13)\n         # obs_low = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0,0,0, -1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1]\n         # obs_high = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0,0,0, 1,1,1,1,1,1,1,1,1,1,1,1]\n         return gym.spaces.Box(obs_low, obs_high, dtype=np.float64)\n     \n"
                },
                {
                    "date": 1691631593430,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -161,9 +161,9 @@\n             self.data.qpos[i] = 0.0\n             self.data.qvel[i] = 0.0\n \n         self.timestep = 0\n-        self.temp_action = np.zeros(20)\n+        self.temp_action = np.zeros(1)\n         \n         return self.get_observation()\n     \n     \n"
                },
                {
                    "date": 1691631623047,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -91,9 +91,9 @@\n \n         print(self.X)\n         print(self.X_goal)\n         obs =self.get_observation()\n-        sum_reward = self.reward()\n+        # sum_reward = self.reward()\n \n         if render:\n             self.render()\n \n"
                },
                {
                    "date": 1691631628725,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -92,9 +92,9 @@\n         print(self.X)\n         print(self.X_goal)\n         obs =self.get_observation()\n         # sum_reward = self.reward()\n-\n+        sum_reward = 0.0\n         if render:\n             self.render()\n \n         # for i in range(self.k):\n"
                },
                {
                    "date": 1691631753340,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -91,8 +91,9 @@\n \n         print(self.X)\n         print(self.X_goal)\n         obs =self.get_observation()\n+        mujoco.mj_step(self.model, self.data)\n         # sum_reward = self.reward()\n         sum_reward = 0.0\n         if render:\n             self.render()\n"
                },
                {
                    "date": 1691631772670,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -88,10 +88,10 @@\n             self.X_goal[i] = self.tmp_X[i]\n         for i in range(6):\n             self.X[i] = self.tmp_X[i+6]\n \n-        print(self.X)\n-        print(self.X_goal)\n+        # print(self.X)\n+        # print(self.X_goal)\n         obs =self.get_observation()\n         mujoco.mj_step(self.model, self.data)\n         # sum_reward = self.reward()\n         sum_reward = 0.0\n"
                },
                {
                    "date": 1691631783989,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -96,30 +96,9 @@\n         # sum_reward = self.reward()\n         sum_reward = 0.0\n         if render:\n             self.render()\n-\n-        # for i in range(self.k):\n-        #     # print(i)\n-        #     # print(\":\")\n-        #     # print(self.sim_state.qpos[i])\n-        #     # print(self._min_joint_position[i])\n-        #     if self.sim_state.qpos[i] >=  self._max_joint_position[i] :\n-        #         done = True\n-        #         sum_reward = sum_reward - 10000\n-        #         self.controller.init_controller()\n-        #         # print('joint{%.1i} > max val'% (i))\n-        #         # print(action)\n-        #         return obs, sum_reward, done, dict()\n-            \n-        #     elif self.sim_state.qpos[i] <= self._min_joint_position[i] :\n-        #         done = True\n-        #         sum_reward = sum_reward - 10000\n-        #         self.controller.init_controller()\n-        #         # print('joint{%.1i} < min val'% (i))\n-        #         # print(action)\n-        #         return obs, sum_reward, done, dict()\n-\n+        \n         return obs, sum_reward, done, dict()\n     \n         \n     def _construct_action_space(self):\n"
                },
                {
                    "date": 1691631845439,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -94,10 +94,10 @@\n         obs =self.get_observation()\n         mujoco.mj_step(self.model, self.data)\n         # sum_reward = self.reward()\n         sum_reward = 0.0\n-        if render:\n-            self.render()\n+        # if render:\n+        #     self.render()\n         \n         return obs, sum_reward, done, dict()\n     \n         \n"
                },
                {
                    "date": 1691631866890,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -71,9 +71,9 @@\n         # self.sim_state = self.sim.get_state()\n         \n         \n     def step(self, action, render=True):\n-        \n+        self.viewer.sync()\n         # print('step')\n         sum_reward = 0\n         done = False\n \n"
                },
                {
                    "date": 1691632152234,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -85,8 +85,9 @@\n                 self.data.ctrl[i] = self.torque[i]\n \n         for i in range(6):\n             self.X_goal[i] = self.tmp_X[i]\n+            # self.data.qpos[0:self.k]\n         for i in range(6):\n             self.X[i] = self.tmp_X[i+6]\n \n         # print(self.X)\n"
                },
                {
                    "date": 1691632215553,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -90,9 +90,9 @@\n         for i in range(6):\n             self.X[i] = self.tmp_X[i+6]\n \n         # print(self.X)\n-        # print(self.X_goal)\n+        print(self.data.xpos)\n         obs =self.get_observation()\n         mujoco.mj_step(self.model, self.data)\n         # sum_reward = self.reward()\n         sum_reward = 0.0\n"
                },
                {
                    "date": 1691632261866,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -90,9 +90,9 @@\n         for i in range(6):\n             self.X[i] = self.tmp_X[i+6]\n \n         # print(self.X)\n-        print(self.data.xpos)\n+        print(self.data.xpos[14,0])\n         obs =self.get_observation()\n         mujoco.mj_step(self.model, self.data)\n         # sum_reward = self.reward()\n         sum_reward = 0.0\n"
                },
                {
                    "date": 1691632268404,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -90,9 +90,9 @@\n         for i in range(6):\n             self.X[i] = self.tmp_X[i+6]\n \n         # print(self.X)\n-        print(self.data.xpos[14,0])\n+        print(self.data.xpos[13,0])\n         obs =self.get_observation()\n         mujoco.mj_step(self.model, self.data)\n         # sum_reward = self.reward()\n         sum_reward = 0.0\n"
                },
                {
                    "date": 1691632288235,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -90,9 +90,12 @@\n         for i in range(6):\n             self.X[i] = self.tmp_X[i+6]\n \n         # print(self.X)\n-        print(self.data.xpos[13,0])\n+        # print(self.data.xpos[13,0])\n+        self.data.xpos[13,0]\n+        self.data.xpos[13,0]\n+        self.data.xpos[13,0]\n         obs =self.get_observation()\n         mujoco.mj_step(self.model, self.data)\n         # sum_reward = self.reward()\n         sum_reward = 0.0\n"
                },
                {
                    "date": 1691632352439,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -91,8 +91,9 @@\n             self.X[i] = self.tmp_X[i+6]\n \n         # print(self.X)\n         # print(self.data.xpos[13,0])\n+        # xpos랑 xmat을 건드려야함\n         self.data.xpos[13,0]\n         self.data.xpos[13,0]\n         self.data.xpos[13,0]\n         obs =self.get_observation()\n"
                },
                {
                    "date": 1691632391944,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -92,11 +92,10 @@\n \n         # print(self.X)\n         # print(self.data.xpos[13,0])\n         # xpos랑 xmat을 건드려야함\n-        self.data.xpos[13,0]\n-        self.data.xpos[13,0]\n-        self.data.xpos[13,0]\n+        for i in range(3):\n+            self.data.xpos[13,i] = self.X_goal[i]\n         obs =self.get_observation()\n         mujoco.mj_step(self.model, self.data)\n         # sum_reward = self.reward()\n         sum_reward = 0.0\n"
                },
                {
                    "date": 1691632617037,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -89,9 +89,9 @@\n             # self.data.qpos[0:self.k]\n         for i in range(6):\n             self.X[i] = self.tmp_X[i+6]\n \n-        # print(self.X)\n+        print(self.data.qpos)\n         # print(self.data.xpos[13,0])\n         # xpos랑 xmat을 건드려야함\n         for i in range(3):\n             self.data.xpos[13,i] = self.X_goal[i]\n"
                },
                {
                    "date": 1691632670051,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -89,13 +89,13 @@\n             # self.data.qpos[0:self.k]\n         for i in range(6):\n             self.X[i] = self.tmp_X[i+6]\n \n-        print(self.data.qpos)\n+        # print(self.data.qpos)\n         # print(self.data.xpos[13,0])\n         # xpos랑 xmat을 건드려야함\n         for i in range(3):\n-            self.data.xpos[13,i] = self.X_goal[i]\n+            self.data.xpos[9+i] = self.X_goal[i]\n         obs =self.get_observation()\n         mujoco.mj_step(self.model, self.data)\n         # sum_reward = self.reward()\n         sum_reward = 0.0\n"
                },
                {
                    "date": 1691632687175,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -93,9 +93,9 @@\n         # print(self.data.qpos)\n         # print(self.data.xpos[13,0])\n         # xpos랑 xmat을 건드려야함\n         for i in range(3):\n-            self.data.xpos[9+i] = self.X_goal[i]\n+            self.data.qpos[9+i] = self.X_goal[i]\n         obs =self.get_observation()\n         mujoco.mj_step(self.model, self.data)\n         # sum_reward = self.reward()\n         sum_reward = 0.0\n"
                },
                {
                    "date": 1691632765625,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -103,9 +103,23 @@\n         #     self.render()\n         \n         return obs, sum_reward, done, dict()\n     \n+    def euler_to_quaternion(roll, pitch, yaw):\n+        cy = np.cos(yaw * 0.5)\n+        sy = np.sin(yaw * 0.5)\n+        cp = np.cos(pitch * 0.5)\n+        sp = np.sin(pitch * 0.5)\n+        cr = np.cos(roll * 0.5)\n+        sr = np.sin(roll * 0.5)\n         \n+        w = cr * cp * cy + sr * sp * sy\n+        x = sr * cp * cy - cr * sp * sy\n+        y = cr * sp * cy + sr * cp * sy\n+        z = cr * cp * sy - sr * sp * cy\n+        \n+        return np.array([w, x, y, z])\n+\n     def _construct_action_space(self):\n         # action_low = -1 * np.ones(12)\n         action_low =  np.zeros(1)\n         action_high = np.ones(1) \n"
                },
                {
                    "date": 1691632831999,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -88,9 +88,9 @@\n             self.X_goal[i] = self.tmp_X[i]\n             # self.data.qpos[0:self.k]\n         for i in range(6):\n             self.X[i] = self.tmp_X[i+6]\n-\n+        self.euler_to_quaternion(self.X_goal[3],self.X_goal[4],self.X_goal[5])\n         # print(self.data.qpos)\n         # print(self.data.xpos[13,0])\n         # xpos랑 xmat을 건드려야함\n         for i in range(3):\n"
                },
                {
                    "date": 1691632848433,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -88,9 +88,9 @@\n             self.X_goal[i] = self.tmp_X[i]\n             # self.data.qpos[0:self.k]\n         for i in range(6):\n             self.X[i] = self.tmp_X[i+6]\n-        self.euler_to_quaternion(self.X_goal[3],self.X_goal[4],self.X_goal[5])\n+        self.quaternion_goal = self.euler_to_quaternion(self.X_goal[3],self.X_goal[4],self.X_goal[5])\n         # print(self.data.qpos)\n         # print(self.data.xpos[13,0])\n         # xpos랑 xmat을 건드려야함\n         for i in range(3):\n"
                },
                {
                    "date": 1691632859335,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -29,8 +29,9 @@\n         self.torque = np.zeros(self.k)\n         self.tmp_X = np.zeros(12)\n         self.X_goal = np.zeros(6)\n         self.X = np.zeros(6)\n+        self.quaternion_goal = np.zeros(4)\n         # self.reward_arr = np.zeros(2) #shape=(2,), dtype=np.float64\n         self.reward_arr = [0,0,0]\n         self.cnt = 1\n         self.temp_action = np.zeros(1)\n"
                },
                {
                    "date": 1691632888137,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -95,8 +95,10 @@\n         # print(self.data.xpos[13,0])\n         # xpos랑 xmat을 건드려야함\n         for i in range(3):\n             self.data.qpos[9+i] = self.X_goal[i]\n+        for i in range(4):\n+            self.data.qpos[12+i] = self.quaternion_goal[i]\n         obs =self.get_observation()\n         mujoco.mj_step(self.model, self.data)\n         # sum_reward = self.reward()\n         sum_reward = 0.0\n"
                },
                {
                    "date": 1691632990452,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -89,9 +89,9 @@\n             self.X_goal[i] = self.tmp_X[i]\n             # self.data.qpos[0:self.k]\n         for i in range(6):\n             self.X[i] = self.tmp_X[i+6]\n-        self.quaternion_goal = self.euler_to_quaternion(self.X_goal[3],self.X_goal[4],self.X_goal[5])\n+        self.euler_to_quaternion(self.X_goal[3],self.X_goal[4],self.X_goal[5])\n         # print(self.data.qpos)\n         # print(self.data.xpos[13,0])\n         # xpos랑 xmat을 건드려야함\n         for i in range(3):\n"
                },
                {
                    "date": 1691633024861,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -106,9 +106,9 @@\n         #     self.render()\n         \n         return obs, sum_reward, done, dict()\n     \n-    def euler_to_quaternion(roll, pitch, yaw):\n+    def euler_to_quaternion(self, roll, pitch, yaw):\n         cy = np.cos(yaw * 0.5)\n         sy = np.sin(yaw * 0.5)\n         cp = np.cos(pitch * 0.5)\n         sp = np.sin(pitch * 0.5)\n@@ -118,10 +118,13 @@\n         w = cr * cp * cy + sr * sp * sy\n         x = sr * cp * cy - cr * sp * sy\n         y = cr * sp * cy + sr * cp * sy\n         z = cr * cp * sy - sr * sp * cy\n-        \n-        return np.array([w, x, y, z])\n+        self.quaternion_goal[0] = w\n+        self.quaternion_goal[0] = x\n+        self.quaternion_goal[0] = y\n+        self.quaternion_goal[0] = z\n+        # return np.array([w, x, y, z])\n \n     def _construct_action_space(self):\n         # action_low = -1 * np.ones(12)\n         action_low =  np.zeros(1)\n"
                },
                {
                    "date": 1691633030148,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -119,11 +119,11 @@\n         x = sr * cp * cy - cr * sp * sy\n         y = cr * sp * cy + sr * cp * sy\n         z = cr * cp * sy - sr * sp * cy\n         self.quaternion_goal[0] = w\n-        self.quaternion_goal[0] = x\n-        self.quaternion_goal[0] = y\n-        self.quaternion_goal[0] = z\n+        self.quaternion_goal[1] = x\n+        self.quaternion_goal[2] = y\n+        self.quaternion_goal[3] = z\n         # return np.array([w, x, y, z])\n \n     def _construct_action_space(self):\n         # action_low = -1 * np.ones(12)\n"
                },
                {
                    "date": 1691633582077,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -99,9 +99,9 @@\n         for i in range(4):\n             self.data.qpos[12+i] = self.quaternion_goal[i]\n         obs =self.get_observation()\n         mujoco.mj_step(self.model, self.data)\n-        # sum_reward = self.reward()\n+        sum_reward = self.reward()\n         sum_reward = 0.0\n         # if render:\n         #     self.render()\n         \n"
                },
                {
                    "date": 1691633615437,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -273,36 +273,15 @@\n \n     def reward(self):\n         #Reaching Goal Poses (Cost Function)\n         # self.timestep = self.timestep + 1\n-        self.reward_arr = self.controller.error()\n-        if self.reward_arr[3] == 1:\n-            return 21498098\n-        reward1 = -self.reward_arr[0] - self.reward_arr[1]\n-        alpha = 1\n-        reward1 = reward1 * alpha\n+        \n \n-        #Joint Limit Avoidance (Cost Function)\n-        reward2 = 0.0\n-        alpha2 = 1.0\n-        for i in range(15) :\n-            if self._min_joint_position_hat[i] > self.sim_state.qpos[i]:\n-                if i == 0:\n-                    reward2 = reward2 - abs(self.sim_state.qpos[i] - self._min_joint_position_hat[i]) * alpha2\n-                else :\n-                    reward2 = reward2 - abs(self.sim_state.qpos[i] - self._min_joint_position_hat[i])\n-            if self._max_joint_position_hat[i] < self.sim_state.qpos[i]:\n-                if i == 0:\n-                    reward2 = reward2 - abs(self._max_joint_position_hat[i] - self.sim_state.qpos[i]) * alpha2\n-                else :\n-                    reward2 = reward2 - abs(self._max_joint_position_hat[i] - self.sim_state.qpos[i])\n-        reward2 = reward2 * alpha2\n-\n         # print(\"reward1 : %.3f, reward2 : %.3f, reward3 : %.3f\"%(reward1,reward2,reward3))\n         # print(reward1)\n         # print(\"reward2 : %.3f\"%reward2)\n         # print(reward2)\n         # print(\"reward3 : %.3f\"%reward3)\n         # print(reward3)\n         # reward = reward1 + reward2 + reward3 + 2\n-        reward = reward1 + reward2 + 1\n+        reward = \n         return reward\n\\ No newline at end of file\n"
                },
                {
                    "date": 1691633622408,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -272,16 +272,6 @@\n         return np.concatenate([self.sim.data.qpos.flat, self.sim.data.qvel.flat])\n \n     def reward(self):\n         #Reaching Goal Poses (Cost Function)\n-        # self.timestep = self.timestep + 1\n-        \n-\n-        # print(\"reward1 : %.3f, reward2 : %.3f, reward3 : %.3f\"%(reward1,reward2,reward3))\n-        # print(reward1)\n-        # print(\"reward2 : %.3f\"%reward2)\n-        # print(reward2)\n-        # print(\"reward3 : %.3f\"%reward3)\n-        # print(reward3)\n-        # reward = reward1 + reward2 + reward3 + 2\n         reward = \n         return reward\n\\ No newline at end of file\n"
                },
                {
                    "date": 1691633695122,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -272,6 +272,9 @@\n         return np.concatenate([self.sim.data.qpos.flat, self.sim.data.qvel.flat])\n \n     def reward(self):\n         #Reaching Goal Poses (Cost Function)\n-        reward = \n+        for i in range(6):\n+            reward += 1/(self.X_goal - self.X)\n+        \n+        reward = self.X_goal\n         return reward\n\\ No newline at end of file\n"
                },
                {
                    "date": 1691633718389,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -275,6 +275,6 @@\n         #Reaching Goal Poses (Cost Function)\n         for i in range(6):\n             reward += 1/(self.X_goal - self.X)\n         \n-        reward = self.X_goal\n+        reward = reward * 100\n         return reward\n\\ No newline at end of file\n"
                },
                {
                    "date": 1691633864132,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -101,10 +101,14 @@\n         obs =self.get_observation()\n         mujoco.mj_step(self.model, self.data)\n         sum_reward = self.reward()\n         sum_reward = 0.0\n-        # if render:\n-        #     self.render()\n+\n+        if time > 10:\n+            for i in range(6) :\n+                if (self.X_goal[i] - self.X[i]) > 0.01\n+                    self.reset()\n+                    sum_reward \n         \n         return obs, sum_reward, done, dict()\n     \n     def euler_to_quaternion(self, roll, pitch, yaw):\n"
                },
                {
                    "date": 1691633881136,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -99,16 +99,16 @@\n         for i in range(4):\n             self.data.qpos[12+i] = self.quaternion_goal[i]\n         obs =self.get_observation()\n         mujoco.mj_step(self.model, self.data)\n-        sum_reward = self.reward()\n-        sum_reward = 0.0\n+        sum_reward += self.reward()\n+        # sum_reward = 0.0\n \n         if time > 10:\n             for i in range(6) :\n                 if (self.X_goal[i] - self.X[i]) > 0.01\n                     self.reset()\n-                    sum_reward \n+                    sum_reward -= 1000\n         \n         return obs, sum_reward, done, dict()\n     \n     def euler_to_quaternion(self, roll, pitch, yaw):\n"
                },
                {
                    "date": 1691633918979,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -102,11 +102,11 @@\n         mujoco.mj_step(self.model, self.data)\n         sum_reward += self.reward()\n         # sum_reward = 0.0\n \n-        if time > 10:\n+        if self.time > 10:\n             for i in range(6) :\n-                if (self.X_goal[i] - self.X[i]) > 0.01\n+                if (self.X_goal[i] - self.X[i]) > 0.01:\n                     self.reset()\n                     sum_reward -= 1000\n         \n         return obs, sum_reward, done, dict()\n"
                },
                {
                    "date": 1691633937377,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -277,8 +277,8 @@\n \n     def reward(self):\n         #Reaching Goal Poses (Cost Function)\n         for i in range(6):\n-            reward += 1/(self.X_goal - self.X)\n+            reward += (self.X_goal - self.X)\n         \n         reward = reward * 100\n         return reward\n\\ No newline at end of file\n"
                },
                {
                    "date": 1691633943567,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -277,8 +277,8 @@\n \n     def reward(self):\n         #Reaching Goal Poses (Cost Function)\n         for i in range(6):\n-            reward += (self.X_goal - self.X)\n+            reward += 1/(self.X_goal[i] - self.X[i])\n         \n         reward = reward * 100\n         return reward\n\\ No newline at end of file\n"
                },
                {
                    "date": 1691633951206,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -277,8 +277,8 @@\n \n     def reward(self):\n         #Reaching Goal Poses (Cost Function)\n         for i in range(6):\n-            reward += 1/(self.X_goal[i] - self.X[i])\n+            reward += (self.X_goal[i] - self.X[i])\n         \n         reward = reward * 100\n         return reward\n\\ No newline at end of file\n"
                },
                {
                    "date": 1691633964144,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -277,8 +277,8 @@\n \n     def reward(self):\n         #Reaching Goal Poses (Cost Function)\n         for i in range(6):\n-            reward += (self.X_goal[i] - self.X[i])\n+            reward = reward + (self.X_goal[i] - self.X[i])\n         \n         reward = reward * 100\n         return reward\n\\ No newline at end of file\n"
                },
                {
                    "date": 1691633985213,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -277,8 +277,8 @@\n \n     def reward(self):\n         #Reaching Goal Poses (Cost Function)\n         for i in range(6):\n-            reward = reward + (self.X_goal[i] - self.X[i])\n+            tmp_reward = tmp_reward + 1/(self.X_goal[i] - self.X[i])\n         \n         reward = reward * 100\n         return reward\n\\ No newline at end of file\n"
                },
                {
                    "date": 1691634001587,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -278,7 +278,6 @@\n     def reward(self):\n         #Reaching Goal Poses (Cost Function)\n         for i in range(6):\n             tmp_reward = tmp_reward + 1/(self.X_goal[i] - self.X[i])\n-        \n-        reward = reward * 100\n+        reward = tmp_reward * 100\n         return reward\n\\ No newline at end of file\n"
                },
                {
                    "date": 1691634032141,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -276,8 +276,9 @@\n         return np.concatenate([self.sim.data.qpos.flat, self.sim.data.qvel.flat])\n \n     def reward(self):\n         #Reaching Goal Poses (Cost Function)\n+        tmp_reward 0 \n         for i in range(6):\n             tmp_reward = tmp_reward + 1/(self.X_goal[i] - self.X[i])\n         reward = tmp_reward * 100\n         return reward\n\\ No newline at end of file\n"
                },
                {
                    "date": 1691634043938,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -278,7 +278,8 @@\n     def reward(self):\n         #Reaching Goal Poses (Cost Function)\n         tmp_reward 0 \n         for i in range(6):\n-            tmp_reward = tmp_reward + 1/(self.X_goal[i] - self.X[i])\n+            if self.X_goal[i] != self.X[i]:  # 분모가 0인 경우 처리\n+            tmp_reward += 1 / (self.X_goal[i] - self.X[i])\n         reward = tmp_reward * 100\n         return reward\n\\ No newline at end of file\n"
                },
                {
                    "date": 1691634049895,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -276,10 +276,10 @@\n         return np.concatenate([self.sim.data.qpos.flat, self.sim.data.qvel.flat])\n \n     def reward(self):\n         #Reaching Goal Poses (Cost Function)\n-        tmp_reward 0 \n+        tmp_reward = 0\n         for i in range(6):\n             if self.X_goal[i] != self.X[i]:  # 분모가 0인 경우 처리\n-            tmp_reward += 1 / (self.X_goal[i] - self.X[i])\n+                tmp_reward += 1 / (self.X_goal[i] - self.X[i])\n         reward = tmp_reward * 100\n         return reward\n\\ No newline at end of file\n"
                },
                {
                    "date": 1691634068317,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -102,9 +102,9 @@\n         mujoco.mj_step(self.model, self.data)\n         sum_reward += self.reward()\n         # sum_reward = 0.0\n \n-        if self.time > 10:\n+        if self.data.time > 10:\n             for i in range(6) :\n                 if (self.X_goal[i] - self.X[i]) > 0.01:\n                     self.reset()\n                     sum_reward -= 1000\n"
                },
                {
                    "date": 1691634138556,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -101,9 +101,9 @@\n         obs =self.get_observation()\n         mujoco.mj_step(self.model, self.data)\n         sum_reward += self.reward()\n         # sum_reward = 0.0\n-\n+        print(self.data.time)\n         if self.data.time > 10:\n             for i in range(6) :\n                 if (self.X_goal[i] - self.X[i]) > 0.01:\n                     self.reset()\n"
                },
                {
                    "date": 1691634169614,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -107,8 +107,9 @@\n             for i in range(6) :\n                 if (self.X_goal[i] - self.X[i]) > 0.01:\n                     self.reset()\n                     sum_reward -= 1000\n+                    return obs, sum_reward, done, dict()\n         \n         return obs, sum_reward, done, dict()\n     \n     def euler_to_quaternion(self, roll, pitch, yaw):\n"
                },
                {
                    "date": 1691634183129,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -102,9 +102,9 @@\n         mujoco.mj_step(self.model, self.data)\n         sum_reward += self.reward()\n         # sum_reward = 0.0\n         print(self.data.time)\n-        if self.data.time > 10:\n+        if self.data.time > 3:\n             for i in range(6) :\n                 if (self.X_goal[i] - self.X[i]) > 0.01:\n                     self.reset()\n                     sum_reward -= 1000\n"
                },
                {
                    "date": 1691634223735,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -102,8 +102,9 @@\n         mujoco.mj_step(self.model, self.data)\n         sum_reward += self.reward()\n         # sum_reward = 0.0\n         print(self.data.time)\n+        \n         if self.data.time > 3:\n             for i in range(6) :\n                 if (self.X_goal[i] - self.X[i]) > 0.01:\n                     self.reset()\n"
                },
                {
                    "date": 1691634247721,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -102,9 +102,9 @@\n         mujoco.mj_step(self.model, self.data)\n         sum_reward += self.reward()\n         # sum_reward = 0.0\n         print(self.data.time)\n-        \n+\n         if self.data.time > 3:\n             for i in range(6) :\n                 if (self.X_goal[i] - self.X[i]) > 0.01:\n                     self.reset()\n@@ -165,9 +165,9 @@\n     def reset(self):\n         # self.qpos = np.zeros(self.k)\n         # qvel = np.zeros(self.k)\n         # self.sim_state.qpos[0:self.k], self.sim_state.qvel[0:self.k]\n-\n+        self.start_time = self.data.time\n         for i in range(self.k):\n             self.data.qpos[i] = 0.0\n             self.data.qvel[i] = 0.0\n \n"
                },
                {
                    "date": 1691634264908,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -62,8 +62,10 @@\n             self._min_joint_position_hat[i] = self._min_joint_position[i] + 0.1*(self._max_joint_position[i] - self._min_joint_position[i])    \n         \n         self.observation_space = self._construct_observation_space()\n         self.action_space = self._construct_action_space()\n+        self.start_time = 0\n+        self.motion_time = 10\n         # self.metadata = {\n         #     \"render.modes\": [\"human\", \"rgb_array\", \"depth_array\"],\n         #     \"video.frames_per_second\": int(np.round(1.0 / self.dt)),\n         #     # \"video.frames_per_second\": int(1000),\n"
                },
                {
                    "date": 1691634287386,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -64,8 +64,9 @@\n         self.observation_space = self._construct_observation_space()\n         self.action_space = self._construct_action_space()\n         self.start_time = 0\n         self.motion_time = 10\n+        self.end_time = self.start_time + self.motion_time\n         # self.metadata = {\n         #     \"render.modes\": [\"human\", \"rgb_array\", \"depth_array\"],\n         #     \"video.frames_per_second\": int(np.round(1.0 / self.dt)),\n         #     # \"video.frames_per_second\": int(1000),\n"
                },
                {
                    "date": 1691634300789,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -168,9 +168,9 @@\n     def reset(self):\n         # self.qpos = np.zeros(self.k)\n         # qvel = np.zeros(self.k)\n         # self.sim_state.qpos[0:self.k], self.sim_state.qvel[0:self.k]\n-        self.start_time = self.data.time\n+        self.end_time = self.start_time + self.motion_time\n         for i in range(self.k):\n             self.data.qpos[i] = 0.0\n             self.data.qvel[i] = 0.0\n \n"
                },
                {
                    "date": 1691634311220,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -106,9 +106,9 @@\n         sum_reward += self.reward()\n         # sum_reward = 0.0\n         print(self.data.time)\n \n-        if self.data.time > 3:\n+        if self.data.time > self.end_time:\n             for i in range(6) :\n                 if (self.X_goal[i] - self.X[i]) > 0.01:\n                     self.reset()\n                     sum_reward -= 1000\n"
                },
                {
                    "date": 1691634389942,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -168,9 +168,11 @@\n     def reset(self):\n         # self.qpos = np.zeros(self.k)\n         # qvel = np.zeros(self.k)\n         # self.sim_state.qpos[0:self.k], self.sim_state.qvel[0:self.k]\n+        self.controller.initialize()\n         self.end_time = self.start_time + self.motion_time\n+\n         for i in range(self.k):\n             self.data.qpos[i] = 0.0\n             self.data.qvel[i] = 0.0\n \n"
                },
                {
                    "date": 1691634433928,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -169,9 +169,9 @@\n         # self.qpos = np.zeros(self.k)\n         # qvel = np.zeros(self.k)\n         # self.sim_state.qpos[0:self.k], self.sim_state.qvel[0:self.k]\n         self.controller.initialize()\n-        self.end_time = self.start_time + self.motion_time\n+        self.end_time = self.data.time + self.motion_time\n \n         for i in range(self.k):\n             self.data.qpos[i] = 0.0\n             self.data.qvel[i] = 0.0\n"
                },
                {
                    "date": 1691634669257,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -112,8 +112,12 @@\n                 if (self.X_goal[i] - self.X[i]) > 0.01:\n                     self.reset()\n                     sum_reward -= 1000\n                     return obs, sum_reward, done, dict()\n+                else:\n+                    if i == 5 :\n+                        sum_reward -= 1000\n+                        return obs, sum_reward, done, dict()\n         \n         return obs, sum_reward, done, dict()\n     \n     def euler_to_quaternion(self, roll, pitch, yaw):\n"
                },
                {
                    "date": 1691634750889,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -106,17 +106,25 @@\n         sum_reward += self.reward()\n         # sum_reward = 0.0\n         print(self.data.time)\n \n+        for i in range(self.k) : \n+            if(self.data.qpos[i] > self._max_joint_position[i]) :\n+                sum_reward -= 1000\n+                return obs, sum_reward, done, dict()\n+            if(self.data.qpos[i] < self._min_joint_position[i]) :\n+                sum_reward -= 1000\n+                return obs, sum_reward, done, dict()\n+\n         if self.data.time > self.end_time:\n             for i in range(6) :\n                 if (self.X_goal[i] - self.X[i]) > 0.01:\n                     self.reset()\n                     sum_reward -= 1000\n                     return obs, sum_reward, done, dict()\n                 else:\n                     if i == 5 :\n-                        sum_reward -= 1000\n+                        sum_reward += 1000\n                         return obs, sum_reward, done, dict()\n         \n         return obs, sum_reward, done, dict()\n     \n"
                },
                {
                    "date": 1691634824341,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -38,23 +38,26 @@\n         self._max_joint_position = np.zeros(self.k)\n         self._min_joint_position = np.zeros(self.k)\n         self.timestep = 0\n \n+        self._min_joint_position[0] = -2.8973\n+        self._min_joint_position[1] = -1.7628\n+        self._min_joint_position[2] = -2.8973\n+        self._min_joint_position[3] = -3.0718\n+        self._min_joint_position[4] = -2.8973\n+        self._min_joint_position[5] = -0.0175\n+        self._min_joint_position[6] = -2.8973\n+\n         self._max_joint_position[0] = 0.15\n-        self._min_joint_position[0] = -0.15\n         self._max_joint_position[1] = 50.0 * DEG2RAD\n-        self._min_joint_position[1] = -160.0 * DEG2RAD\n         self._max_joint_position[2] = 170.0 * DEG2RAD\n-        self._min_joint_position[2] = -30.0 * DEG2RAD\n-        self._max_joint_position[3] = 180.0 * DEG2RAD # 몸 안쪽 회전\n-        self._min_joint_position[3] = -150.0 * DEG2RAD # 몸 바깥쪽 회전\n+        self._max_joint_position[3] = 180.0 * DEG2RAD\n         self._max_joint_position[4] = 160.0 * DEG2RAD\n-        self._min_joint_position[4] = -30.0 * DEG2RAD\n         self._max_joint_position[5] = 170.0 * DEG2RAD\n-        self._min_joint_position[5] = -170.0 * DEG2RAD\n         self._max_joint_position[6] = 90.0 * DEG2RAD\n-        self._min_joint_position[6] = -90.0 * DEG2RAD\n \n+\n+\n         self._max_joint_position_hat = np.zeros(self.k)\n         self._min_joint_position_hat = np.zeros(self.k)\n \n         for i in range(self.k):\n"
                },
                {
                    "date": 1691634835042,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -46,15 +46,15 @@\n         self._min_joint_position[4] = -2.8973\n         self._min_joint_position[5] = -0.0175\n         self._min_joint_position[6] = -2.8973\n \n-        self._max_joint_position[0] = 0.15\n-        self._max_joint_position[1] = 50.0 * DEG2RAD\n-        self._max_joint_position[2] = 170.0 * DEG2RAD\n-        self._max_joint_position[3] = 180.0 * DEG2RAD\n-        self._max_joint_position[4] = 160.0 * DEG2RAD\n-        self._max_joint_position[5] = 170.0 * DEG2RAD\n-        self._max_joint_position[6] = 90.0 * DEG2RAD\n+        self._max_joint_position[0] = 2.8973\n+        self._max_joint_position[1] = 1.7628\n+        self._max_joint_position[2] = 2.8973\n+        self._max_joint_position[3] = -0.0698\n+        self._max_joint_position[4] = 2.8973\n+        self._max_joint_position[5] = 3.7525\n+        self._max_joint_position[6] = 2.8973\n \n \n \n         self._max_joint_position_hat = np.zeros(self.k)\n"
                },
                {
                    "date": 1691634886655,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -54,10 +54,8 @@\n         self._max_joint_position[4] = 2.8973\n         self._max_joint_position[5] = 3.7525\n         self._max_joint_position[6] = 2.8973\n \n-\n-\n         self._max_joint_position_hat = np.zeros(self.k)\n         self._min_joint_position_hat = np.zeros(self.k)\n \n         for i in range(self.k):\n@@ -107,9 +105,9 @@\n         obs =self.get_observation()\n         mujoco.mj_step(self.model, self.data)\n         sum_reward += self.reward()\n         # sum_reward = 0.0\n-        print(self.data.time)\n+        # print(self.data.time)\n \n         for i in range(self.k) : \n             if(self.data.qpos[i] > self._max_joint_position[i]) :\n                 sum_reward -= 1000\n"
                },
                {
                    "date": 1691634901934,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -109,11 +109,13 @@\n         # print(self.data.time)\n \n         for i in range(self.k) : \n             if(self.data.qpos[i] > self._max_joint_position[i]) :\n+                self.reset()\n                 sum_reward -= 1000\n                 return obs, sum_reward, done, dict()\n             if(self.data.qpos[i] < self._min_joint_position[i]) :\n+                self.reset()\n                 sum_reward -= 1000\n                 return obs, sum_reward, done, dict()\n \n         if self.data.time > self.end_time:\n"
                },
                {
                    "date": 1691634970241,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -190,8 +190,10 @@\n         for i in range(self.k):\n             self.data.qpos[i] = 0.0\n             self.data.qvel[i] = 0.0\n \n+        self.data.qpos[5] = 30.0 * DEG2RAD\n+\n         self.timestep = 0\n         self.temp_action = np.zeros(1)\n         \n         return self.get_observation()\n"
                },
                {
                    "date": 1691635017996,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -190,8 +190,9 @@\n         for i in range(self.k):\n             self.data.qpos[i] = 0.0\n             self.data.qvel[i] = 0.0\n \n+        self.data.qpos[3] = 1.1\n         self.data.qpos[5] = 30.0 * DEG2RAD\n \n         self.timestep = 0\n         self.temp_action = np.zeros(1)\n"
                },
                {
                    "date": 1691635373153,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -66,8 +66,17 @@\n         self.action_space = self._construct_action_space()\n         self.start_time = 0\n         self.motion_time = 10\n         self.end_time = self.start_time + self.motion_time\n+\n+        for i in range(self.k):\n+            self.data.qpos[i] = 0.0\n+            self.data.qvel[i] = 0.0\n+\n+        self.data.qpos[3] = -1.1\n+        self.data.qpos[5] = 30.0 * DEG2RAD\n+\n+\n         # self.metadata = {\n         #     \"render.modes\": [\"human\", \"rgb_array\", \"depth_array\"],\n         #     \"video.frames_per_second\": int(np.round(1.0 / self.dt)),\n         #     # \"video.frames_per_second\": int(1000),\n@@ -190,9 +199,9 @@\n         for i in range(self.k):\n             self.data.qpos[i] = 0.0\n             self.data.qvel[i] = 0.0\n \n-        self.data.qpos[3] = 1.1\n+        self.data.qpos[3] = -1.1\n         self.data.qpos[5] = 30.0 * DEG2RAD\n \n         self.timestep = 0\n         self.temp_action = np.zeros(1)\n"
                },
                {
                    "date": 1691635429340,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -71,10 +71,9 @@\n         for i in range(self.k):\n             self.data.qpos[i] = 0.0\n             self.data.qvel[i] = 0.0\n \n-        self.data.qpos[3] = -1.1\n-        self.data.qpos[5] = 30.0 * DEG2RAD\n+        self.reset()\n \n \n         # self.metadata = {\n         #     \"render.modes\": [\"human\", \"rgb_array\", \"depth_array\"],\n"
                },
                {
                    "date": 1691635443799,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -72,8 +72,9 @@\n             self.data.qpos[i] = 0.0\n             self.data.qvel[i] = 0.0\n \n         self.reset()\n+        mujoco.mj_step(self.model, self.data)\n \n \n         # self.metadata = {\n         #     \"render.modes\": [\"human\", \"rgb_array\", \"depth_array\"],\n"
                },
                {
                    "date": 1691635635722,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -111,9 +111,9 @@\n             self.data.qpos[9+i] = self.X_goal[i]\n         for i in range(4):\n             self.data.qpos[12+i] = self.quaternion_goal[i]\n         obs =self.get_observation()\n-        mujoco.mj_step(self.model, self.data)\n+        \n         sum_reward += self.reward()\n         # sum_reward = 0.0\n         # print(self.data.time)\n \n"
                },
                {
                    "date": 1691635647043,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -204,8 +204,10 @@\n         self.data.qpos[5] = 30.0 * DEG2RAD\n \n         self.timestep = 0\n         self.temp_action = np.zeros(1)\n+\n+        mujoco.mj_step(self.model, self.data)\n         \n         return self.get_observation()\n     \n     \n"
                },
                {
                    "date": 1691635673240,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -72,9 +72,9 @@\n             self.data.qpos[i] = 0.0\n             self.data.qvel[i] = 0.0\n \n         self.reset()\n-        mujoco.mj_step(self.model, self.data)\n+        # mujoco.mj_step(self.model, self.data)\n \n \n         # self.metadata = {\n         #     \"render.modes\": [\"human\", \"rgb_array\", \"depth_array\"],\n"
                },
                {
                    "date": 1691635682108,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -71,9 +71,9 @@\n         for i in range(self.k):\n             self.data.qpos[i] = 0.0\n             self.data.qvel[i] = 0.0\n \n-        self.reset()\n+        # self.reset()\n         # mujoco.mj_step(self.model, self.data)\n \n \n         # self.metadata = {\n"
                },
                {
                    "date": 1691635692616,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -205,9 +205,9 @@\n \n         self.timestep = 0\n         self.temp_action = np.zeros(1)\n \n-        mujoco.mj_step(self.model, self.data)\n+        # mujoco.mj_step(self.model, self.data)\n         \n         return self.get_observation()\n     \n     \n"
                },
                {
                    "date": 1691635710533,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -67,11 +67,11 @@\n         self.start_time = 0\n         self.motion_time = 10\n         self.end_time = self.start_time + self.motion_time\n \n-        for i in range(self.k):\n-            self.data.qpos[i] = 0.0\n-            self.data.qvel[i] = 0.0\n+        # for i in range(self.k):\n+        #     self.data.qpos[i] = 0.0\n+        #     self.data.qvel[i] = 0.0\n \n         # self.reset()\n         # mujoco.mj_step(self.model, self.data)\n \n"
                },
                {
                    "date": 1691635819776,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -66,17 +66,9 @@\n         self.action_space = self._construct_action_space()\n         self.start_time = 0\n         self.motion_time = 10\n         self.end_time = self.start_time + self.motion_time\n-\n-        # for i in range(self.k):\n-        #     self.data.qpos[i] = 0.0\n-        #     self.data.qvel[i] = 0.0\n-\n-        # self.reset()\n-        # mujoco.mj_step(self.model, self.data)\n-\n-\n+        \n         # self.metadata = {\n         #     \"render.modes\": [\"human\", \"rgb_array\", \"depth_array\"],\n         #     \"video.frames_per_second\": int(np.round(1.0 / self.dt)),\n         #     # \"video.frames_per_second\": int(1000),\n@@ -111,9 +103,9 @@\n             self.data.qpos[9+i] = self.X_goal[i]\n         for i in range(4):\n             self.data.qpos[12+i] = self.quaternion_goal[i]\n         obs =self.get_observation()\n-        \n+        mujoco.mj_step(self.model, self.data)\n         sum_reward += self.reward()\n         # sum_reward = 0.0\n         # print(self.data.time)\n \n@@ -204,10 +196,8 @@\n         self.data.qpos[5] = 30.0 * DEG2RAD\n \n         self.timestep = 0\n         self.temp_action = np.zeros(1)\n-\n-        # mujoco.mj_step(self.model, self.data)\n         \n         return self.get_observation()\n     \n     \n"
                },
                {
                    "date": 1691635831423,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -196,8 +196,10 @@\n         self.data.qpos[5] = 30.0 * DEG2RAD\n \n         self.timestep = 0\n         self.temp_action = np.zeros(1)\n+\n+        mujoco.mj_step(self.model, self.data)\n         \n         return self.get_observation()\n     \n     \n"
                },
                {
                    "date": 1691635915222,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -66,8 +66,17 @@\n         self.action_space = self._construct_action_space()\n         self.start_time = 0\n         self.motion_time = 10\n         self.end_time = self.start_time + self.motion_time\n+\n+\n+        for i in range(self.k):\n+            self.data.qpos[i] = 0.0\n+            self.data.qvel[i] = 0.0\n+\n+        self.data.qpos[3] = -1.1\n+        self.data.qpos[5] = 30.0 * DEG2RAD\n+        mujoco.mj_step(self.model, self.data)\n         \n         # self.metadata = {\n         #     \"render.modes\": [\"human\", \"rgb_array\", \"depth_array\"],\n         #     \"video.frames_per_second\": int(np.round(1.0 / self.dt)),\n"
                },
                {
                    "date": 1691635947544,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -117,28 +117,28 @@\n         sum_reward += self.reward()\n         # sum_reward = 0.0\n         # print(self.data.time)\n \n-        for i in range(self.k) : \n-            if(self.data.qpos[i] > self._max_joint_position[i]) :\n-                self.reset()\n-                sum_reward -= 1000\n-                return obs, sum_reward, done, dict()\n-            if(self.data.qpos[i] < self._min_joint_position[i]) :\n-                self.reset()\n-                sum_reward -= 1000\n-                return obs, sum_reward, done, dict()\n+        # for i in range(self.k) : \n+        #     if(self.data.qpos[i] > self._max_joint_position[i]) :\n+        #         self.reset()\n+        #         sum_reward -= 1000\n+        #         return obs, sum_reward, done, dict()\n+        #     if(self.data.qpos[i] < self._min_joint_position[i]) :\n+        #         self.reset()\n+        #         sum_reward -= 1000\n+        #         return obs, sum_reward, done, dict()\n \n-        if self.data.time > self.end_time:\n-            for i in range(6) :\n-                if (self.X_goal[i] - self.X[i]) > 0.01:\n-                    self.reset()\n-                    sum_reward -= 1000\n-                    return obs, sum_reward, done, dict()\n-                else:\n-                    if i == 5 :\n-                        sum_reward += 1000\n-                        return obs, sum_reward, done, dict()\n+        # if self.data.time > self.end_time:\n+        #     for i in range(6) :\n+        #         if (self.X_goal[i] - self.X[i]) > 0.01:\n+        #             self.reset()\n+        #             sum_reward -= 1000\n+        #             return obs, sum_reward, done, dict()\n+        #         else:\n+        #             if i == 5 :\n+        #                 sum_reward += 1000\n+        #                 return obs, sum_reward, done, dict()\n         \n         return obs, sum_reward, done, dict()\n     \n     def euler_to_quaternion(self, roll, pitch, yaw):\n"
                },
                {
                    "date": 1691635958635,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -117,28 +117,28 @@\n         sum_reward += self.reward()\n         # sum_reward = 0.0\n         # print(self.data.time)\n \n-        # for i in range(self.k) : \n-        #     if(self.data.qpos[i] > self._max_joint_position[i]) :\n-        #         self.reset()\n-        #         sum_reward -= 1000\n-        #         return obs, sum_reward, done, dict()\n-        #     if(self.data.qpos[i] < self._min_joint_position[i]) :\n-        #         self.reset()\n-        #         sum_reward -= 1000\n-        #         return obs, sum_reward, done, dict()\n+        for i in range(self.k) : \n+            if(self.data.qpos[i] > self._max_joint_position[i]) :\n+                self.reset()\n+                sum_reward -= 1000\n+                return obs, sum_reward, done, dict()\n+            if(self.data.qpos[i] < self._min_joint_position[i]) :\n+                self.reset()\n+                sum_reward -= 1000\n+                return obs, sum_reward, done, dict()\n \n-        # if self.data.time > self.end_time:\n-        #     for i in range(6) :\n-        #         if (self.X_goal[i] - self.X[i]) > 0.01:\n-        #             self.reset()\n-        #             sum_reward -= 1000\n-        #             return obs, sum_reward, done, dict()\n-        #         else:\n-        #             if i == 5 :\n-        #                 sum_reward += 1000\n-        #                 return obs, sum_reward, done, dict()\n+        if self.data.time > self.end_time:\n+            for i in range(6) :\n+                if (self.X_goal[i] - self.X[i]) > 0.01:\n+                    self.reset()\n+                    sum_reward -= 1000\n+                    return obs, sum_reward, done, dict()\n+                else:\n+                    if i == 5 :\n+                        sum_reward += 1000\n+                        return obs, sum_reward, done, dict()\n         \n         return obs, sum_reward, done, dict()\n     \n     def euler_to_quaternion(self, roll, pitch, yaw):\n"
                },
                {
                    "date": 1691636017036,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -115,9 +115,9 @@\n         obs =self.get_observation()\n         mujoco.mj_step(self.model, self.data)\n         sum_reward += self.reward()\n         # sum_reward = 0.0\n-        # print(self.data.time)\n+        print(self.data.time)\n \n         for i in range(self.k) : \n             if(self.data.qpos[i] > self._max_joint_position[i]) :\n                 self.reset()\n"
                },
                {
                    "date": 1691636029831,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -115,9 +115,9 @@\n         obs =self.get_observation()\n         mujoco.mj_step(self.model, self.data)\n         sum_reward += self.reward()\n         # sum_reward = 0.0\n-        print(self.data.time)\n+        # print(self.data.time)\n \n         for i in range(self.k) : \n             if(self.data.qpos[i] > self._max_joint_position[i]) :\n                 self.reset()\n"
                },
                {
                    "date": 1691636556698,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -136,8 +136,9 @@\n                     return obs, sum_reward, done, dict()\n                 else:\n                     if i == 5 :\n                         sum_reward += 1000\n+                        self.reset()\n                         return obs, sum_reward, done, dict()\n         \n         return obs, sum_reward, done, dict()\n     \n"
                },
                {
                    "date": 1691640033556,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -136,9 +136,11 @@\n                     return obs, sum_reward, done, dict()\n                 else:\n                     if i == 5 :\n                         sum_reward += 1000\n-                        self.reset()\n+                        # self.reset()\n+                        self.controller.initialize()\n+                        mujoco.mj_step(self.model, self.data)\n                         return obs, sum_reward, done, dict()\n         \n         return obs, sum_reward, done, dict()\n     \n"
                },
                {
                    "date": 1691640112189,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -137,9 +137,9 @@\n                 else:\n                     if i == 5 :\n                         sum_reward += 1000\n                         # self.reset()\n-                        self.controller.initialize()\n+                        self.controller.reset_goal()\n                         mujoco.mj_step(self.model, self.data)\n                         return obs, sum_reward, done, dict()\n         \n         return obs, sum_reward, done, dict()\n"
                },
                {
                    "date": 1691640872644,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -138,9 +138,9 @@\n                     if i == 5 :\n                         sum_reward += 1000\n                         # self.reset()\n                         self.controller.reset_goal()\n-                        mujoco.mj_step(self.model, self.data)\n+                        # mujoco.mj_step(self.model, self.data)\n                         return obs, sum_reward, done, dict()\n         \n         return obs, sum_reward, done, dict()\n     \n"
                },
                {
                    "date": 1691641725705,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -182,9 +182,9 @@\n         # self.sim_state = self.sim.get_state()\n         \n         # position = self.# controller로부터 받은 X\n         # goal = # controller로부터 받은 goal\n-        self.temp_action = self.temp_action * 10\n+        # self.temp_action = self.temp_action * 10\n         # self.temp_action = self.temp_action - 1\n         return np.concatenate(\n         (\n             self.X, #6\n"
                },
                {
                    "date": 1691641741546,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -188,9 +188,9 @@\n         return np.concatenate(\n         (\n             self.X, #6\n             self.X_goal, #6\n-            self.temp_action, # 20\n+            self.action, # 20\n         )\n         )\n         \n     def reset(self):\n"
                },
                {
                    "date": 1691641754172,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -207,9 +207,9 @@\n         self.data.qpos[3] = -1.1\n         self.data.qpos[5] = 30.0 * DEG2RAD\n \n         self.timestep = 0\n-        self.temp_action = np.zeros(1)\n+        # self.temp_action = np.zeros(1)\n \n         mujoco.mj_step(self.model, self.data)\n         \n         return self.get_observation()\n"
                },
                {
                    "date": 1691641776382,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -90,9 +90,9 @@\n         self.viewer.sync()\n         # print('step')\n         sum_reward = 0\n         done = False\n-\n+        self.temp_action = action\n         self.controller.read(self.data.time, self.data.qpos[0:self.k], self.data.qvel[0:self.k], action)\n         self.controller.control_mujoco()\n         self.tmp_X = self.controller.state_controller()\n         self.torque = self.controller.write()\n"
                },
                {
                    "date": 1691641791953,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -91,8 +91,9 @@\n         # print('step')\n         sum_reward = 0\n         done = False\n         self.temp_action = action\n+        action = action * 10\n         self.controller.read(self.data.time, self.data.qpos[0:self.k], self.data.qvel[0:self.k], action)\n         self.controller.control_mujoco()\n         self.tmp_X = self.controller.state_controller()\n         self.torque = self.controller.write()\n"
                },
                {
                    "date": 1691641800479,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -189,9 +189,9 @@\n         return np.concatenate(\n         (\n             self.X, #6\n             self.X_goal, #6\n-            self.action, # 20\n+            self.temp_action, # 20\n         )\n         )\n         \n     def reset(self):\n"
                },
                {
                    "date": 1691641935548,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -181,9 +181,25 @@\n     \n     def get_observation(self):\n         # self.sim_state = self.sim.get_state()\n         \n-        # position = self.# controller로부터 받은 X\n+        position[0] = self.X[0] + 0.75\n+        position[0] = self.X[0] / 1.5\n+        \n+        position[1] = self.X[1] + 0.75\n+        position[1] = self.X[1] / 1.5\n+        \n+        position[2] = self.X[2] + 0.75\n+        position[2] = self.X[2] / 1.5\n+\n+        position[3] = self.X[3] + 3.14\n+        position[3] = self.X[3] / 6.28\n+        \n+        position[4] = self.X[4] + 3.14\n+        position[4] = self.X[4] / 6.28\n+        \n+        position[5] = self.X[5] + 3.14\n+        position[5] = self.X[5] / 6.28\n         # goal = # controller로부터 받은 goal\n         # self.temp_action = self.temp_action * 10\n         # self.temp_action = self.temp_action - 1\n         return np.concatenate(\n"
                },
                {
                    "date": 1691641967690,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -30,8 +30,11 @@\n         self.tmp_X = np.zeros(12)\n         self.X_goal = np.zeros(6)\n         self.X = np.zeros(6)\n         self.quaternion_goal = np.zeros(4)\n+\n+        self.position = np.zeros(6)\n+        self.goal = np.zeros(6)\n         # self.reward_arr = np.zeros(2) #shape=(2,), dtype=np.float64\n         self.reward_arr = [0,0,0]\n         self.cnt = 1\n         self.temp_action = np.zeros(1)\n@@ -203,9 +206,9 @@\n         # self.temp_action = self.temp_action * 10\n         # self.temp_action = self.temp_action - 1\n         return np.concatenate(\n         (\n-            self.X, #6\n+            position, #6\n             self.X_goal, #6\n             self.temp_action, # 20\n         )\n         )\n"
                },
                {
                    "date": 1691642019661,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -184,25 +184,43 @@\n     \n     def get_observation(self):\n         # self.sim_state = self.sim.get_state()\n         \n-        position[0] = self.X[0] + 0.75\n-        position[0] = self.X[0] / 1.5\n+        self.position[0] = self.X[0] + 0.75\n+        self.position[0] = self.X[0] / 1.5\n         \n-        position[1] = self.X[1] + 0.75\n-        position[1] = self.X[1] / 1.5\n+        self.position[1] = self.X[1] + 0.75\n+        self.position[1] = self.X[1] / 1.5\n         \n-        position[2] = self.X[2] + 0.75\n-        position[2] = self.X[2] / 1.5\n+        self.position[2] = self.X[2] + 0.75\n+        self.position[2] = self.X[2] / 1.5\n \n-        position[3] = self.X[3] + 3.14\n-        position[3] = self.X[3] / 6.28\n+        self.position[3] = self.X[3] + 3.14\n+        self.position[3] = self.X[3] / 6.28\n         \n-        position[4] = self.X[4] + 3.14\n-        position[4] = self.X[4] / 6.28\n+        self.position[4] = self.X[4] + 3.14\n+        self.position[4] = self.X[4] / 6.28\n         \n-        position[5] = self.X[5] + 3.14\n-        position[5] = self.X[5] / 6.28\n+        self.position[5] = self.X[5] + 3.14\n+        self.position[5] = self.X[5] / 6.28\n+\n+        self.goal[0] = self.X_goal[0] + 0.75\n+        self.goal[0] = self.X_goal[0] / 1.5\n+        \n+        self.goal[1] = self.X_goal[1] + 0.75\n+        self.goal[1] = self.X_goal[1] / 1.5\n+        \n+        self.goal[2] = self.X_goal[2] + 0.75\n+        self.goal[2] = self.X_goal[2] / 1.5\n+\n+        self.goal[3] = self.X_goal[3] + 3.14\n+        self.goal[3] = self.X_goal[3] / 6.28\n+        \n+        self.goal[4] = self.X_goal[4] + 3.14\n+        self.goal[4] = self.X_goal[4] / 6.28\n+        \n+        self.goal[5] = self.X_goal[5] + 3.14\n+        self.goal[5] = self.X_goal[5] / 6.28\n         # goal = # controller로부터 받은 goal\n         # self.temp_action = self.temp_action * 10\n         # self.temp_action = self.temp_action - 1\n         return np.concatenate(\n"
                },
                {
                    "date": 1691642027661,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -224,10 +224,10 @@\n         # self.temp_action = self.temp_action * 10\n         # self.temp_action = self.temp_action - 1\n         return np.concatenate(\n         (\n-            position, #6\n-            self.X_goal, #6\n+            self.position, #6\n+            self.goal, #6\n             self.temp_action, # 20\n         )\n         )\n         \n"
                },
                {
                    "date": 1691642073628,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -184,43 +184,31 @@\n     \n     def get_observation(self):\n         # self.sim_state = self.sim.get_state()\n         \n-        self.position[0] = self.X[0] + 0.75\n-        self.position[0] = self.X[0] / 1.5\n+        self.position[0] = self.X[0] / 0.75\n         \n-        self.position[1] = self.X[1] + 0.75\n-        self.position[1] = self.X[1] / 1.5\n+        self.position[1] = self.X[1] / 0.75\n         \n-        self.position[2] = self.X[2] + 0.75\n-        self.position[2] = self.X[2] / 1.5\n+        self.position[2] = self.X[2] / 0.75\n \n-        self.position[3] = self.X[3] + 3.14\n-        self.position[3] = self.X[3] / 6.28\n+        self.position[3] = self.X[3] / 3.14\n         \n-        self.position[4] = self.X[4] + 3.14\n-        self.position[4] = self.X[4] / 6.28\n+        self.position[4] = self.X[4] / 3.14\n         \n-        self.position[5] = self.X[5] + 3.14\n-        self.position[5] = self.X[5] / 6.28\n+        self.position[5] = self.X[5] / 3.14\n \n-        self.goal[0] = self.X_goal[0] + 0.75\n-        self.goal[0] = self.X_goal[0] / 1.5\n+        self.goal[0] = self.X_goal[0] / 0.75\n         \n-        self.goal[1] = self.X_goal[1] + 0.75\n-        self.goal[1] = self.X_goal[1] / 1.5\n+        self.goal[1] = self.X_goal[1] / 0.75\n         \n-        self.goal[2] = self.X_goal[2] + 0.75\n-        self.goal[2] = self.X_goal[2] / 1.5\n+        self.goal[2] = self.X_goal[2] / 0.75\n \n-        self.goal[3] = self.X_goal[3] + 3.14\n-        self.goal[3] = self.X_goal[3] / 6.28\n+        self.goal[3] = self.X_goal[3] / 3.14\n         \n-        self.goal[4] = self.X_goal[4] + 3.14\n-        self.goal[4] = self.X_goal[4] / 6.28\n+        self.goal[4] = self.X_goal[4] / 3.14\n         \n-        self.goal[5] = self.X_goal[5] + 3.14\n-        self.goal[5] = self.X_goal[5] / 6.28\n+        self.goal[5] = self.X_goal[5] / 3.14\n         # goal = # controller로부터 받은 goal\n         # self.temp_action = self.temp_action * 10\n         # self.temp_action = self.temp_action - 1\n         return np.concatenate(\n"
                },
                {
                    "date": 1691642088282,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -185,29 +185,19 @@\n     def get_observation(self):\n         # self.sim_state = self.sim.get_state()\n         \n         self.position[0] = self.X[0] / 0.75\n-        \n         self.position[1] = self.X[1] / 0.75\n-        \n         self.position[2] = self.X[2] / 0.75\n-\n         self.position[3] = self.X[3] / 3.14\n-        \n         self.position[4] = self.X[4] / 3.14\n-        \n         self.position[5] = self.X[5] / 3.14\n \n         self.goal[0] = self.X_goal[0] / 0.75\n-        \n         self.goal[1] = self.X_goal[1] / 0.75\n-        \n         self.goal[2] = self.X_goal[2] / 0.75\n-\n         self.goal[3] = self.X_goal[3] / 3.14\n-        \n         self.goal[4] = self.X_goal[4] / 3.14\n-        \n         self.goal[5] = self.X_goal[5] / 3.14\n         # goal = # controller로부터 받은 goal\n         # self.temp_action = self.temp_action * 10\n         # self.temp_action = self.temp_action - 1\n"
                },
                {
                    "date": 1691642148904,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -142,8 +142,9 @@\n                     if i == 5 :\n                         sum_reward += 1000\n                         # self.reset()\n                         self.controller.reset_goal()\n+                        reset_time()\n                         # mujoco.mj_step(self.model, self.data)\n                         return obs, sum_reward, done, dict()\n         \n         return obs, sum_reward, done, dict()\n@@ -229,10 +230,13 @@\n         mujoco.mj_step(self.model, self.data)\n         \n         return self.get_observation()\n     \n+    def reset_time(self):\n+        self.end_time = self.data.time + self.motion_time\n     \n     \n+    \n     def set_state(self, qpos, qvel):\n         old_state = self.sim.get_state()\n         new_state = mujoco.MjSimState(\n             old_state.time, qpos, qvel, old_state.act, old_state.udd_state\n"
                },
                {
                    "date": 1691642233792,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -142,9 +142,10 @@\n                     if i == 5 :\n                         sum_reward += 1000\n                         # self.reset()\n                         self.controller.reset_goal()\n-                        reset_time()\n+                        self.reset_time()\n+                        print(\"!!??\")\n                         # mujoco.mj_step(self.model, self.data)\n                         return obs, sum_reward, done, dict()\n         \n         return obs, sum_reward, done, dict()\n"
                },
                {
                    "date": 1691642284975,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -143,9 +143,9 @@\n                         sum_reward += 1000\n                         # self.reset()\n                         self.controller.reset_goal()\n                         self.reset_time()\n-                        print(\"!!??\")\n+                        # print(\"!!??\")\n                         # mujoco.mj_step(self.model, self.data)\n                         return obs, sum_reward, done, dict()\n         \n         return obs, sum_reward, done, dict()\n"
                },
                {
                    "date": 1691643687880,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -342,5 +342,5 @@\n         for i in range(6):\n             if self.X_goal[i] != self.X[i]:  # 분모가 0인 경우 처리\n                 tmp_reward += 1 / (self.X_goal[i] - self.X[i])\n         reward = tmp_reward * 100\n-        return reward\n\\ No newline at end of file\n+        return 1\n\\ No newline at end of file\n"
                },
                {
                    "date": 1691643696214,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -342,5 +342,5 @@\n         for i in range(6):\n             if self.X_goal[i] != self.X[i]:  # 분모가 0인 경우 처리\n                 tmp_reward += 1 / (self.X_goal[i] - self.X[i])\n         reward = tmp_reward * 100\n-        return 1\n\\ No newline at end of file\n+        return reward\n\\ No newline at end of file\n"
                },
                {
                    "date": 1691644592883,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,7 +1,8 @@\n from tokenize import Double\n import gym\n import numpy as np\n+from gym import spaces\n \n # from mujoco_py import load_model_from_path, MjSim, MjViewer\n # import mujoco_py\n import mujoco\n"
                },
                {
                    "date": 1691644645845,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,6 +1,6 @@\n from tokenize import Double\n-import gym\n+import gymnasium as gym\n import numpy as np\n from gym import spaces\n \n # from mujoco_py import load_model_from_path, MjSim, MjViewer\n"
                },
                {
                    "date": 1691645064109,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -6,9 +6,8 @@\n # from mujoco_py import load_model_from_path, MjSim, MjViewer\n # import mujoco_py\n import mujoco\n \n-from gym import ObservationWrapper, spaces\n from mujoco import viewer\n import controller\n \n DEFAULT_SIZE = 300\n"
                },
                {
                    "date": 1691645132757,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -210,9 +210,9 @@\n             self.temp_action, # 20\n         )\n         )\n         \n-    def reset(self):\n+    def reset(self, *, seed=None, options=None):\n         # self.qpos = np.zeros(self.k)\n         # qvel = np.zeros(self.k)\n         # self.sim_state.qpos[0:self.k], self.sim_state.qvel[0:self.k]\n         self.controller.initialize()\n"
                },
                {
                    "date": 1691645165354,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -210,9 +210,9 @@\n             self.temp_action, # 20\n         )\n         )\n         \n-    def reset(self, *, seed=None, options=None):\n+    def reset(self):\n         # self.qpos = np.zeros(self.k)\n         # qvel = np.zeros(self.k)\n         # self.sim_state.qpos[0:self.k], self.sim_state.qvel[0:self.k]\n         self.controller.initialize()\n"
                },
                {
                    "date": 1691646658051,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -14,8 +14,9 @@\n DEG2RAD = 3.14 / 180.0\n \n class FrankaEnv(gym.Env):\n     def __init__(self):\n+        self.seed()\n         # self.frame_skip = 1\n         self.k = 7  # for jacobian calculation\n         self.model_path = 'model/fr3.xml'\n         self.model = mujoco.MjModel.from_xml_path(self.model_path)\n"
                },
                {
                    "date": 1691646676891,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -88,9 +88,9 @@\n         # }\n         \n         # self.sim_state = self.sim.get_state()\n         \n-        \n+       \n     def step(self, action, render=True):\n         self.viewer.sync()\n         # print('step')\n         sum_reward = 0\n@@ -343,5 +343,21 @@\n         for i in range(6):\n             if self.X_goal[i] != self.X[i]:  # 분모가 0인 경우 처리\n                 tmp_reward += 1 / (self.X_goal[i] - self.X[i])\n         reward = tmp_reward * 100\n-        return reward\n\\ No newline at end of file\n+        return reward\n+    \n+    def seed (self, seed=None):\n+        \"\"\"Sets the seed for this env's random number generator(s).\n+        Note:\n+            Some environments use multiple pseudorandom number generators.\n+            We want to capture all such seeds used in order to ensure that\n+            there aren't accidental correlations between multiple generators.\n+        Returns:\n+            list<bigint>: Returns the list of seeds used in this env's random\n+              number generators. The first value in the list should be the\n+              \"main\" seed, or the value which a reproducer should pass to\n+              'seed'. Often, the main seed equals the provided 'seed', but\n+              this won't be true if seed=None, for example.\n+        \"\"\"\n+        self.np_random, seed = seeding.np_random(seed)\n+        return [seed]\n\\ No newline at end of file\n"
                },
                {
                    "date": 1691646696033,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,8 +1,9 @@\n from tokenize import Double\n import gymnasium as gym\n import numpy as np\n from gym import spaces\n+from gym.utils import seeding\n \n # from mujoco_py import load_model_from_path, MjSim, MjViewer\n # import mujoco_py\n import mujoco\n"
                },
                {
                    "date": 1691646765317,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -125,20 +125,20 @@\n         # print(self.data.time)\n \n         for i in range(self.k) : \n             if(self.data.qpos[i] > self._max_joint_position[i]) :\n-                self.reset()\n+                self.reset(0)\n                 sum_reward -= 1000\n                 return obs, sum_reward, done, dict()\n             if(self.data.qpos[i] < self._min_joint_position[i]) :\n-                self.reset()\n+                self.reset(0)\n                 sum_reward -= 1000\n                 return obs, sum_reward, done, dict()\n \n         if self.data.time > self.end_time:\n             for i in range(6) :\n                 if (self.X_goal[i] - self.X[i]) > 0.01:\n-                    self.reset()\n+                    self.reset(0)\n                     sum_reward -= 1000\n                     return obs, sum_reward, done, dict()\n                 else:\n                     if i == 5 :\n@@ -212,9 +212,9 @@\n             self.temp_action, # 20\n         )\n         )\n         \n-    def reset(self):\n+    def reset(self,seed):\n         # self.qpos = np.zeros(self.k)\n         # qvel = np.zeros(self.k)\n         # self.sim_state.qpos[0:self.k], self.sim_state.qvel[0:self.k]\n         self.controller.initialize()\n"
                },
                {
                    "date": 1691646773400,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -216,8 +216,9 @@\n     def reset(self,seed):\n         # self.qpos = np.zeros(self.k)\n         # qvel = np.zeros(self.k)\n         # self.sim_state.qpos[0:self.k], self.sim_state.qvel[0:self.k]\n+        self.seed=seed\n         self.controller.initialize()\n         self.end_time = self.data.time + self.motion_time\n \n         for i in range(self.k):\n"
                },
                {
                    "date": 1691646783786,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,9 +1,8 @@\n from tokenize import Double\n import gymnasium as gym\n import numpy as np\n from gym import spaces\n-from gym.utils import seeding\n \n # from mujoco_py import load_model_from_path, MjSim, MjViewer\n # import mujoco_py\n import mujoco\n@@ -15,9 +14,8 @@\n DEG2RAD = 3.14 / 180.0\n \n class FrankaEnv(gym.Env):\n     def __init__(self):\n-        self.seed()\n         # self.frame_skip = 1\n         self.k = 7  # for jacobian calculation\n         self.model_path = 'model/fr3.xml'\n         self.model = mujoco.MjModel.from_xml_path(self.model_path)\n@@ -89,9 +87,9 @@\n         # }\n         \n         # self.sim_state = self.sim.get_state()\n         \n-       \n+        \n     def step(self, action, render=True):\n         self.viewer.sync()\n         # print('step')\n         sum_reward = 0\n@@ -125,20 +123,20 @@\n         # print(self.data.time)\n \n         for i in range(self.k) : \n             if(self.data.qpos[i] > self._max_joint_position[i]) :\n-                self.reset(0)\n+                self.reset()\n                 sum_reward -= 1000\n                 return obs, sum_reward, done, dict()\n             if(self.data.qpos[i] < self._min_joint_position[i]) :\n-                self.reset(0)\n+                self.reset()\n                 sum_reward -= 1000\n                 return obs, sum_reward, done, dict()\n \n         if self.data.time > self.end_time:\n             for i in range(6) :\n                 if (self.X_goal[i] - self.X[i]) > 0.01:\n-                    self.reset(0)\n+                    self.reset()\n                     sum_reward -= 1000\n                     return obs, sum_reward, done, dict()\n                 else:\n                     if i == 5 :\n@@ -212,13 +210,12 @@\n             self.temp_action, # 20\n         )\n         )\n         \n-    def reset(self,seed):\n+    def reset(self):\n         # self.qpos = np.zeros(self.k)\n         # qvel = np.zeros(self.k)\n         # self.sim_state.qpos[0:self.k], self.sim_state.qvel[0:self.k]\n-        self.seed=seed\n         self.controller.initialize()\n         self.end_time = self.data.time + self.motion_time\n \n         for i in range(self.k):\n@@ -345,21 +342,5 @@\n         for i in range(6):\n             if self.X_goal[i] != self.X[i]:  # 분모가 0인 경우 처리\n                 tmp_reward += 1 / (self.X_goal[i] - self.X[i])\n         reward = tmp_reward * 100\n-        return reward\n-    \n-    def seed (self, seed=None):\n-        \"\"\"Sets the seed for this env's random number generator(s).\n-        Note:\n-            Some environments use multiple pseudorandom number generators.\n-            We want to capture all such seeds used in order to ensure that\n-            there aren't accidental correlations between multiple generators.\n-        Returns:\n-            list<bigint>: Returns the list of seeds used in this env's random\n-              number generators. The first value in the list should be the\n-              \"main\" seed, or the value which a reproducer should pass to\n-              'seed'. Often, the main seed equals the provided 'seed', but\n-              this won't be true if seed=None, for example.\n-        \"\"\"\n-        self.np_random, seed = seeding.np_random(seed)\n-        return [seed]\n\\ No newline at end of file\n+        return reward\n\\ No newline at end of file\n"
                },
                {
                    "date": 1691646804267,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -211,8 +211,9 @@\n         )\n         )\n         \n     def reset(self):\n+        super().reset(seed=seed)\n         # self.qpos = np.zeros(self.k)\n         # qvel = np.zeros(self.k)\n         # self.sim_state.qpos[0:self.k], self.sim_state.qvel[0:self.k]\n         self.controller.initialize()\n"
                },
                {
                    "date": 1691646827484,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -211,9 +211,9 @@\n         )\n         )\n         \n     def reset(self):\n-        super().reset(seed=seed)\n+        super().reset(seed=0)\n         # self.qpos = np.zeros(self.k)\n         # qvel = np.zeros(self.k)\n         # self.sim_state.qpos[0:self.k], self.sim_state.qvel[0:self.k]\n         self.controller.initialize()\n"
                },
                {
                    "date": 1691646876579,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -211,9 +211,8 @@\n         )\n         )\n         \n     def reset(self):\n-        super().reset(seed=0)\n         # self.qpos = np.zeros(self.k)\n         # qvel = np.zeros(self.k)\n         # self.sim_state.qpos[0:self.k], self.sim_state.qvel[0:self.k]\n         self.controller.initialize()\n"
                },
                {
                    "date": 1691646958010,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -342,5 +342,8 @@\n         for i in range(6):\n             if self.X_goal[i] != self.X[i]:  # 분모가 0인 경우 처리\n                 tmp_reward += 1 / (self.X_goal[i] - self.X[i])\n         reward = tmp_reward * 100\n-        return reward\n\\ No newline at end of file\n+        return reward\n+    \n+    def seed(self, seed=None):\n+        return\n\\ No newline at end of file\n"
                },
                {
                    "date": 1691647113036,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -345,5 +345,5 @@\n         reward = tmp_reward * 100\n         return reward\n     \n     def seed(self, seed=None):\n-        return\n\\ No newline at end of file\n+        return \n\\ No newline at end of file\n"
                },
                {
                    "date": 1691647232429,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -210,9 +210,9 @@\n             self.temp_action, # 20\n         )\n         )\n         \n-    def reset(self):\n+    def reset(self, seed=None, options=None):\n         # self.qpos = np.zeros(self.k)\n         # qvel = np.zeros(self.k)\n         # self.sim_state.qpos[0:self.k], self.sim_state.qvel[0:self.k]\n         self.controller.initialize()\n"
                },
                {
                    "date": 1691647537407,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,8 +1,8 @@\n from tokenize import Double\n import gymnasium as gym\n import numpy as np\n-from gym import spaces\n+from gymnasium import spaces\n \n # from mujoco_py import load_model_from_path, MjSim, MjViewer\n # import mujoco_py\n import mujoco\n"
                },
                {
                    "date": 1691647549501,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -210,9 +210,9 @@\n             self.temp_action, # 20\n         )\n         )\n         \n-    def reset(self, seed=None, options=None):\n+    def reset(self):\n         # self.qpos = np.zeros(self.k)\n         # qvel = np.zeros(self.k)\n         # self.sim_state.qpos[0:self.k], self.sim_state.qvel[0:self.k]\n         self.controller.initialize()\n"
                },
                {
                    "date": 1691648150966,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -210,9 +210,9 @@\n             self.temp_action, # 20\n         )\n         )\n         \n-    def reset(self):\n+    def reset(self, seed=None, options=None):\n         # self.qpos = np.zeros(self.k)\n         # qvel = np.zeros(self.k)\n         # self.sim_state.qpos[0:self.k], self.sim_state.qvel[0:self.k]\n         self.controller.initialize()\n@@ -229,9 +229,9 @@\n         # self.temp_action = np.zeros(1)\n \n         mujoco.mj_step(self.model, self.data)\n         \n-        return self.get_observation()\n+        return self.get_observation(),info\n     \n     def reset_time(self):\n         self.end_time = self.data.time + self.motion_time\n     \n"
                },
                {
                    "date": 1691648174766,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -229,9 +229,9 @@\n         # self.temp_action = np.zeros(1)\n \n         mujoco.mj_step(self.model, self.data)\n         \n-        return self.get_observation(),info\n+        return self.get_observation()\n     \n     def reset_time(self):\n         self.end_time = self.data.time + self.motion_time\n     \n"
                },
                {
                    "date": 1691648385865,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -210,9 +210,9 @@\n             self.temp_action, # 20\n         )\n         )\n         \n-    def reset(self, seed=None, options=None):\n+    def reset(self, seed=None):\n         # self.qpos = np.zeros(self.k)\n         # qvel = np.zeros(self.k)\n         # self.sim_state.qpos[0:self.k], self.sim_state.qvel[0:self.k]\n         self.controller.initialize()\n"
                },
                {
                    "date": 1691648477361,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -210,9 +210,9 @@\n             self.temp_action, # 20\n         )\n         )\n         \n-    def reset(self, seed=None):\n+    def reset(self):\n         # self.qpos = np.zeros(self.k)\n         # qvel = np.zeros(self.k)\n         # self.sim_state.qpos[0:self.k], self.sim_state.qvel[0:self.k]\n         self.controller.initialize()\n"
                },
                {
                    "date": 1691649348653,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -210,9 +210,10 @@\n             self.temp_action, # 20\n         )\n         )\n         \n-    def reset(self):\n+    def reset(self, seed=None):\n+        self.seedNum = seed\n         # self.qpos = np.zeros(self.k)\n         # qvel = np.zeros(self.k)\n         # self.sim_state.qpos[0:self.k], self.sim_state.qvel[0:self.k]\n         self.controller.initialize()\n"
                },
                {
                    "date": 1691650052817,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -7,14 +7,15 @@\n # import mujoco_py\n import mujoco\n \n from mujoco import viewer\n+from gym import Wrapper, spaces\n import controller\n \n DEFAULT_SIZE = 300\n DEG2RAD = 3.14 / 180.0\n \n-class FrankaEnv(gym.Env):\n+class FrankaEnv(gym.Env,Wrapper):\n     def __init__(self):\n         # self.frame_skip = 1\n         self.k = 7  # for jacobian calculation\n         self.model_path = 'model/fr3.xml'\n"
                },
                {
                    "date": 1691650275939,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,21 +1,69 @@\n-from tokenize import Double\n import gymnasium as gym\n import numpy as np\n from gymnasium import spaces\n-\n-# from mujoco_py import load_model_from_path, MjSim, MjViewer\n-# import mujoco_py\n import mujoco\n-\n from mujoco import viewer\n-from gym import Wrapper, spaces\n import controller\n \n DEFAULT_SIZE = 300\n DEG2RAD = 3.14 / 180.0\n \n-class FrankaEnv(gym.Env,Wrapper):\n+import gymnasium as gym\n+import numpy as np\n+from gymnasium import spaces\n+\n+\n+class FrankaEnv(gym.Env):\n+    \"\"\"Custom Environment that follows gym interface.\"\"\"\n+\n+    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 30}\n+\n+    def __init__(self, arg1, arg2, ...):\n+        super().__init__()\n+        # Define action and observation space\n+        # They must be gym.spaces objects\n+        # Example when using discrete actions:\n+        self.action_space = spaces.Discrete(N_DISCRETE_ACTIONS)\n+        # Example for using image as input (channel-first; channel-last also works):\n+        self.observation_space = spaces.Box(low=0, high=255,\n+                                            shape=(N_CHANNELS, HEIGHT, WIDTH), dtype=np.uint8)\n+\n+    def step(self, action):\n+        ...\n+        return observation, reward, terminated, truncated, info\n+\n+    def reset(self, seed=None, options=None):\n+        ...\n+        return observation, info\n+\n+    def render(self):\n+        ...\n+\n+    def close(self):\n+        ...\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+class FrankaEnv(gym.Env):\n     def __init__(self):\n         # self.frame_skip = 1\n         self.k = 7  # for jacobian calculation\n         self.model_path = 'model/fr3.xml'\n"
                },
                {
                    "date": 1691650295922,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -17,9 +17,9 @@\n     \"\"\"Custom Environment that follows gym interface.\"\"\"\n \n     metadata = {\"render_modes\": [\"human\"], \"render_fps\": 30}\n \n-    def __init__(self, arg1, arg2, ...):\n+    def __init__(self):\n         super().__init__()\n         # Define action and observation space\n         # They must be gym.spaces objects\n         # Example when using discrete actions:\n"
                },
                {
                    "date": 1691650343219,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -19,52 +19,8 @@\n     metadata = {\"render_modes\": [\"human\"], \"render_fps\": 30}\n \n     def __init__(self):\n         super().__init__()\n-        # Define action and observation space\n-        # They must be gym.spaces objects\n-        # Example when using discrete actions:\n-        self.action_space = spaces.Discrete(N_DISCRETE_ACTIONS)\n-        # Example for using image as input (channel-first; channel-last also works):\n-        self.observation_space = spaces.Box(low=0, high=255,\n-                                            shape=(N_CHANNELS, HEIGHT, WIDTH), dtype=np.uint8)\n-\n-    def step(self, action):\n-        ...\n-        return observation, reward, terminated, truncated, info\n-\n-    def reset(self, seed=None, options=None):\n-        ...\n-        return observation, info\n-\n-    def render(self):\n-        ...\n-\n-    def close(self):\n-        ...\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-class FrankaEnv(gym.Env):\n-    def __init__(self):\n         # self.frame_skip = 1\n         self.k = 7  # for jacobian calculation\n         self.model_path = 'model/fr3.xml'\n         self.model = mujoco.MjModel.from_xml_path(self.model_path)\n@@ -128,8 +84,51 @@\n         self.data.qpos[3] = -1.1\n         self.data.qpos[5] = 30.0 * DEG2RAD\n         mujoco.mj_step(self.model, self.data)\n         \n+        self.action_space = spaces.Discrete(N_DISCRETE_ACTIONS)\n+        # Example for using image as input (channel-first; channel-last also works):\n+        self.observation_space = spaces.Box(low=0, high=255,\n+                                            shape=(N_CHANNELS, HEIGHT, WIDTH), dtype=np.uint8)\n+\n+    def step(self, action):\n+        ...\n+        return observation, reward, terminated, truncated, info\n+\n+    def reset(self, seed=None, options=None):\n+        ...\n+        return observation, info\n+\n+    def render(self):\n+        ...\n+\n+    def close(self):\n+        ...\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+class FrankaEnv(gym.Env):\n+    def __init__(self):\n+        \n+        \n         # self.metadata = {\n         #     \"render.modes\": [\"human\", \"rgb_array\", \"depth_array\"],\n         #     \"video.frames_per_second\": int(np.round(1.0 / self.dt)),\n         #     # \"video.frames_per_second\": int(1000),\n"
                },
                {
                    "date": 1691650365356,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -83,14 +83,14 @@\n \n         self.data.qpos[3] = -1.1\n         self.data.qpos[5] = 30.0 * DEG2RAD\n         mujoco.mj_step(self.model, self.data)\n-        \n-        self.action_space = spaces.Discrete(N_DISCRETE_ACTIONS)\n-        # Example for using image as input (channel-first; channel-last also works):\n-        self.observation_space = spaces.Box(low=0, high=255,\n-                                            shape=(N_CHANNELS, HEIGHT, WIDTH), dtype=np.uint8)\n \n+        # self.action_space = spaces.Discrete(N_DISCRETE_ACTIONS)\n+        # # Example for using image as input (channel-first; channel-last also works):\n+        # self.observation_space = spaces.Box(low=0, high=255,\n+        #                                     shape=(N_CHANNELS, HEIGHT, WIDTH), dtype=np.uint8)\n+\n     def step(self, action):\n         ...\n         return observation, reward, terminated, truncated, info\n \n"
                },
                {
                    "date": 1691650571852,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,26 +1,18 @@\n+from tokenize import Double\n import gymnasium as gym\n import numpy as np\n from gymnasium import spaces\n import mujoco\n+\n from mujoco import viewer\n import controller\n \n DEFAULT_SIZE = 300\n DEG2RAD = 3.14 / 180.0\n \n-import gymnasium as gym\n-import numpy as np\n-from gymnasium import spaces\n-\n-\n class FrankaEnv(gym.Env):\n-    \"\"\"Custom Environment that follows gym interface.\"\"\"\n-\n-    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 30}\n-\n     def __init__(self):\n-        super().__init__()\n         # self.frame_skip = 1\n         self.k = 7  # for jacobian calculation\n         self.model_path = 'model/fr3.xml'\n         self.model = mujoco.MjModel.from_xml_path(self.model_path)\n@@ -83,52 +75,9 @@\n \n         self.data.qpos[3] = -1.1\n         self.data.qpos[5] = 30.0 * DEG2RAD\n         mujoco.mj_step(self.model, self.data)\n-\n-        # self.action_space = spaces.Discrete(N_DISCRETE_ACTIONS)\n-        # # Example for using image as input (channel-first; channel-last also works):\n-        # self.observation_space = spaces.Box(low=0, high=255,\n-        #                                     shape=(N_CHANNELS, HEIGHT, WIDTH), dtype=np.uint8)\n-\n-    def step(self, action):\n-        ...\n-        return observation, reward, terminated, truncated, info\n-\n-    def reset(self, seed=None, options=None):\n-        ...\n-        return observation, info\n-\n-    def render(self):\n-        ...\n-\n-    def close(self):\n-        ...\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-class FrankaEnv(gym.Env):\n-    def __init__(self):\n         \n-        \n         # self.metadata = {\n         #     \"render.modes\": [\"human\", \"rgb_array\", \"depth_array\"],\n         #     \"video.frames_per_second\": int(np.round(1.0 / self.dt)),\n         #     # \"video.frames_per_second\": int(1000),\n"
                },
                {
                    "date": 1691650678482,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -226,10 +226,16 @@\n         self.timestep = 0\n         # self.temp_action = np.zeros(1)\n \n         mujoco.mj_step(self.model, self.data)\n+\n+        info = {\n+            'initial_reward': 0.0,  # 초기 보상 값\n+            'initial_state': self.current_state,  # 초기 상태 정보\n+            'custom_info': 'This is custom environment info'  # 기타 정보\n+        }\n         \n-        return self.get_observation()\n+        return self.get_observation(), info\n     \n     def reset_time(self):\n         self.end_time = self.data.time + self.motion_time\n     \n"
                },
                {
                    "date": 1691650699174,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -229,9 +229,9 @@\n         mujoco.mj_step(self.model, self.data)\n \n         info = {\n             'initial_reward': 0.0,  # 초기 보상 값\n-            'initial_state': self.current_state,  # 초기 상태 정보\n+            'initial_state': self.get_observation,  # 초기 상태 정보\n             'custom_info': 'This is custom environment info'  # 기타 정보\n         }\n         \n         return self.get_observation(), info\n"
                },
                {
                    "date": 1691650780860,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -144,9 +144,9 @@\n                         # print(\"!!??\")\n                         # mujoco.mj_step(self.model, self.data)\n                         return obs, sum_reward, done, dict()\n         \n-        return obs, sum_reward, done, dict()\n+        return obs, sum_reward, done, dict() {}\n     \n     def euler_to_quaternion(self, roll, pitch, yaw):\n         cy = np.cos(yaw * 0.5)\n         sy = np.sin(yaw * 0.5)\n@@ -227,15 +227,15 @@\n         # self.temp_action = np.zeros(1)\n \n         mujoco.mj_step(self.model, self.data)\n \n-        info = {\n-            'initial_reward': 0.0,  # 초기 보상 값\n-            'initial_state': self.get_observation,  # 초기 상태 정보\n-            'custom_info': 'This is custom environment info'  # 기타 정보\n-        }\n+        # info = {\n+        #     'initial_reward': 0.0,  # 초기 보상 값\n+        #     'initial_state': self.get_observation,  # 초기 상태 정보\n+        #     'custom_info': 'This is custom environment info'  # 기타 정보\n+        # }\n         \n-        return self.get_observation(), info\n+        return self.get_observation(), {}\n     \n     def reset_time(self):\n         self.end_time = self.data.time + self.motion_time\n     \n"
                },
                {
                    "date": 1691650789065,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -144,9 +144,9 @@\n                         # print(\"!!??\")\n                         # mujoco.mj_step(self.model, self.data)\n                         return obs, sum_reward, done, dict()\n         \n-        return obs, sum_reward, done, dict() {}\n+        return obs, sum_reward, done, dict(), {}\n     \n     def euler_to_quaternion(self, roll, pitch, yaw):\n         cy = np.cos(yaw * 0.5)\n         sy = np.sin(yaw * 0.5)\n"
                },
                {
                    "date": 1691650826026,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -122,29 +122,29 @@\n         for i in range(self.k) : \n             if(self.data.qpos[i] > self._max_joint_position[i]) :\n                 self.reset()\n                 sum_reward -= 1000\n-                return obs, sum_reward, done, dict()\n+                return obs, sum_reward, done, dict(), {}\n             if(self.data.qpos[i] < self._min_joint_position[i]) :\n                 self.reset()\n                 sum_reward -= 1000\n-                return obs, sum_reward, done, dict()\n+                return obs, sum_reward, done, dict(), {}\n \n         if self.data.time > self.end_time:\n             for i in range(6) :\n                 if (self.X_goal[i] - self.X[i]) > 0.01:\n                     self.reset()\n                     sum_reward -= 1000\n-                    return obs, sum_reward, done, dict()\n+                    return obs, sum_reward, done, dict(), {}\n                 else:\n                     if i == 5 :\n                         sum_reward += 1000\n                         # self.reset()\n                         self.controller.reset_goal()\n                         self.reset_time()\n                         # print(\"!!??\")\n                         # mujoco.mj_step(self.model, self.data)\n-                        return obs, sum_reward, done, dict()\n+                        return obs, sum_reward, done, dict(), {}\n         \n         return obs, sum_reward, done, dict(), {}\n     \n     def euler_to_quaternion(self, roll, pitch, yaw):\n"
                },
                {
                    "date": 1691653305831,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -88,9 +88,9 @@\n         \n     def step(self, action, render=True):\n         self.viewer.sync()\n         # print('step')\n-        sum_reward = 0\n+        \n         done = False\n         self.temp_action = action\n         action = action * 10\n         self.controller.read(self.data.time, self.data.qpos[0:self.k], self.data.qvel[0:self.k], action)\n"
                },
                {
                    "date": 1691653324353,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -88,9 +88,9 @@\n         \n     def step(self, action, render=True):\n         self.viewer.sync()\n         # print('step')\n-        \n+        sum_reward = 0\n         done = False\n         self.temp_action = action\n         action = action * 10\n         self.controller.read(self.data.time, self.data.qpos[0:self.k], self.data.qvel[0:self.k], action)\n"
                },
                {
                    "date": 1691653399332,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -114,9 +114,9 @@\n         for i in range(4):\n             self.data.qpos[12+i] = self.quaternion_goal[i]\n         obs =self.get_observation()\n         mujoco.mj_step(self.model, self.data)\n-        sum_reward += self.reward()\n+        sum_reward = self.reward()\n         # sum_reward = 0.0\n         # print(self.data.time)\n \n         for i in range(self.k) : \n"
                },
                {
                    "date": 1691654011678,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -130,9 +130,9 @@\n                 return obs, sum_reward, done, dict(), {}\n \n         if self.data.time > self.end_time:\n             for i in range(6) :\n-                if (self.X_goal[i] - self.X[i]) > 0.01:\n+                if (self.X_goal[i] - self.X[i]) > 0.001:\n                     self.reset()\n                     sum_reward -= 1000\n                     return obs, sum_reward, done, dict(), {}\n                 else:\n"
                },
                {
                    "date": 1691714842225,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -341,13 +341,13 @@\n         return np.concatenate([self.sim.data.qpos.flat, self.sim.data.qvel.flat])\n \n     def reward(self):\n         #Reaching Goal Poses (Cost Function)\n-        tmp_reward = 0\n+        # tmp_reward = 0\n         for i in range(6):\n             if self.X_goal[i] != self.X[i]:  # 분모가 0인 경우 처리\n-                tmp_reward += 1 / (self.X_goal[i] - self.X[i])\n-        reward = tmp_reward * 100\n+                reward += 1 / (self.X_goal[i] - self.X[i])\n+        # reward = tmp_reward\n         return reward\n     \n     def seed(self, seed=None):\n         return \n\\ No newline at end of file\n"
                },
                {
                    "date": 1691714854188,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -341,13 +341,13 @@\n         return np.concatenate([self.sim.data.qpos.flat, self.sim.data.qvel.flat])\n \n     def reward(self):\n         #Reaching Goal Poses (Cost Function)\n-        # tmp_reward = 0\n+        tmp_reward = 0\n         for i in range(6):\n             if self.X_goal[i] != self.X[i]:  # 분모가 0인 경우 처리\n-                reward += 1 / (self.X_goal[i] - self.X[i])\n-        # reward = tmp_reward\n+                tmp_reward += 1 / (self.X_goal[i] - self.X[i])\n+        reward = tmp_reward\n         return reward\n     \n     def seed(self, seed=None):\n         return \n\\ No newline at end of file\n"
                },
                {
                    "date": 1691714886960,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -183,18 +183,18 @@\n     \n     def get_observation(self):\n         # self.sim_state = self.sim.get_state()\n         \n-        self.position[0] = self.X[0] / 0.75\n-        self.position[1] = self.X[1] / 0.75\n-        self.position[2] = self.X[2] / 0.75\n+        self.position[0] = self.X[0] / 0.6\n+        self.position[1] = self.X[1] / 0.6\n+        self.position[2] = self.X[2] / 0.6\n         self.position[3] = self.X[3] / 3.14\n         self.position[4] = self.X[4] / 3.14\n         self.position[5] = self.X[5] / 3.14\n \n-        self.goal[0] = self.X_goal[0] / 0.75\n-        self.goal[1] = self.X_goal[1] / 0.75\n-        self.goal[2] = self.X_goal[2] / 0.75\n+        self.goal[0] = self.X_goal[0] / 0.6\n+        self.goal[1] = self.X_goal[1] / 0.6\n+        self.goal[2] = self.X_goal[2] / 0.6\n         self.goal[3] = self.X_goal[3] / 3.14\n         self.goal[4] = self.X_goal[4] / 3.14\n         self.goal[5] = self.X_goal[5] / 3.14\n         # goal = # controller로부터 받은 goal\n"
                },
                {
                    "date": 1691715035372,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -185,9 +185,11 @@\n         # self.sim_state = self.sim.get_state()\n         \n         self.position[0] = self.X[0] / 0.6\n         self.position[1] = self.X[1] / 0.6\n-        self.position[2] = self.X[2] / 0.6\n+        self.position[2] = self.X[2] -0.35\n+        self.position[2] = self.position[2] / 0.45\n+\n         self.position[3] = self.X[3] / 3.14\n         self.position[4] = self.X[4] / 3.14\n         self.position[5] = self.X[5] / 3.14\n \n"
                },
                {
                    "date": 1691715110073,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -183,20 +183,22 @@\n     \n     def get_observation(self):\n         # self.sim_state = self.sim.get_state()\n         \n-        self.position[0] = self.X[0] / 0.6\n-        self.position[1] = self.X[1] / 0.6\n-        self.position[2] = self.X[2] -0.35\n+        self.position[0] = self.X[0] / 0.6 # -0.6~0.6\n+        self.position[1] = self.X[1] / 0.6 # -0.6~0.6\n+        self.position[2] = self.X[2] -0.35 # 0.1~0.8\n         self.position[2] = self.position[2] / 0.45\n \n-        self.position[3] = self.X[3] / 3.14\n-        self.position[4] = self.X[4] / 3.14\n-        self.position[5] = self.X[5] / 3.14\n+        self.position[3] = self.X[3] / 3.14 # -3.14~3.14\n+        self.position[4] = self.X[4] / 3.14 #      \"\n+        self.position[5] = self.X[5] / 3.14 #      \"\n \n         self.goal[0] = self.X_goal[0] / 0.6\n         self.goal[1] = self.X_goal[1] / 0.6\n-        self.goal[2] = self.X_goal[2] / 0.6\n+        self.goal[2] = self.X_goal[2] -0.35\n+        self.goal[2] = self.goal[2] / 0.45\n+\n         self.goal[3] = self.X_goal[3] / 3.14\n         self.goal[4] = self.X_goal[4] / 3.14\n         self.goal[5] = self.X_goal[5] / 3.14\n         # goal = # controller로부터 받은 goal\n"
                },
                {
                    "date": 1691715170255,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -197,9 +197,9 @@\n         self.goal[1] = self.X_goal[1] / 0.6\n         self.goal[2] = self.X_goal[2] -0.35\n         self.goal[2] = self.goal[2] / 0.45\n \n-        self.goal[3] = self.X_goal[3] / 3.14\n+        self.goal[3] = self.X_goal[3] / PI\n         self.goal[4] = self.X_goal[4] / 3.14\n         self.goal[5] = self.X_goal[5] / 3.14\n         # goal = # controller로부터 받은 goal\n         # self.temp_action = self.temp_action * 10\n"
                },
                {
                    "date": 1691715218108,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -7,9 +7,9 @@\n from mujoco import viewer\n import controller\n \n DEFAULT_SIZE = 300\n-DEG2RAD = 3.14 / 180.0\n+DEG2RAD = np.pi / 180.0\n \n class FrankaEnv(gym.Env):\n     def __init__(self):\n         # self.frame_skip = 1\n"
                },
                {
                    "date": 1691715245734,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -188,20 +188,20 @@\n         self.position[1] = self.X[1] / 0.6 # -0.6~0.6\n         self.position[2] = self.X[2] -0.35 # 0.1~0.8\n         self.position[2] = self.position[2] / 0.45\n \n-        self.position[3] = self.X[3] / 3.14 # -3.14~3.14\n-        self.position[4] = self.X[4] / 3.14 #      \"\n-        self.position[5] = self.X[5] / 3.14 #      \"\n+        self.position[3] = self.X[3] / np.pi # -np.pi~np.pi\n+        self.position[4] = self.X[4] / np.pi #      \"\n+        self.position[5] = self.X[5] / np.pi #      \"\n \n         self.goal[0] = self.X_goal[0] / 0.6\n         self.goal[1] = self.X_goal[1] / 0.6\n         self.goal[2] = self.X_goal[2] -0.35\n         self.goal[2] = self.goal[2] / 0.45\n \n-        self.goal[3] = self.X_goal[3] / PI\n-        self.goal[4] = self.X_goal[4] / 3.14\n-        self.goal[5] = self.X_goal[5] / 3.14\n+        self.goal[3] = self.X_goal[3] / np.pi\n+        self.goal[4] = self.X_goal[4] / np.pi\n+        self.goal[5] = self.X_goal[5] / np.pi\n         # goal = # controller로부터 받은 goal\n         # self.temp_action = self.temp_action * 10\n         # self.temp_action = self.temp_action - 1\n         return np.concatenate(\n"
                },
                {
                    "date": 1691715296274,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -166,16 +166,16 @@\n         # return np.array([w, x, y, z])\n \n     def _construct_action_space(self):\n         # action_low = -1 * np.ones(12)\n-        action_low =  np.zeros(1)\n-        action_high = np.ones(1) \n+        action_low =  np.zeros(2)\n+        action_high = np.ones(2) \n         return gym.spaces.Box(low=action_low, high=action_high, dtype=np.float64)\n     \n     \n     def _construct_observation_space(self):\n-        obs_low = -1 * np.ones(13) # X(6), X_goal(6), action(1) \n-        obs_high = 1 * np.ones(13)\n+        obs_low = -1 * np.ones(14) # X(6), X_goal(6), action(2) \n+        obs_high = 1 * np.ones(14)\n         # obs_low = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0,0,0, -1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1]\n         # obs_high = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0,0,0, 1,1,1,1,1,1,1,1,1,1,1,1]\n         return gym.spaces.Box(obs_low, obs_high, dtype=np.float64)\n     \n"
                },
                {
                    "date": 1691715301419,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -207,9 +207,9 @@\n         return np.concatenate(\n         (\n             self.position, #6\n             self.goal, #6\n-            self.temp_action, # 20\n+            self.temp_action, # 2\n         )\n         )\n         \n     def reset(self, seed=None):\n"
                },
                {
                    "date": 1691715323242,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -33,9 +33,9 @@\n         self.goal = np.zeros(6)\n         # self.reward_arr = np.zeros(2) #shape=(2,), dtype=np.float64\n         self.reward_arr = [0,0,0]\n         self.cnt = 1\n-        self.temp_action = np.zeros(1)\n+        self.temp_action = np.zeros(2)\n         self._max_joint_position = np.zeros(self.k)\n         self._min_joint_position = np.zeros(self.k)\n         self.timestep = 0\n \n"
                },
                {
                    "date": 1691715823990,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -347,9 +347,11 @@\n     def reward(self):\n         #Reaching Goal Poses (Cost Function)\n         tmp_reward = 0\n         for i in range(6):\n-            if self.X_goal[i] != self.X[i]:  # 분모가 0인 경우 처리\n+            if self.X_goal[i] == self.X[i]:  # 분모가 0인 경우 처리\n+                tmp_reward += 1000\n+            else:\n                 tmp_reward += 1 / (self.X_goal[i] - self.X[i])\n         reward = tmp_reward\n         return reward\n     \n"
                },
                {
                    "date": 1691723091262,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -130,9 +130,9 @@\n                 return obs, sum_reward, done, dict(), {}\n \n         if self.data.time > self.end_time:\n             for i in range(6) :\n-                if (self.X_goal[i] - self.X[i]) > 0.001:\n+                if (self.X_goal[i] - self.X[i]) >= 0.005:\n                     self.reset()\n                     sum_reward -= 1000\n                     return obs, sum_reward, done, dict(), {}\n                 else:\n"
                },
                {
                    "date": 1691723420101,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -130,9 +130,9 @@\n                 return obs, sum_reward, done, dict(), {}\n \n         if self.data.time > self.end_time:\n             for i in range(6) :\n-                if (self.X_goal[i] - self.X[i]) >= 0.005:\n+                if (self.X_goal[i] - self.X[i]) >= 0.01:\n                     self.reset()\n                     sum_reward -= 1000\n                     return obs, sum_reward, done, dict(), {}\n                 else:\n"
                },
                {
                    "date": 1691740196026,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,360 @@\n+from tokenize import Double\n+import gymnasium as gym\n+import numpy as np\n+from gymnasium import spaces\n+import mujoco\n+\n+from mujoco import viewer\n+import controller\n+\n+DEFAULT_SIZE = 300\n+DEG2RAD = np.pi / 180.0\n+\n+class FrankaEnv(gym.Env):\n+    def __init__(self):\n+        # self.frame_skip = 1\n+        self.k = 7  # for jacobian calculation\n+        self.model_path = 'model/fr3.xml'\n+        self.model = mujoco.MjModel.from_xml_path(self.model_path)\n+        self.data = mujoco.MjData(self.model)\n+\n+        self.viewer = viewer.launch_passive(model=self.model, data=self.data)\n+        self.duration = 380  # (seconds)\n+        self.framerate = 10  # (Hz)\n+\n+        self.controller = controller.CController()\n+        self.torque = np.zeros(self.k)\n+        self.tmp_X = np.zeros(12)\n+        self.X_goal = np.zeros(6)\n+        self.X = np.zeros(6)\n+        self.quaternion_goal = np.zeros(4)\n+\n+        self.position = np.zeros(6)\n+        self.goal = np.zeros(6)\n+        # self.reward_arr = np.zeros(2) #shape=(2,), dtype=np.float64\n+        self.reward_arr = [0,0,0]\n+        self.cnt = 1\n+        self.temp_action = np.zeros(2)\n+        self._max_joint_position = np.zeros(self.k)\n+        self._min_joint_position = np.zeros(self.k)\n+        self.timestep = 0\n+\n+        self._min_joint_position[0] = -2.8973\n+        self._min_joint_position[1] = -1.7628\n+        self._min_joint_position[2] = -2.8973\n+        self._min_joint_position[3] = -3.0718\n+        self._min_joint_position[4] = -2.8973\n+        self._min_joint_position[5] = -0.0175\n+        self._min_joint_position[6] = -2.8973\n+\n+        self._max_joint_position[0] = 2.8973\n+        self._max_joint_position[1] = 1.7628\n+        self._max_joint_position[2] = 2.8973\n+        self._max_joint_position[3] = -0.0698\n+        self._max_joint_position[4] = 2.8973\n+        self._max_joint_position[5] = 3.7525\n+        self._max_joint_position[6] = 2.8973\n+\n+        self._max_joint_position_hat = np.zeros(self.k)\n+        self._min_joint_position_hat = np.zeros(self.k)\n+\n+        for i in range(self.k):\n+            self._max_joint_position_hat[i] = self._max_joint_position[i] - 0.1*(self._max_joint_position[i] - self._min_joint_position[i])\n+            self._min_joint_position_hat[i] = self._min_joint_position[i] + 0.1*(self._max_joint_position[i] - self._min_joint_position[i])    \n+        \n+        self.observation_space = self._construct_observation_space()\n+        self.action_space = self._construct_action_space()\n+        self.start_time = 0\n+        self.motion_time = 10\n+        self.end_time = self.start_time + self.motion_time\n+\n+\n+        for i in range(self.k):\n+            self.data.qpos[i] = 0.0\n+            self.data.qvel[i] = 0.0\n+\n+        self.data.qpos[3] = -1.1\n+        self.data.qpos[5] = 30.0 * DEG2RAD\n+        mujoco.mj_step(self.model, self.data)\n+        \n+        # self.metadata = {\n+        #     \"render.modes\": [\"human\", \"rgb_array\", \"depth_array\"],\n+        #     \"video.frames_per_second\": int(np.round(1.0 / self.dt)),\n+        #     # \"video.frames_per_second\": int(1000),\n+        # }\n+        \n+        # self.sim_state = self.sim.get_state()\n+        \n+        \n+    def step(self, action, render=True):\n+        self.viewer.sync()\n+        # print('step')\n+        sum_reward = 0\n+        done = False\n+        self.temp_action = action\n+        action = action * 2\n+        action = action + 0.003\n+        self.controller.read(self.data.time, self.data.qpos[0:self.k], self.data.qvel[0:self.k], action)\n+        self.controller.control_mujoco()\n+        self.tmp_X = self.controller.state_controller()\n+        self.torque = self.controller.write()\n+        for i in range(self.k):\n+                self.data.ctrl[i] = self.torque[i]\n+\n+        for i in range(6):\n+            self.X_goal[i] = self.tmp_X[i]\n+            # self.data.qpos[0:self.k]\n+        for i in range(6):\n+            self.X[i] = self.tmp_X[i+6]\n+        self.euler_to_quaternion(self.X_goal[3],self.X_goal[4],self.X_goal[5])\n+        # print(self.data.qpos)\n+        # print(self.data.xpos[13,0])\n+        # xpos랑 xmat을 건드려야함\n+        for i in range(3):\n+            self.data.qpos[9+i] = self.X_goal[i]\n+        for i in range(4):\n+            self.data.qpos[12+i] = self.quaternion_goal[i]\n+        obs =self.get_observation()\n+        mujoco.mj_step(self.model, self.data)\n+        sum_reward = self.reward()\n+        # sum_reward = 0.0\n+        # print(self.data.time)\n+\n+        for i in range(self.k) : \n+            if(self.data.qpos[i] > self._max_joint_position[i]) :\n+                self.reset()\n+                sum_reward -= 1000\n+                return obs, sum_reward, done, dict(), {}\n+            if(self.data.qpos[i] < self._min_joint_position[i]) :\n+                self.reset()\n+                sum_reward -= 1000\n+                return obs, sum_reward, done, dict(), {}\n+\n+        if self.data.time > self.end_time:\n+            for i in range(6) :\n+                if (self.X_goal[i] - self.X[i]) >= 0.01:\n+                    self.reset()\n+                    sum_reward -= 1000\n+                    return obs, sum_reward, done, dict(), {}\n+                else:\n+                    if i == 5 :\n+                        sum_reward += 1000\n+                        # self.reset()\n+                        self.controller.reset_goal()\n+                        self.reset_time()\n+                        # print(\"!!??\")\n+                        # mujoco.mj_step(self.model, self.data)\n+                        return obs, sum_reward, done, dict(), {}\n+        \n+        return obs, sum_reward, done, dict(), {}\n+    \n+    def euler_to_quaternion(self, roll, pitch, yaw):\n+        cy = np.cos(yaw * 0.5)\n+        sy = np.sin(yaw * 0.5)\n+        cp = np.cos(pitch * 0.5)\n+        sp = np.sin(pitch * 0.5)\n+        cr = np.cos(roll * 0.5)\n+        sr = np.sin(roll * 0.5)\n+        \n+        w = cr * cp * cy + sr * sp * sy\n+        x = sr * cp * cy - cr * sp * sy\n+        y = cr * sp * cy + sr * cp * sy\n+        z = cr * cp * sy - sr * sp * cy\n+        self.quaternion_goal[0] = w\n+        self.quaternion_goal[1] = x\n+        self.quaternion_goal[2] = y\n+        self.quaternion_goal[3] = z\n+        # return np.array([w, x, y, z])\n+\n+    def _construct_action_space(self):\n+        # action_low = -1 * np.ones(12)\n+        action_low =  np.zeros(1)\n+        action_high = np.ones(1) \n+        return gym.spaces.Box(low=action_low, high=action_high, dtype=np.float64)\n+    \n+    \n+    def _construct_observation_space(self):\n+        obs_low = -1 * np.ones(14) # X(6), X_goal(6), action(2) \n+        obs_high = 1 * np.ones(14)\n+        # obs_low = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0,0,0, -1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1]\n+        # obs_high = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0,0,0, 1,1,1,1,1,1,1,1,1,1,1,1]\n+        return gym.spaces.Box(obs_low, obs_high, dtype=np.float64)\n+    \n+     \n+    \n+    def get_observation(self):\n+        # self.sim_state = self.sim.get_state()\n+        \n+        self.position[0] = self.X[0] / 0.6 # -0.6~0.6\n+        self.position[1] = self.X[1] / 0.6 # -0.6~0.6\n+        self.position[2] = self.X[2] -0.35 # 0.1~0.8\n+        self.position[2] = self.position[2] / 0.45\n+\n+        self.position[3] = self.X[3] / np.pi # -np.pi~np.pi\n+        self.position[4] = self.X[4] / np.pi #      \"\n+        self.position[5] = self.X[5] / np.pi #      \"\n+\n+        self.goal[0] = self.X_goal[0] / 0.6\n+        self.goal[1] = self.X_goal[1] / 0.6\n+        self.goal[2] = self.X_goal[2] -0.35\n+        self.goal[2] = self.goal[2] / 0.45\n+\n+        self.goal[3] = self.X_goal[3] / np.pi\n+        self.goal[4] = self.X_goal[4] / np.pi\n+        self.goal[5] = self.X_goal[5] / np.pi\n+        # goal = # controller로부터 받은 goal\n+        # self.temp_action = self.temp_action * 10\n+        # self.temp_action = self.temp_action - 1\n+        return np.concatenate(\n+        (\n+            self.position, #6\n+            self.goal, #6\n+            self.temp_action, # 1\n+        )\n+        )\n+        \n+    def reset(self, seed=None):\n+        self.seedNum = seed\n+        # self.qpos = np.zeros(self.k)\n+        # qvel = np.zeros(self.k)\n+        # self.sim_state.qpos[0:self.k], self.sim_state.qvel[0:self.k]\n+        self.controller.initialize()\n+        self.end_time = self.data.time + self.motion_time\n+\n+        for i in range(self.k):\n+            self.data.qpos[i] = 0.0\n+            self.data.qvel[i] = 0.0\n+\n+        self.data.qpos[3] = -1.1\n+        self.data.qpos[5] = 30.0 * DEG2RAD\n+\n+        self.timestep = 0\n+        # self.temp_action = np.zeros(1)\n+\n+        mujoco.mj_step(self.model, self.data)\n+\n+        # info = {\n+        #     'initial_reward': 0.0,  # 초기 보상 값\n+        #     'initial_state': self.get_observation,  # 초기 상태 정보\n+        #     'custom_info': 'This is custom environment info'  # 기타 정보\n+        # }\n+        \n+        return self.get_observation(), {}\n+    \n+    def reset_time(self):\n+        self.end_time = self.data.time + self.motion_time\n+    \n+    \n+    \n+    def set_state(self, qpos, qvel):\n+        old_state = self.sim.get_state()\n+        new_state = mujoco.MjSimState(\n+            old_state.time, qpos, qvel, old_state.act, old_state.udd_state\n+        )\n+        self.sim.set_state(new_state)\n+        self.sim.forward()\n+    \n+    @property\n+    def dt(self):\n+        return self.model.opt.timestep * self.frame_skip\n+\n+    # def do_simulation(self, ctrl, n_frames):\n+    #     self.sim.data.ctrl[:] = ctrl\n+    #     self.sim.step()\n+    #     # for _ in range(n_frames):\n+    #     #     self.sim.step()\n+    \n+    \n+    def render(\n+        self,\n+        mode=\"human\",\n+        width=DEFAULT_SIZE,\n+        height=DEFAULT_SIZE,\n+        camera_id=None,\n+        camera_name=None,\n+    ):\n+        if mode == \"rgb_array\" or mode == \"depth_array\":\n+            if camera_id is not None and camera_name is not None:\n+                raise ValueError(\n+                    \"Both `camera_id` and `camera_name` cannot be\"\n+                    \" specified at the same time.\"\n+                )\n+\n+            no_camera_specified = camera_name is None and camera_id is None\n+            if no_camera_specified:\n+                camera_name = \"track\"\n+\n+            if camera_id is None and camera_name in self.model._camera_name2id:\n+                camera_id = self.model.camera_name2id(camera_name)\n+\n+            self._get_viewer(mode).render(width, height, camera_id=camera_id)\n+\n+        if mode == \"rgb_array\":\n+            # window size used for old mujoco-py:\n+            data = self._get_viewer(mode).read_pixels(width, height, depth=False)\n+            # original image is upside-down, so flip it\n+            return data[::-1, :, :]\n+        elif mode == \"depth_array\":\n+            self._get_viewer(mode).render(width, height)\n+            # window size used for old mujoco-py:\n+            # Extract depth part of the read_pixels() tuple\n+            data = self._get_viewer(mode).read_pixels(width, height, depth=True)[1]\n+            # original image is upside-down, so flip it\n+            return data[::-1, :]\n+        elif mode == \"human\":\n+            self._get_viewer(mode).render()\n+            \n+            \n+            \n+    def viewer_setup(self):\n+        \"\"\"\n+        This method is called when the viewer is initialized.\n+        Optionally implement this method, if you need to tinker with camera position\n+        and so forth.\n+        \"\"\"\n+        self.viewer.cam.trackbodyid = 1   #id of the body to track()\n+        self.viewer.cam.distance = self.model.stat.extent * 1.5 #how much zoom in\n+        self.viewer.cam.lookat[0] -= 0 #offset x\n+        self.viewer.cam.lookat[1] -= 0 #offset y\n+        self.viewer.cam.lookat[2] += 0 #offset z\n+        self.viewer.cam.elevation = 0   #cam rotation around the axis in the plane going throug the frame origin\n+\n+        pass\n+\n+    def close(self):\n+        if self.viewer is not None:\n+            # self.viewer.finish()\n+            self.viewer = None\n+            self._viewers = {}\n+\n+    def _get_viewer(self, mode):\n+        self.viewer = self._viewers.get(mode)\n+        if self.viewer is None:\n+            if mode == \"human\":\n+                self.viewer = mujoco.MjViewer(self.sim)\n+            elif mode == \"rgb_array\" or mode == \"depth_array\":\n+                self.viewer = mujoco.MjRenderContextOffscreen(self.sim, -1)\n+\n+            self.viewer_setup()\n+            self._viewers[mode] = self.viewer\n+        return self.viewer\n+\n+    def get_body_com(self, body_name):\n+        return self.data.get_body_xpos(body_name)\n+\n+    def state_vector(self):\n+        return np.concatenate([self.sim.data.qpos.flat, self.sim.data.qvel.flat])\n+\n+    def reward(self):\n+        #Reaching Goal Poses (Cost Function)\n+        tmp_reward = 0\n+        for i in range(6):\n+            if self.X_goal[i] == self.X[i]:  # 분모가 0인 경우 처리\n+                tmp_reward += 1000\n+            else:\n+                tmp_reward += 1 / (self.X_goal[i] - self.X[i])\n+        reward = tmp_reward\n+        return reward\n+    \n+    def seed(self, seed=None):\n+        return \n\\ No newline at end of file\n"
                },
                {
                    "date": 1691740229837,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -167,9 +167,9 @@\n         # return np.array([w, x, y, z])\n \n     def _construct_action_space(self):\n         # action_low = -1 * np.ones(12)\n-        action_low =  np.zeros(1)\n+        action_low =  -1 * np.ones(1)\n         action_high = np.ones(1) \n         return gym.spaces.Box(low=action_low, high=action_high, dtype=np.float64)\n     \n     \n@@ -356,364 +356,5 @@\n         reward = tmp_reward\n         return reward\n     \n     def seed(self, seed=None):\n-        return \n-from tokenize import Double\n-import gymnasium as gym\n-import numpy as np\n-from gymnasium import spaces\n-import mujoco\n-\n-from mujoco import viewer\n-import controller\n-\n-DEFAULT_SIZE = 300\n-DEG2RAD = np.pi / 180.0\n-\n-class FrankaEnv(gym.Env):\n-    def __init__(self):\n-        # self.frame_skip = 1\n-        self.k = 7  # for jacobian calculation\n-        self.model_path = 'model/fr3.xml'\n-        self.model = mujoco.MjModel.from_xml_path(self.model_path)\n-        self.data = mujoco.MjData(self.model)\n-\n-        self.viewer = viewer.launch_passive(model=self.model, data=self.data)\n-        self.duration = 380  # (seconds)\n-        self.framerate = 10  # (Hz)\n-\n-        self.controller = controller.CController()\n-        self.torque = np.zeros(self.k)\n-        self.tmp_X = np.zeros(12)\n-        self.X_goal = np.zeros(6)\n-        self.X = np.zeros(6)\n-        self.quaternion_goal = np.zeros(4)\n-\n-        self.position = np.zeros(6)\n-        self.goal = np.zeros(6)\n-        # self.reward_arr = np.zeros(2) #shape=(2,), dtype=np.float64\n-        self.reward_arr = [0,0,0]\n-        self.cnt = 1\n-        self.temp_action = np.zeros(2)\n-        self._max_joint_position = np.zeros(self.k)\n-        self._min_joint_position = np.zeros(self.k)\n-        self.timestep = 0\n-\n-        self._min_joint_position[0] = -2.8973\n-        self._min_joint_position[1] = -1.7628\n-        self._min_joint_position[2] = -2.8973\n-        self._min_joint_position[3] = -3.0718\n-        self._min_joint_position[4] = -2.8973\n-        self._min_joint_position[5] = -0.0175\n-        self._min_joint_position[6] = -2.8973\n-\n-        self._max_joint_position[0] = 2.8973\n-        self._max_joint_position[1] = 1.7628\n-        self._max_joint_position[2] = 2.8973\n-        self._max_joint_position[3] = -0.0698\n-        self._max_joint_position[4] = 2.8973\n-        self._max_joint_position[5] = 3.7525\n-        self._max_joint_position[6] = 2.8973\n-\n-        self._max_joint_position_hat = np.zeros(self.k)\n-        self._min_joint_position_hat = np.zeros(self.k)\n-\n-        for i in range(self.k):\n-            self._max_joint_position_hat[i] = self._max_joint_position[i] - 0.1*(self._max_joint_position[i] - self._min_joint_position[i])\n-            self._min_joint_position_hat[i] = self._min_joint_position[i] + 0.1*(self._max_joint_position[i] - self._min_joint_position[i])    \n-        \n-        self.observation_space = self._construct_observation_space()\n-        self.action_space = self._construct_action_space()\n-        self.start_time = 0\n-        self.motion_time = 10\n-        self.end_time = self.start_time + self.motion_time\n-\n-\n-        for i in range(self.k):\n-            self.data.qpos[i] = 0.0\n-            self.data.qvel[i] = 0.0\n-\n-        self.data.qpos[3] = -1.1\n-        self.data.qpos[5] = 30.0 * DEG2RAD\n-        mujoco.mj_step(self.model, self.data)\n-        \n-        # self.metadata = {\n-        #     \"render.modes\": [\"human\", \"rgb_array\", \"depth_array\"],\n-        #     \"video.frames_per_second\": int(np.round(1.0 / self.dt)),\n-        #     # \"video.frames_per_second\": int(1000),\n-        # }\n-        \n-        # self.sim_state = self.sim.get_state()\n-        \n-        \n-    def step(self, action, render=True):\n-        self.viewer.sync()\n-        # print('step')\n-        sum_reward = 0\n-        done = False\n-        self.temp_action = action\n-        action = action * 10\n-        self.controller.read(self.data.time, self.data.qpos[0:self.k], self.data.qvel[0:self.k], action)\n-        self.controller.control_mujoco()\n-        self.tmp_X = self.controller.state_controller()\n-        self.torque = self.controller.write()\n-        for i in range(self.k):\n-                self.data.ctrl[i] = self.torque[i]\n-\n-        for i in range(6):\n-            self.X_goal[i] = self.tmp_X[i]\n-            # self.data.qpos[0:self.k]\n-        for i in range(6):\n-            self.X[i] = self.tmp_X[i+6]\n-        self.euler_to_quaternion(self.X_goal[3],self.X_goal[4],self.X_goal[5])\n-        # print(self.data.qpos)\n-        # print(self.data.xpos[13,0])\n-        # xpos랑 xmat을 건드려야함\n-        for i in range(3):\n-            self.data.qpos[9+i] = self.X_goal[i]\n-        for i in range(4):\n-            self.data.qpos[12+i] = self.quaternion_goal[i]\n-        obs =self.get_observation()\n-        mujoco.mj_step(self.model, self.data)\n-        sum_reward = self.reward()\n-        # sum_reward = 0.0\n-        # print(self.data.time)\n-\n-        for i in range(self.k) : \n-            if(self.data.qpos[i] > self._max_joint_position[i]) :\n-                self.reset()\n-                sum_reward -= 1000\n-                return obs, sum_reward, done, dict(), {}\n-            if(self.data.qpos[i] < self._min_joint_position[i]) :\n-                self.reset()\n-                sum_reward -= 1000\n-                return obs, sum_reward, done, dict(), {}\n-\n-        if self.data.time > self.end_time:\n-            for i in range(6) :\n-                if (self.X_goal[i] - self.X[i]) >= 0.01:\n-                    self.reset()\n-                    sum_reward -= 1000\n-                    return obs, sum_reward, done, dict(), {}\n-                else:\n-                    if i == 5 :\n-                        sum_reward += 1000\n-                        # self.reset()\n-                        self.controller.reset_goal()\n-                        self.reset_time()\n-                        # print(\"!!??\")\n-                        # mujoco.mj_step(self.model, self.data)\n-                        return obs, sum_reward, done, dict(), {}\n-        \n-        return obs, sum_reward, done, dict(), {}\n-    \n-    def euler_to_quaternion(self, roll, pitch, yaw):\n-        cy = np.cos(yaw * 0.5)\n-        sy = np.sin(yaw * 0.5)\n-        cp = np.cos(pitch * 0.5)\n-        sp = np.sin(pitch * 0.5)\n-        cr = np.cos(roll * 0.5)\n-        sr = np.sin(roll * 0.5)\n-        \n-        w = cr * cp * cy + sr * sp * sy\n-        x = sr * cp * cy - cr * sp * sy\n-        y = cr * sp * cy + sr * cp * sy\n-        z = cr * cp * sy - sr * sp * cy\n-        self.quaternion_goal[0] = w\n-        self.quaternion_goal[1] = x\n-        self.quaternion_goal[2] = y\n-        self.quaternion_goal[3] = z\n-        # return np.array([w, x, y, z])\n-\n-    def _construct_action_space(self):\n-        # action_low = -1 * np.ones(12)\n-        action_low =  np.zeros(2)\n-        action_high = np.ones(2) \n-        return gym.spaces.Box(low=action_low, high=action_high, dtype=np.float64)\n-    \n-    \n-    def _construct_observation_space(self):\n-        obs_low = -1 * np.ones(14) # X(6), X_goal(6), action(2) \n-        obs_high = 1 * np.ones(14)\n-        # obs_low = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0,0,0, -1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1]\n-        # obs_high = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0,0,0, 1,1,1,1,1,1,1,1,1,1,1,1]\n-        return gym.spaces.Box(obs_low, obs_high, dtype=np.float64)\n-    \n-     \n-    \n-    def get_observation(self):\n-        # self.sim_state = self.sim.get_state()\n-        \n-        self.position[0] = self.X[0] / 0.6 # -0.6~0.6\n-        self.position[1] = self.X[1] / 0.6 # -0.6~0.6\n-        self.position[2] = self.X[2] -0.35 # 0.1~0.8\n-        self.position[2] = self.position[2] / 0.45\n-\n-        self.position[3] = self.X[3] / np.pi # -np.pi~np.pi\n-        self.position[4] = self.X[4] / np.pi #      \"\n-        self.position[5] = self.X[5] / np.pi #      \"\n-\n-        self.goal[0] = self.X_goal[0] / 0.6\n-        self.goal[1] = self.X_goal[1] / 0.6\n-        self.goal[2] = self.X_goal[2] -0.35\n-        self.goal[2] = self.goal[2] / 0.45\n-\n-        self.goal[3] = self.X_goal[3] / np.pi\n-        self.goal[4] = self.X_goal[4] / np.pi\n-        self.goal[5] = self.X_goal[5] / np.pi\n-        # goal = # controller로부터 받은 goal\n-        # self.temp_action = self.temp_action * 10\n-        # self.temp_action = self.temp_action - 1\n-        return np.concatenate(\n-        (\n-            self.position, #6\n-            self.goal, #6\n-            self.temp_action, # 2\n-        )\n-        )\n-        \n-    def reset(self, seed=None):\n-        self.seedNum = seed\n-        # self.qpos = np.zeros(self.k)\n-        # qvel = np.zeros(self.k)\n-        # self.sim_state.qpos[0:self.k], self.sim_state.qvel[0:self.k]\n-        self.controller.initialize()\n-        self.end_time = self.data.time + self.motion_time\n-\n-        for i in range(self.k):\n-            self.data.qpos[i] = 0.0\n-            self.data.qvel[i] = 0.0\n-\n-        self.data.qpos[3] = -1.1\n-        self.data.qpos[5] = 30.0 * DEG2RAD\n-\n-        self.timestep = 0\n-        # self.temp_action = np.zeros(1)\n-\n-        mujoco.mj_step(self.model, self.data)\n-\n-        # info = {\n-        #     'initial_reward': 0.0,  # 초기 보상 값\n-        #     'initial_state': self.get_observation,  # 초기 상태 정보\n-        #     'custom_info': 'This is custom environment info'  # 기타 정보\n-        # }\n-        \n-        return self.get_observation(), {}\n-    \n-    def reset_time(self):\n-        self.end_time = self.data.time + self.motion_time\n-    \n-    \n-    \n-    def set_state(self, qpos, qvel):\n-        old_state = self.sim.get_state()\n-        new_state = mujoco.MjSimState(\n-            old_state.time, qpos, qvel, old_state.act, old_state.udd_state\n-        )\n-        self.sim.set_state(new_state)\n-        self.sim.forward()\n-    \n-    @property\n-    def dt(self):\n-        return self.model.opt.timestep * self.frame_skip\n-\n-    # def do_simulation(self, ctrl, n_frames):\n-    #     self.sim.data.ctrl[:] = ctrl\n-    #     self.sim.step()\n-    #     # for _ in range(n_frames):\n-    #     #     self.sim.step()\n-    \n-    \n-    def render(\n-        self,\n-        mode=\"human\",\n-        width=DEFAULT_SIZE,\n-        height=DEFAULT_SIZE,\n-        camera_id=None,\n-        camera_name=None,\n-    ):\n-        if mode == \"rgb_array\" or mode == \"depth_array\":\n-            if camera_id is not None and camera_name is not None:\n-                raise ValueError(\n-                    \"Both `camera_id` and `camera_name` cannot be\"\n-                    \" specified at the same time.\"\n-                )\n-\n-            no_camera_specified = camera_name is None and camera_id is None\n-            if no_camera_specified:\n-                camera_name = \"track\"\n-\n-            if camera_id is None and camera_name in self.model._camera_name2id:\n-                camera_id = self.model.camera_name2id(camera_name)\n-\n-            self._get_viewer(mode).render(width, height, camera_id=camera_id)\n-\n-        if mode == \"rgb_array\":\n-            # window size used for old mujoco-py:\n-            data = self._get_viewer(mode).read_pixels(width, height, depth=False)\n-            # original image is upside-down, so flip it\n-            return data[::-1, :, :]\n-        elif mode == \"depth_array\":\n-            self._get_viewer(mode).render(width, height)\n-            # window size used for old mujoco-py:\n-            # Extract depth part of the read_pixels() tuple\n-            data = self._get_viewer(mode).read_pixels(width, height, depth=True)[1]\n-            # original image is upside-down, so flip it\n-            return data[::-1, :]\n-        elif mode == \"human\":\n-            self._get_viewer(mode).render()\n-            \n-            \n-            \n-    def viewer_setup(self):\n-        \"\"\"\n-        This method is called when the viewer is initialized.\n-        Optionally implement this method, if you need to tinker with camera position\n-        and so forth.\n-        \"\"\"\n-        self.viewer.cam.trackbodyid = 1   #id of the body to track()\n-        self.viewer.cam.distance = self.model.stat.extent * 1.5 #how much zoom in\n-        self.viewer.cam.lookat[0] -= 0 #offset x\n-        self.viewer.cam.lookat[1] -= 0 #offset y\n-        self.viewer.cam.lookat[2] += 0 #offset z\n-        self.viewer.cam.elevation = 0   #cam rotation around the axis in the plane going throug the frame origin\n-\n-        pass\n-\n-    def close(self):\n-        if self.viewer is not None:\n-            # self.viewer.finish()\n-            self.viewer = None\n-            self._viewers = {}\n-\n-    def _get_viewer(self, mode):\n-        self.viewer = self._viewers.get(mode)\n-        if self.viewer is None:\n-            if mode == \"human\":\n-                self.viewer = mujoco.MjViewer(self.sim)\n-            elif mode == \"rgb_array\" or mode == \"depth_array\":\n-                self.viewer = mujoco.MjRenderContextOffscreen(self.sim, -1)\n-\n-            self.viewer_setup()\n-            self._viewers[mode] = self.viewer\n-        return self.viewer\n-\n-    def get_body_com(self, body_name):\n-        return self.data.get_body_xpos(body_name)\n-\n-    def state_vector(self):\n-        return np.concatenate([self.sim.data.qpos.flat, self.sim.data.qvel.flat])\n-\n-    def reward(self):\n-        #Reaching Goal Poses (Cost Function)\n-        tmp_reward = 0\n-        for i in range(6):\n-            if self.X_goal[i] == self.X[i]:  # 분모가 0인 경우 처리\n-                tmp_reward += 1000\n-            else:\n-                tmp_reward += 1 / (self.X_goal[i] - self.X[i])\n-        reward = tmp_reward\n-        return reward\n-    \n-    def seed(self, seed=None):\n         return \n\\ No newline at end of file\n"
                },
                {
                    "date": 1691740260285,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -91,10 +91,10 @@\n         # print('step')\n         sum_reward = 0\n         done = False\n         self.temp_action = action\n-        action = action * 2\n-        action = action + 0.003\n+        action = action + 1.003\n+        # action = action + 0.003\n         self.controller.read(self.data.time, self.data.qpos[0:self.k], self.data.qvel[0:self.k], action)\n         self.controller.control_mujoco()\n         self.tmp_X = self.controller.state_controller()\n         self.torque = self.controller.write()\n"
                },
                {
                    "date": 1691740672443,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -173,10 +173,10 @@\n         return gym.spaces.Box(low=action_low, high=action_high, dtype=np.float64)\n     \n     \n     def _construct_observation_space(self):\n-        obs_low = -1 * np.ones(14) # X(6), X_goal(6), action(2) \n-        obs_high = 1 * np.ones(14)\n+        obs_low = -1 * np.ones(13) # X(6), X_goal(6), action(2) \n+        obs_high = 1 * np.ones(13)\n         # obs_low = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0,0,0, -1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1]\n         # obs_high = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0,0,0, 1,1,1,1,1,1,1,1,1,1,1,1]\n         return gym.spaces.Box(obs_low, obs_high, dtype=np.float64)\n     \n"
                },
                {
                    "date": 1691740718936,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -173,9 +173,9 @@\n         return gym.spaces.Box(low=action_low, high=action_high, dtype=np.float64)\n     \n     \n     def _construct_observation_space(self):\n-        obs_low = -1 * np.ones(13) # X(6), X_goal(6), action(2) \n+        obs_low = -1 * np.ones(13) # X(6), X_goal(6), action(1) \n         obs_high = 1 * np.ones(13)\n         # obs_low = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0,0,0, -1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1]\n         # obs_high = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0,0,0, 1,1,1,1,1,1,1,1,1,1,1,1]\n         return gym.spaces.Box(obs_low, obs_high, dtype=np.float64)\n"
                },
                {
                    "date": 1691740737718,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -33,9 +33,9 @@\n         self.goal = np.zeros(6)\n         # self.reward_arr = np.zeros(2) #shape=(2,), dtype=np.float64\n         self.reward_arr = [0,0,0]\n         self.cnt = 1\n-        self.temp_action = np.zeros(2)\n+        self.temp_action = np.zeros(1)\n         self._max_joint_position = np.zeros(self.k)\n         self._min_joint_position = np.zeros(self.k)\n         self.timestep = 0\n \n"
                },
                {
                    "date": 1691740743708,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,360 @@\n+from tokenize import Double\n+import gymnasium as gym\n+import numpy as np\n+from gymnasium import spaces\n+import mujoco\n+\n+from mujoco import viewer\n+import controller\n+\n+DEFAULT_SIZE = 300\n+DEG2RAD = np.pi / 180.0\n+\n+class FrankaEnv(gym.Env):\n+    def __init__(self):\n+        # self.frame_skip = 1\n+        self.k = 7  # for jacobian calculation\n+        self.model_path = 'model/fr3.xml'\n+        self.model = mujoco.MjModel.from_xml_path(self.model_path)\n+        self.data = mujoco.MjData(self.model)\n+\n+        self.viewer = viewer.launch_passive(model=self.model, data=self.data)\n+        self.duration = 380  # (seconds)\n+        self.framerate = 10  # (Hz)\n+\n+        self.controller = controller.CController()\n+        self.torque = np.zeros(self.k)\n+        self.tmp_X = np.zeros(12)\n+        self.X_goal = np.zeros(6)\n+        self.X = np.zeros(6)\n+        self.quaternion_goal = np.zeros(4)\n+\n+        self.position = np.zeros(6)\n+        self.goal = np.zeros(6)\n+        # self.reward_arr = np.zeros(2) #shape=(2,), dtype=np.float64\n+        self.reward_arr = [0,0,0]\n+        self.cnt = 1\n+        self.temp_action = np.zeros(1)\n+        self._max_joint_position = np.zeros(self.k)\n+        self._min_joint_position = np.zeros(self.k)\n+        self.timestep = 0\n+\n+        self._min_joint_position[0] = -2.8973\n+        self._min_joint_position[1] = -1.7628\n+        self._min_joint_position[2] = -2.8973\n+        self._min_joint_position[3] = -3.0718\n+        self._min_joint_position[4] = -2.8973\n+        self._min_joint_position[5] = -0.0175\n+        self._min_joint_position[6] = -2.8973\n+\n+        self._max_joint_position[0] = 2.8973\n+        self._max_joint_position[1] = 1.7628\n+        self._max_joint_position[2] = 2.8973\n+        self._max_joint_position[3] = -0.0698\n+        self._max_joint_position[4] = 2.8973\n+        self._max_joint_position[5] = 3.7525\n+        self._max_joint_position[6] = 2.8973\n+\n+        self._max_joint_position_hat = np.zeros(self.k)\n+        self._min_joint_position_hat = np.zeros(self.k)\n+\n+        for i in range(self.k):\n+            self._max_joint_position_hat[i] = self._max_joint_position[i] - 0.1*(self._max_joint_position[i] - self._min_joint_position[i])\n+            self._min_joint_position_hat[i] = self._min_joint_position[i] + 0.1*(self._max_joint_position[i] - self._min_joint_position[i])    \n+        \n+        self.observation_space = self._construct_observation_space()\n+        self.action_space = self._construct_action_space()\n+        self.start_time = 0\n+        self.motion_time = 10\n+        self.end_time = self.start_time + self.motion_time\n+\n+\n+        for i in range(self.k):\n+            self.data.qpos[i] = 0.0\n+            self.data.qvel[i] = 0.0\n+\n+        self.data.qpos[3] = -1.1\n+        self.data.qpos[5] = 30.0 * DEG2RAD\n+        mujoco.mj_step(self.model, self.data)\n+        \n+        # self.metadata = {\n+        #     \"render.modes\": [\"human\", \"rgb_array\", \"depth_array\"],\n+        #     \"video.frames_per_second\": int(np.round(1.0 / self.dt)),\n+        #     # \"video.frames_per_second\": int(1000),\n+        # }\n+        \n+        # self.sim_state = self.sim.get_state()\n+        \n+        \n+    def step(self, action, render=True):\n+        self.viewer.sync()\n+        # print('step')\n+        sum_reward = 0\n+        done = False\n+        self.temp_action = action\n+        action = action + 1.003\n+        # action = action + 0.003\n+        self.controller.read(self.data.time, self.data.qpos[0:self.k], self.data.qvel[0:self.k], action)\n+        self.controller.control_mujoco()\n+        self.tmp_X = self.controller.state_controller()\n+        self.torque = self.controller.write()\n+        for i in range(self.k):\n+                self.data.ctrl[i] = self.torque[i]\n+\n+        for i in range(6):\n+            self.X_goal[i] = self.tmp_X[i]\n+            # self.data.qpos[0:self.k]\n+        for i in range(6):\n+            self.X[i] = self.tmp_X[i+6]\n+        self.euler_to_quaternion(self.X_goal[3],self.X_goal[4],self.X_goal[5])\n+        # print(self.data.qpos)\n+        # print(self.data.xpos[13,0])\n+        # xpos랑 xmat을 건드려야함\n+        for i in range(3):\n+            self.data.qpos[9+i] = self.X_goal[i]\n+        for i in range(4):\n+            self.data.qpos[12+i] = self.quaternion_goal[i]\n+        obs =self.get_observation()\n+        mujoco.mj_step(self.model, self.data)\n+        sum_reward = self.reward()\n+        # sum_reward = 0.0\n+        # print(self.data.time)\n+\n+        for i in range(self.k) : \n+            if(self.data.qpos[i] > self._max_joint_position[i]) :\n+                self.reset()\n+                sum_reward -= 1000\n+                return obs, sum_reward, done, dict(), {}\n+            if(self.data.qpos[i] < self._min_joint_position[i]) :\n+                self.reset()\n+                sum_reward -= 1000\n+                return obs, sum_reward, done, dict(), {}\n+\n+        if self.data.time > self.end_time:\n+            for i in range(6) :\n+                if (self.X_goal[i] - self.X[i]) >= 0.01:\n+                    self.reset()\n+                    sum_reward -= 1000\n+                    return obs, sum_reward, done, dict(), {}\n+                else:\n+                    if i == 5 :\n+                        sum_reward += 1000\n+                        # self.reset()\n+                        self.controller.reset_goal()\n+                        self.reset_time()\n+                        # print(\"!!??\")\n+                        # mujoco.mj_step(self.model, self.data)\n+                        return obs, sum_reward, done, dict(), {}\n+        \n+        return obs, sum_reward, done, dict(), {}\n+    \n+    def euler_to_quaternion(self, roll, pitch, yaw):\n+        cy = np.cos(yaw * 0.5)\n+        sy = np.sin(yaw * 0.5)\n+        cp = np.cos(pitch * 0.5)\n+        sp = np.sin(pitch * 0.5)\n+        cr = np.cos(roll * 0.5)\n+        sr = np.sin(roll * 0.5)\n+        \n+        w = cr * cp * cy + sr * sp * sy\n+        x = sr * cp * cy - cr * sp * sy\n+        y = cr * sp * cy + sr * cp * sy\n+        z = cr * cp * sy - sr * sp * cy\n+        self.quaternion_goal[0] = w\n+        self.quaternion_goal[1] = x\n+        self.quaternion_goal[2] = y\n+        self.quaternion_goal[3] = z\n+        # return np.array([w, x, y, z])\n+\n+    def _construct_action_space(self):\n+        # action_low = -1 * np.ones(12)\n+        action_low =  -1 * np.ones(1)\n+        action_high = np.ones(1) \n+        return gym.spaces.Box(low=action_low, high=action_high, dtype=np.float64)\n+    \n+    \n+    def _construct_observation_space(self):\n+        obs_low = -1 * np.ones(13) # X(6), X_goal(6), action(1) \n+        obs_high = 1 * np.ones(13)\n+        # obs_low = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0,0,0, -1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1]\n+        # obs_high = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0,0,0, 1,1,1,1,1,1,1,1,1,1,1,1]\n+        return gym.spaces.Box(obs_low, obs_high, dtype=np.float64)\n+    \n+     \n+    \n+    def get_observation(self):\n+        # self.sim_state = self.sim.get_state()\n+        \n+        self.position[0] = self.X[0] / 0.6 # -0.6~0.6\n+        self.position[1] = self.X[1] / 0.6 # -0.6~0.6\n+        self.position[2] = self.X[2] -0.35 # 0.1~0.8\n+        self.position[2] = self.position[2] / 0.45\n+\n+        self.position[3] = self.X[3] / np.pi # -np.pi~np.pi\n+        self.position[4] = self.X[4] / np.pi #      \"\n+        self.position[5] = self.X[5] / np.pi #      \"\n+\n+        self.goal[0] = self.X_goal[0] / 0.6\n+        self.goal[1] = self.X_goal[1] / 0.6\n+        self.goal[2] = self.X_goal[2] -0.35\n+        self.goal[2] = self.goal[2] / 0.45\n+\n+        self.goal[3] = self.X_goal[3] / np.pi\n+        self.goal[4] = self.X_goal[4] / np.pi\n+        self.goal[5] = self.X_goal[5] / np.pi\n+        # goal = # controller로부터 받은 goal\n+        # self.temp_action = self.temp_action * 10\n+        # self.temp_action = self.temp_action - 1\n+        return np.concatenate(\n+        (\n+            self.position, #6\n+            self.goal, #6\n+            self.temp_action, # 1\n+        )\n+        )\n+        \n+    def reset(self, seed=None):\n+        self.seedNum = seed\n+        # self.qpos = np.zeros(self.k)\n+        # qvel = np.zeros(self.k)\n+        # self.sim_state.qpos[0:self.k], self.sim_state.qvel[0:self.k]\n+        self.controller.initialize()\n+        self.end_time = self.data.time + self.motion_time\n+\n+        for i in range(self.k):\n+            self.data.qpos[i] = 0.0\n+            self.data.qvel[i] = 0.0\n+\n+        self.data.qpos[3] = -1.1\n+        self.data.qpos[5] = 30.0 * DEG2RAD\n+\n+        self.timestep = 0\n+        # self.temp_action = np.zeros(1)\n+\n+        mujoco.mj_step(self.model, self.data)\n+\n+        # info = {\n+        #     'initial_reward': 0.0,  # 초기 보상 값\n+        #     'initial_state': self.get_observation,  # 초기 상태 정보\n+        #     'custom_info': 'This is custom environment info'  # 기타 정보\n+        # }\n+        \n+        return self.get_observation(), {}\n+    \n+    def reset_time(self):\n+        self.end_time = self.data.time + self.motion_time\n+    \n+    \n+    \n+    def set_state(self, qpos, qvel):\n+        old_state = self.sim.get_state()\n+        new_state = mujoco.MjSimState(\n+            old_state.time, qpos, qvel, old_state.act, old_state.udd_state\n+        )\n+        self.sim.set_state(new_state)\n+        self.sim.forward()\n+    \n+    @property\n+    def dt(self):\n+        return self.model.opt.timestep * self.frame_skip\n+\n+    # def do_simulation(self, ctrl, n_frames):\n+    #     self.sim.data.ctrl[:] = ctrl\n+    #     self.sim.step()\n+    #     # for _ in range(n_frames):\n+    #     #     self.sim.step()\n+    \n+    \n+    def render(\n+        self,\n+        mode=\"human\",\n+        width=DEFAULT_SIZE,\n+        height=DEFAULT_SIZE,\n+        camera_id=None,\n+        camera_name=None,\n+    ):\n+        if mode == \"rgb_array\" or mode == \"depth_array\":\n+            if camera_id is not None and camera_name is not None:\n+                raise ValueError(\n+                    \"Both `camera_id` and `camera_name` cannot be\"\n+                    \" specified at the same time.\"\n+                )\n+\n+            no_camera_specified = camera_name is None and camera_id is None\n+            if no_camera_specified:\n+                camera_name = \"track\"\n+\n+            if camera_id is None and camera_name in self.model._camera_name2id:\n+                camera_id = self.model.camera_name2id(camera_name)\n+\n+            self._get_viewer(mode).render(width, height, camera_id=camera_id)\n+\n+        if mode == \"rgb_array\":\n+            # window size used for old mujoco-py:\n+            data = self._get_viewer(mode).read_pixels(width, height, depth=False)\n+            # original image is upside-down, so flip it\n+            return data[::-1, :, :]\n+        elif mode == \"depth_array\":\n+            self._get_viewer(mode).render(width, height)\n+            # window size used for old mujoco-py:\n+            # Extract depth part of the read_pixels() tuple\n+            data = self._get_viewer(mode).read_pixels(width, height, depth=True)[1]\n+            # original image is upside-down, so flip it\n+            return data[::-1, :]\n+        elif mode == \"human\":\n+            self._get_viewer(mode).render()\n+            \n+            \n+            \n+    def viewer_setup(self):\n+        \"\"\"\n+        This method is called when the viewer is initialized.\n+        Optionally implement this method, if you need to tinker with camera position\n+        and so forth.\n+        \"\"\"\n+        self.viewer.cam.trackbodyid = 1   #id of the body to track()\n+        self.viewer.cam.distance = self.model.stat.extent * 1.5 #how much zoom in\n+        self.viewer.cam.lookat[0] -= 0 #offset x\n+        self.viewer.cam.lookat[1] -= 0 #offset y\n+        self.viewer.cam.lookat[2] += 0 #offset z\n+        self.viewer.cam.elevation = 0   #cam rotation around the axis in the plane going throug the frame origin\n+\n+        pass\n+\n+    def close(self):\n+        if self.viewer is not None:\n+            # self.viewer.finish()\n+            self.viewer = None\n+            self._viewers = {}\n+\n+    def _get_viewer(self, mode):\n+        self.viewer = self._viewers.get(mode)\n+        if self.viewer is None:\n+            if mode == \"human\":\n+                self.viewer = mujoco.MjViewer(self.sim)\n+            elif mode == \"rgb_array\" or mode == \"depth_array\":\n+                self.viewer = mujoco.MjRenderContextOffscreen(self.sim, -1)\n+\n+            self.viewer_setup()\n+            self._viewers[mode] = self.viewer\n+        return self.viewer\n+\n+    def get_body_com(self, body_name):\n+        return self.data.get_body_xpos(body_name)\n+\n+    def state_vector(self):\n+        return np.concatenate([self.sim.data.qpos.flat, self.sim.data.qvel.flat])\n+\n+    def reward(self):\n+        #Reaching Goal Poses (Cost Function)\n+        tmp_reward = 0\n+        for i in range(6):\n+            if self.X_goal[i] == self.X[i]:  # 분모가 0인 경우 처리\n+                tmp_reward += 1000\n+            else:\n+                tmp_reward += 1 / (self.X_goal[i] - self.X[i])\n+        reward = tmp_reward\n+        return reward\n+    \n+    def seed(self, seed=None):\n+        return \n\\ No newline at end of file\n"
                },
                {
                    "date": 1691744255508,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -91,9 +91,11 @@\n         # print('step')\n         sum_reward = 0\n         done = False\n         self.temp_action = action\n-        action = action + 1.003\n+        action = action + 1.0\n+        action = action / 2.0\n+        action = action + 0.003\n         # action = action + 0.003\n         self.controller.read(self.data.time, self.data.qpos[0:self.k], self.data.qvel[0:self.k], action)\n         self.controller.control_mujoco()\n         self.tmp_X = self.controller.state_controller()\n@@ -356,365 +358,5 @@\n         reward = tmp_reward\n         return reward\n     \n     def seed(self, seed=None):\n-        return \n-from tokenize import Double\n-import gymnasium as gym\n-import numpy as np\n-from gymnasium import spaces\n-import mujoco\n-\n-from mujoco import viewer\n-import controller\n-\n-DEFAULT_SIZE = 300\n-DEG2RAD = np.pi / 180.0\n-\n-class FrankaEnv(gym.Env):\n-    def __init__(self):\n-        # self.frame_skip = 1\n-        self.k = 7  # for jacobian calculation\n-        self.model_path = 'model/fr3.xml'\n-        self.model = mujoco.MjModel.from_xml_path(self.model_path)\n-        self.data = mujoco.MjData(self.model)\n-\n-        self.viewer = viewer.launch_passive(model=self.model, data=self.data)\n-        self.duration = 380  # (seconds)\n-        self.framerate = 10  # (Hz)\n-\n-        self.controller = controller.CController()\n-        self.torque = np.zeros(self.k)\n-        self.tmp_X = np.zeros(12)\n-        self.X_goal = np.zeros(6)\n-        self.X = np.zeros(6)\n-        self.quaternion_goal = np.zeros(4)\n-\n-        self.position = np.zeros(6)\n-        self.goal = np.zeros(6)\n-        # self.reward_arr = np.zeros(2) #shape=(2,), dtype=np.float64\n-        self.reward_arr = [0,0,0]\n-        self.cnt = 1\n-        self.temp_action = np.zeros(1)\n-        self._max_joint_position = np.zeros(self.k)\n-        self._min_joint_position = np.zeros(self.k)\n-        self.timestep = 0\n-\n-        self._min_joint_position[0] = -2.8973\n-        self._min_joint_position[1] = -1.7628\n-        self._min_joint_position[2] = -2.8973\n-        self._min_joint_position[3] = -3.0718\n-        self._min_joint_position[4] = -2.8973\n-        self._min_joint_position[5] = -0.0175\n-        self._min_joint_position[6] = -2.8973\n-\n-        self._max_joint_position[0] = 2.8973\n-        self._max_joint_position[1] = 1.7628\n-        self._max_joint_position[2] = 2.8973\n-        self._max_joint_position[3] = -0.0698\n-        self._max_joint_position[4] = 2.8973\n-        self._max_joint_position[5] = 3.7525\n-        self._max_joint_position[6] = 2.8973\n-\n-        self._max_joint_position_hat = np.zeros(self.k)\n-        self._min_joint_position_hat = np.zeros(self.k)\n-\n-        for i in range(self.k):\n-            self._max_joint_position_hat[i] = self._max_joint_position[i] - 0.1*(self._max_joint_position[i] - self._min_joint_position[i])\n-            self._min_joint_position_hat[i] = self._min_joint_position[i] + 0.1*(self._max_joint_position[i] - self._min_joint_position[i])    \n-        \n-        self.observation_space = self._construct_observation_space()\n-        self.action_space = self._construct_action_space()\n-        self.start_time = 0\n-        self.motion_time = 10\n-        self.end_time = self.start_time + self.motion_time\n-\n-\n-        for i in range(self.k):\n-            self.data.qpos[i] = 0.0\n-            self.data.qvel[i] = 0.0\n-\n-        self.data.qpos[3] = -1.1\n-        self.data.qpos[5] = 30.0 * DEG2RAD\n-        mujoco.mj_step(self.model, self.data)\n-        \n-        # self.metadata = {\n-        #     \"render.modes\": [\"human\", \"rgb_array\", \"depth_array\"],\n-        #     \"video.frames_per_second\": int(np.round(1.0 / self.dt)),\n-        #     # \"video.frames_per_second\": int(1000),\n-        # }\n-        \n-        # self.sim_state = self.sim.get_state()\n-        \n-        \n-    def step(self, action, render=True):\n-        self.viewer.sync()\n-        # print('step')\n-        sum_reward = 0\n-        done = False\n-        self.temp_action = action\n-        action = action + 1.003\n-        # action = action + 0.003\n-        self.controller.read(self.data.time, self.data.qpos[0:self.k], self.data.qvel[0:self.k], action)\n-        self.controller.control_mujoco()\n-        self.tmp_X = self.controller.state_controller()\n-        self.torque = self.controller.write()\n-        for i in range(self.k):\n-                self.data.ctrl[i] = self.torque[i]\n-\n-        for i in range(6):\n-            self.X_goal[i] = self.tmp_X[i]\n-            # self.data.qpos[0:self.k]\n-        for i in range(6):\n-            self.X[i] = self.tmp_X[i+6]\n-        self.euler_to_quaternion(self.X_goal[3],self.X_goal[4],self.X_goal[5])\n-        # print(self.data.qpos)\n-        # print(self.data.xpos[13,0])\n-        # xpos랑 xmat을 건드려야함\n-        for i in range(3):\n-            self.data.qpos[9+i] = self.X_goal[i]\n-        for i in range(4):\n-            self.data.qpos[12+i] = self.quaternion_goal[i]\n-        obs =self.get_observation()\n-        mujoco.mj_step(self.model, self.data)\n-        sum_reward = self.reward()\n-        # sum_reward = 0.0\n-        # print(self.data.time)\n-\n-        for i in range(self.k) : \n-            if(self.data.qpos[i] > self._max_joint_position[i]) :\n-                self.reset()\n-                sum_reward -= 1000\n-                return obs, sum_reward, done, dict(), {}\n-            if(self.data.qpos[i] < self._min_joint_position[i]) :\n-                self.reset()\n-                sum_reward -= 1000\n-                return obs, sum_reward, done, dict(), {}\n-\n-        if self.data.time > self.end_time:\n-            for i in range(6) :\n-                if (self.X_goal[i] - self.X[i]) >= 0.01:\n-                    self.reset()\n-                    sum_reward -= 1000\n-                    return obs, sum_reward, done, dict(), {}\n-                else:\n-                    if i == 5 :\n-                        sum_reward += 1000\n-                        # self.reset()\n-                        self.controller.reset_goal()\n-                        self.reset_time()\n-                        # print(\"!!??\")\n-                        # mujoco.mj_step(self.model, self.data)\n-                        return obs, sum_reward, done, dict(), {}\n-        \n-        return obs, sum_reward, done, dict(), {}\n-    \n-    def euler_to_quaternion(self, roll, pitch, yaw):\n-        cy = np.cos(yaw * 0.5)\n-        sy = np.sin(yaw * 0.5)\n-        cp = np.cos(pitch * 0.5)\n-        sp = np.sin(pitch * 0.5)\n-        cr = np.cos(roll * 0.5)\n-        sr = np.sin(roll * 0.5)\n-        \n-        w = cr * cp * cy + sr * sp * sy\n-        x = sr * cp * cy - cr * sp * sy\n-        y = cr * sp * cy + sr * cp * sy\n-        z = cr * cp * sy - sr * sp * cy\n-        self.quaternion_goal[0] = w\n-        self.quaternion_goal[1] = x\n-        self.quaternion_goal[2] = y\n-        self.quaternion_goal[3] = z\n-        # return np.array([w, x, y, z])\n-\n-    def _construct_action_space(self):\n-        # action_low = -1 * np.ones(12)\n-        action_low =  -1 * np.ones(1)\n-        action_high = np.ones(1) \n-        return gym.spaces.Box(low=action_low, high=action_high, dtype=np.float64)\n-    \n-    \n-    def _construct_observation_space(self):\n-        obs_low = -1 * np.ones(13) # X(6), X_goal(6), action(1) \n-        obs_high = 1 * np.ones(13)\n-        # obs_low = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0,0,0, -1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1]\n-        # obs_high = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0,0,0, 1,1,1,1,1,1,1,1,1,1,1,1]\n-        return gym.spaces.Box(obs_low, obs_high, dtype=np.float64)\n-    \n-     \n-    \n-    def get_observation(self):\n-        # self.sim_state = self.sim.get_state()\n-        \n-        self.position[0] = self.X[0] / 0.6 # -0.6~0.6\n-        self.position[1] = self.X[1] / 0.6 # -0.6~0.6\n-        self.position[2] = self.X[2] -0.35 # 0.1~0.8\n-        self.position[2] = self.position[2] / 0.45\n-\n-        self.position[3] = self.X[3] / np.pi # -np.pi~np.pi\n-        self.position[4] = self.X[4] / np.pi #      \"\n-        self.position[5] = self.X[5] / np.pi #      \"\n-\n-        self.goal[0] = self.X_goal[0] / 0.6\n-        self.goal[1] = self.X_goal[1] / 0.6\n-        self.goal[2] = self.X_goal[2] -0.35\n-        self.goal[2] = self.goal[2] / 0.45\n-\n-        self.goal[3] = self.X_goal[3] / np.pi\n-        self.goal[4] = self.X_goal[4] / np.pi\n-        self.goal[5] = self.X_goal[5] / np.pi\n-        # goal = # controller로부터 받은 goal\n-        # self.temp_action = self.temp_action * 10\n-        # self.temp_action = self.temp_action - 1\n-        return np.concatenate(\n-        (\n-            self.position, #6\n-            self.goal, #6\n-            self.temp_action, # 1\n-        )\n-        )\n-        \n-    def reset(self, seed=None):\n-        self.seedNum = seed\n-        # self.qpos = np.zeros(self.k)\n-        # qvel = np.zeros(self.k)\n-        # self.sim_state.qpos[0:self.k], self.sim_state.qvel[0:self.k]\n-        self.controller.initialize()\n-        self.end_time = self.data.time + self.motion_time\n-\n-        for i in range(self.k):\n-            self.data.qpos[i] = 0.0\n-            self.data.qvel[i] = 0.0\n-\n-        self.data.qpos[3] = -1.1\n-        self.data.qpos[5] = 30.0 * DEG2RAD\n-\n-        self.timestep = 0\n-        # self.temp_action = np.zeros(1)\n-\n-        mujoco.mj_step(self.model, self.data)\n-\n-        # info = {\n-        #     'initial_reward': 0.0,  # 초기 보상 값\n-        #     'initial_state': self.get_observation,  # 초기 상태 정보\n-        #     'custom_info': 'This is custom environment info'  # 기타 정보\n-        # }\n-        \n-        return self.get_observation(), {}\n-    \n-    def reset_time(self):\n-        self.end_time = self.data.time + self.motion_time\n-    \n-    \n-    \n-    def set_state(self, qpos, qvel):\n-        old_state = self.sim.get_state()\n-        new_state = mujoco.MjSimState(\n-            old_state.time, qpos, qvel, old_state.act, old_state.udd_state\n-        )\n-        self.sim.set_state(new_state)\n-        self.sim.forward()\n-    \n-    @property\n-    def dt(self):\n-        return self.model.opt.timestep * self.frame_skip\n-\n-    # def do_simulation(self, ctrl, n_frames):\n-    #     self.sim.data.ctrl[:] = ctrl\n-    #     self.sim.step()\n-    #     # for _ in range(n_frames):\n-    #     #     self.sim.step()\n-    \n-    \n-    def render(\n-        self,\n-        mode=\"human\",\n-        width=DEFAULT_SIZE,\n-        height=DEFAULT_SIZE,\n-        camera_id=None,\n-        camera_name=None,\n-    ):\n-        if mode == \"rgb_array\" or mode == \"depth_array\":\n-            if camera_id is not None and camera_name is not None:\n-                raise ValueError(\n-                    \"Both `camera_id` and `camera_name` cannot be\"\n-                    \" specified at the same time.\"\n-                )\n-\n-            no_camera_specified = camera_name is None and camera_id is None\n-            if no_camera_specified:\n-                camera_name = \"track\"\n-\n-            if camera_id is None and camera_name in self.model._camera_name2id:\n-                camera_id = self.model.camera_name2id(camera_name)\n-\n-            self._get_viewer(mode).render(width, height, camera_id=camera_id)\n-\n-        if mode == \"rgb_array\":\n-            # window size used for old mujoco-py:\n-            data = self._get_viewer(mode).read_pixels(width, height, depth=False)\n-            # original image is upside-down, so flip it\n-            return data[::-1, :, :]\n-        elif mode == \"depth_array\":\n-            self._get_viewer(mode).render(width, height)\n-            # window size used for old mujoco-py:\n-            # Extract depth part of the read_pixels() tuple\n-            data = self._get_viewer(mode).read_pixels(width, height, depth=True)[1]\n-            # original image is upside-down, so flip it\n-            return data[::-1, :]\n-        elif mode == \"human\":\n-            self._get_viewer(mode).render()\n-            \n-            \n-            \n-    def viewer_setup(self):\n-        \"\"\"\n-        This method is called when the viewer is initialized.\n-        Optionally implement this method, if you need to tinker with camera position\n-        and so forth.\n-        \"\"\"\n-        self.viewer.cam.trackbodyid = 1   #id of the body to track()\n-        self.viewer.cam.distance = self.model.stat.extent * 1.5 #how much zoom in\n-        self.viewer.cam.lookat[0] -= 0 #offset x\n-        self.viewer.cam.lookat[1] -= 0 #offset y\n-        self.viewer.cam.lookat[2] += 0 #offset z\n-        self.viewer.cam.elevation = 0   #cam rotation around the axis in the plane going throug the frame origin\n-\n-        pass\n-\n-    def close(self):\n-        if self.viewer is not None:\n-            # self.viewer.finish()\n-            self.viewer = None\n-            self._viewers = {}\n-\n-    def _get_viewer(self, mode):\n-        self.viewer = self._viewers.get(mode)\n-        if self.viewer is None:\n-            if mode == \"human\":\n-                self.viewer = mujoco.MjViewer(self.sim)\n-            elif mode == \"rgb_array\" or mode == \"depth_array\":\n-                self.viewer = mujoco.MjRenderContextOffscreen(self.sim, -1)\n-\n-            self.viewer_setup()\n-            self._viewers[mode] = self.viewer\n-        return self.viewer\n-\n-    def get_body_com(self, body_name):\n-        return self.data.get_body_xpos(body_name)\n-\n-    def state_vector(self):\n-        return np.concatenate([self.sim.data.qpos.flat, self.sim.data.qvel.flat])\n-\n-    def reward(self):\n-        #Reaching Goal Poses (Cost Function)\n-        tmp_reward = 0\n-        for i in range(6):\n-            if self.X_goal[i] == self.X[i]:  # 분모가 0인 경우 처리\n-                tmp_reward += 1000\n-            else:\n-                tmp_reward += 1 / (self.X_goal[i] - self.X[i])\n-        reward = tmp_reward\n-        return reward\n-    \n-    def seed(self, seed=None):\n         return \n\\ No newline at end of file\n"
                },
                {
                    "date": 1691744465670,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -84,8 +84,15 @@\n         # }\n         \n         # self.sim_state = self.sim.get_state()\n         \n+        #######################################################\n+        # TODO (0814)\n+        # rotation정보 matrix로 처리하기\n+        # 속도와 관련된 reward 만들지 생각해 보기\n+        # action이 dt_cuda[2]의 delta를 구하는 것으로 바꿔서도 해보기  \n+        ########################################################\n+\n         \n     def step(self, action, render=True):\n         self.viewer.sync()\n         # print('step')\n"
                },
                {
                    "date": 1691744587500,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -89,11 +89,12 @@\n         # TODO (0814)\n         # rotation정보 matrix로 처리하기\n         # 속도와 관련된 reward 만들지 생각해 보기\n         # action이 dt_cuda[2]의 delta를 구하는 것으로 바꿔서도 해보기  \n+        # done 기준 rotation에는 조금 더 여유롭게 정해주기 (cuda.cu와 비슷한 구조?)\n         ########################################################\n \n-        \n+\n     def step(self, action, render=True):\n         self.viewer.sync()\n         # print('step')\n         sum_reward = 0\n"
                },
                {
                    "date": 1691981942207,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -92,9 +92,9 @@\n         # action이 dt_cuda[2]의 delta를 구하는 것으로 바꿔서도 해보기  \n         # done 기준 rotation에는 조금 더 여유롭게 정해주기 (cuda.cu와 비슷한 구조?)\n         ########################################################\n \n-\n+    \n     def step(self, action, render=True):\n         self.viewer.sync()\n         # print('step')\n         sum_reward = 0\n"
                },
                {
                    "date": 1691981976260,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -156,9 +156,15 @@\n                         # mujoco.mj_step(self.model, self.data)\n                         return obs, sum_reward, done, dict(), {}\n         \n         return obs, sum_reward, done, dict(), {}\n-    \n+    # def render(self):\n+    #     if self.viewer is None:\n+    #         self.viewer = viewer.launch_passive(model=self.model, data=self.data)\n+    #     else:\n+    #         self.viewer.sync()\n+    #         sleep(0.002)\n+\n     def euler_to_quaternion(self, roll, pitch, yaw):\n         cy = np.cos(yaw * 0.5)\n         sy = np.sin(yaw * 0.5)\n         cp = np.cos(pitch * 0.5)\n"
                },
                {
                    "date": 1691982525283,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -90,8 +90,9 @@\n         # rotation정보 matrix로 처리하기\n         # 속도와 관련된 reward 만들지 생각해 보기\n         # action이 dt_cuda[2]의 delta를 구하는 것으로 바꿔서도 해보기  \n         # done 기준 rotation에는 조금 더 여유롭게 정해주기 (cuda.cu와 비슷한 구조?)\n+        # obs에 q정보도 넣어주기\n         ########################################################\n \n     \n     def step(self, action, render=True):\n"
                },
                {
                    "date": 1691985637322,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -23,9 +23,9 @@\n         self.framerate = 10  # (Hz)\n \n         self.controller = controller.CController()\n         self.torque = np.zeros(self.k)\n-        self.tmp_X = np.zeros(12)\n+        self.tmp_X = np.zeros(19)\n         self.X_goal = np.zeros(6)\n         self.X = np.zeros(6)\n         self.quaternion_goal = np.zeros(4)\n \n"
                },
                {
                    "date": 1691985651478,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -116,8 +116,10 @@\n             self.X_goal[i] = self.tmp_X[i]\n             # self.data.qpos[0:self.k]\n         for i in range(6):\n             self.X[i] = self.tmp_X[i+6]\n+        for i in range(6):\n+            self.q[i] = self.tmp_X[i+6]\n         self.euler_to_quaternion(self.X_goal[3],self.X_goal[4],self.X_goal[5])\n         # print(self.data.qpos)\n         # print(self.data.xpos[13,0])\n         # xpos랑 xmat을 건드려야함\n"
                },
                {
                    "date": 1691985662385,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -26,8 +26,9 @@\n         self.torque = np.zeros(self.k)\n         self.tmp_X = np.zeros(19)\n         self.X_goal = np.zeros(6)\n         self.X = np.zeros(6)\n+        self.q = np.zeros(7)\n         self.quaternion_goal = np.zeros(4)\n \n         self.position = np.zeros(6)\n         self.goal = np.zeros(6)\n"
                },
                {
                    "date": 1691985728180,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -185,8 +185,53 @@\n         self.quaternion_goal[2] = y\n         self.quaternion_goal[3] = z\n         # return np.array([w, x, y, z])\n \n+    __global__ void GetBodyRotationMatrix(double *xyzrpy_goal, double *RGyro)\n+{\n+    // printf(\"GetBodyRotationMatrix OK \\n\");\n+    double Roll = xyzrpy_goal[3];\n+    double Pitch = xyzrpy_goal[4];\n+    double Yaw = xyzrpy_goal[5];\n+\n+    double R_yaw[9];\n+    R_yaw[0] = cos(Yaw);\n+    R_yaw[1] = -sin(Yaw);\n+    R_yaw[2] = 0.0;\n+    R_yaw[3] = sin(Yaw);\n+    R_yaw[4] = cos(Yaw);\n+    R_yaw[5] = 0.0;\n+    R_yaw[6] = 0.0;\n+    R_yaw[7] = 0.0;\n+    R_yaw[8] = 1.0;\n+\n+    double R_pitch[9];\n+    R_pitch[0] = cos(Pitch);\n+    R_pitch[1] = 0.0;\n+    R_pitch[2] = sin(Pitch);\n+    R_pitch[3] = 0.0;\n+    R_pitch[4] = 1.0;\n+    R_pitch[5] = 0.0;\n+    R_pitch[6] = -sin(Pitch);\n+    R_pitch[7] = 0.0;\n+    R_pitch[8] = cos(Pitch);\n+\n+    double R_roll[9];\n+    R_roll[0] = 1.0;\n+    R_roll[1] = 0.0;\n+    R_roll[2] = 0.0;\n+    R_roll[3] = 0.0;\n+    R_roll[4] = cos(Roll);\n+    R_roll[5] = -sin(Roll);\n+    R_roll[6] = 0.0;\n+    R_roll[7] = sin(Roll);\n+    R_roll[8] = cos(Roll);\n+    \n+    double tmp_mat3x3[9];\n+    matrixMultiply_3x3(R_yaw, R_pitch, tmp_mat3x3);\n+    matrixMultiply_3x3(tmp_mat3x3, R_roll, RGyro);\n+\n+\n     def _construct_action_space(self):\n         # action_low = -1 * np.ones(12)\n         action_low =  -1 * np.ones(1)\n         action_high = np.ones(1) \n"
                },
                {
                    "date": 1691985746792,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -185,53 +185,52 @@\n         self.quaternion_goal[2] = y\n         self.quaternion_goal[3] = z\n         # return np.array([w, x, y, z])\n \n-    __global__ void GetBodyRotationMatrix(double *xyzrpy_goal, double *RGyro)\n-{\n-    // printf(\"GetBodyRotationMatrix OK \\n\");\n-    double Roll = xyzrpy_goal[3];\n-    double Pitch = xyzrpy_goal[4];\n-    double Yaw = xyzrpy_goal[5];\n+    def GetBodyRotationMatrix(double *xyzrpy_goal, double *RGyro):\n \n-    double R_yaw[9];\n-    R_yaw[0] = cos(Yaw);\n-    R_yaw[1] = -sin(Yaw);\n-    R_yaw[2] = 0.0;\n-    R_yaw[3] = sin(Yaw);\n-    R_yaw[4] = cos(Yaw);\n-    R_yaw[5] = 0.0;\n-    R_yaw[6] = 0.0;\n-    R_yaw[7] = 0.0;\n-    R_yaw[8] = 1.0;\n+        double Roll = xyzrpy_goal[3];\n+        double Pitch = xyzrpy_goal[4];\n+        double Yaw = xyzrpy_goal[5];\n \n-    double R_pitch[9];\n-    R_pitch[0] = cos(Pitch);\n-    R_pitch[1] = 0.0;\n-    R_pitch[2] = sin(Pitch);\n-    R_pitch[3] = 0.0;\n-    R_pitch[4] = 1.0;\n-    R_pitch[5] = 0.0;\n-    R_pitch[6] = -sin(Pitch);\n-    R_pitch[7] = 0.0;\n-    R_pitch[8] = cos(Pitch);\n+        double R_yaw[9];\n+        R_yaw[0] = cos(Yaw);\n+        R_yaw[1] = -sin(Yaw);\n+        R_yaw[2] = 0.0;\n+        R_yaw[3] = sin(Yaw);\n+        R_yaw[4] = cos(Yaw);\n+        R_yaw[5] = 0.0;\n+        R_yaw[6] = 0.0;\n+        R_yaw[7] = 0.0;\n+        R_yaw[8] = 1.0;\n \n-    double R_roll[9];\n-    R_roll[0] = 1.0;\n-    R_roll[1] = 0.0;\n-    R_roll[2] = 0.0;\n-    R_roll[3] = 0.0;\n-    R_roll[4] = cos(Roll);\n-    R_roll[5] = -sin(Roll);\n-    R_roll[6] = 0.0;\n-    R_roll[7] = sin(Roll);\n-    R_roll[8] = cos(Roll);\n-    \n-    double tmp_mat3x3[9];\n-    matrixMultiply_3x3(R_yaw, R_pitch, tmp_mat3x3);\n-    matrixMultiply_3x3(tmp_mat3x3, R_roll, RGyro);\n+        double R_pitch[9];\n+        R_pitch[0] = cos(Pitch);\n+        R_pitch[1] = 0.0;\n+        R_pitch[2] = sin(Pitch);\n+        R_pitch[3] = 0.0;\n+        R_pitch[4] = 1.0;\n+        R_pitch[5] = 0.0;\n+        R_pitch[6] = -sin(Pitch);\n+        R_pitch[7] = 0.0;\n+        R_pitch[8] = cos(Pitch);\n \n+        double R_roll[9];\n+        R_roll[0] = 1.0;\n+        R_roll[1] = 0.0;\n+        R_roll[2] = 0.0;\n+        R_roll[3] = 0.0;\n+        R_roll[4] = cos(Roll);\n+        R_roll[5] = -sin(Roll);\n+        R_roll[6] = 0.0;\n+        R_roll[7] = sin(Roll);\n+        R_roll[8] = cos(Roll);\n+        \n+        double tmp_mat3x3[9];\n+        matrixMultiply_3x3(R_yaw, R_pitch, tmp_mat3x3);\n+        matrixMultiply_3x3(tmp_mat3x3, R_roll, RGyro);\n \n+\n     def _construct_action_space(self):\n         # action_low = -1 * np.ones(12)\n         action_low =  -1 * np.ones(1)\n         action_high = np.ones(1) \n"
                },
                {
                    "date": 1691985783903,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -185,9 +185,9 @@\n         self.quaternion_goal[2] = y\n         self.quaternion_goal[3] = z\n         # return np.array([w, x, y, z])\n \n-    def GetBodyRotationMatrix(double *xyzrpy_goal, double *RGyro):\n+    def GetBodyRotationMatrix(self, roll, pitch, yaw):\n \n         double Roll = xyzrpy_goal[3];\n         double Pitch = xyzrpy_goal[4];\n         double Yaw = xyzrpy_goal[5];\n"
                },
                {
                    "date": 1691985827586,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -27,8 +27,11 @@\n         self.tmp_X = np.zeros(19)\n         self.X_goal = np.zeros(6)\n         self.X = np.zeros(6)\n         self.q = np.zeros(7)\n+\n+        self.rot_mat_goal = np.zeros(9)\n+        self.rot_mat_pre = np.zeros(9)\n         self.quaternion_goal = np.zeros(4)\n \n         self.position = np.zeros(6)\n         self.goal = np.zeros(6)\n@@ -186,14 +189,9 @@\n         self.quaternion_goal[3] = z\n         # return np.array([w, x, y, z])\n \n     def GetBodyRotationMatrix(self, roll, pitch, yaw):\n-\n-        double Roll = xyzrpy_goal[3];\n-        double Pitch = xyzrpy_goal[4];\n-        double Yaw = xyzrpy_goal[5];\n-\n-        double R_yaw[9];\n+        rot_mat_[0] = \n         R_yaw[0] = cos(Yaw);\n         R_yaw[1] = -sin(Yaw);\n         R_yaw[2] = 0.0;\n         R_yaw[3] = sin(Yaw);\n"
                },
                {
                    "date": 1691985854973,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -189,9 +189,9 @@\n         self.quaternion_goal[3] = z\n         # return np.array([w, x, y, z])\n \n     def GetBodyRotationMatrix(self, roll, pitch, yaw):\n-        rot_mat_[0] = \n+        rot_mat[0] = np.cos(Yaw)\n         R_yaw[0] = cos(Yaw);\n         R_yaw[1] = -sin(Yaw);\n         R_yaw[2] = 0.0;\n         R_yaw[3] = sin(Yaw);\n"
                },
                {
                    "date": 1691985862356,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -189,10 +189,9 @@\n         self.quaternion_goal[3] = z\n         # return np.array([w, x, y, z])\n \n     def GetBodyRotationMatrix(self, roll, pitch, yaw):\n-        rot_mat[0] = np.cos(Yaw)\n-        R_yaw[0] = cos(Yaw);\n+        rot_mat[0] = np.cos(yaw)\n         R_yaw[1] = -sin(Yaw);\n         R_yaw[2] = 0.0;\n         R_yaw[3] = sin(Yaw);\n         R_yaw[4] = cos(Yaw);\n"
                },
                {
                    "date": 1691985886167,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -189,10 +189,11 @@\n         self.quaternion_goal[3] = z\n         # return np.array([w, x, y, z])\n \n     def GetBodyRotationMatrix(self, roll, pitch, yaw):\n+        rot_mat = np.zeros(9)\n         rot_mat[0] = np.cos(yaw)\n-        R_yaw[1] = -sin(Yaw);\n+        rot_mat[1] = -sin(Yaw);\n         R_yaw[2] = 0.0;\n         R_yaw[3] = sin(Yaw);\n         R_yaw[4] = cos(Yaw);\n         R_yaw[5] = 0.0;\n"
                },
                {
                    "date": 1691985892736,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -191,9 +191,9 @@\n \n     def GetBodyRotationMatrix(self, roll, pitch, yaw):\n         rot_mat = np.zeros(9)\n         rot_mat[0] = np.cos(yaw)\n-        rot_mat[1] = -sin(Yaw);\n+        rot_mat[1] = -np.sin(yaw)\n         R_yaw[2] = 0.0;\n         R_yaw[3] = sin(Yaw);\n         R_yaw[4] = cos(Yaw);\n         R_yaw[5] = 0.0;\n"
                },
                {
                    "date": 1691985897887,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -192,41 +192,43 @@\n     def GetBodyRotationMatrix(self, roll, pitch, yaw):\n         rot_mat = np.zeros(9)\n         rot_mat[0] = np.cos(yaw)\n         rot_mat[1] = -np.sin(yaw)\n-        R_yaw[2] = 0.0;\n-        R_yaw[3] = sin(Yaw);\n-        R_yaw[4] = cos(Yaw);\n-        R_yaw[5] = 0.0;\n-        R_yaw[6] = 0.0;\n-        R_yaw[7] = 0.0;\n-        R_yaw[8] = 1.0;\n+        \n+        \n+        # R_yaw[2] = 0.0;\n+        # R_yaw[3] = sin(Yaw);\n+        # R_yaw[4] = cos(Yaw);\n+        # R_yaw[5] = 0.0;\n+        # R_yaw[6] = 0.0;\n+        # R_yaw[7] = 0.0;\n+        # R_yaw[8] = 1.0;\n \n-        double R_pitch[9];\n-        R_pitch[0] = cos(Pitch);\n-        R_pitch[1] = 0.0;\n-        R_pitch[2] = sin(Pitch);\n-        R_pitch[3] = 0.0;\n-        R_pitch[4] = 1.0;\n-        R_pitch[5] = 0.0;\n-        R_pitch[6] = -sin(Pitch);\n-        R_pitch[7] = 0.0;\n-        R_pitch[8] = cos(Pitch);\n+        # double R_pitch[9];\n+        # R_pitch[0] = cos(Pitch);\n+        # R_pitch[1] = 0.0;\n+        # R_pitch[2] = sin(Pitch);\n+        # R_pitch[3] = 0.0;\n+        # R_pitch[4] = 1.0;\n+        # R_pitch[5] = 0.0;\n+        # R_pitch[6] = -sin(Pitch);\n+        # R_pitch[7] = 0.0;\n+        # R_pitch[8] = cos(Pitch);\n \n-        double R_roll[9];\n-        R_roll[0] = 1.0;\n-        R_roll[1] = 0.0;\n-        R_roll[2] = 0.0;\n-        R_roll[3] = 0.0;\n-        R_roll[4] = cos(Roll);\n-        R_roll[5] = -sin(Roll);\n-        R_roll[6] = 0.0;\n-        R_roll[7] = sin(Roll);\n-        R_roll[8] = cos(Roll);\n+        # double R_roll[9];\n+        # R_roll[0] = 1.0;\n+        # R_roll[1] = 0.0;\n+        # R_roll[2] = 0.0;\n+        # R_roll[3] = 0.0;\n+        # R_roll[4] = cos(Roll);\n+        # R_roll[5] = -sin(Roll);\n+        # R_roll[6] = 0.0;\n+        # R_roll[7] = sin(Roll);\n+        # R_roll[8] = cos(Roll);\n         \n-        double tmp_mat3x3[9];\n-        matrixMultiply_3x3(R_yaw, R_pitch, tmp_mat3x3);\n-        matrixMultiply_3x3(tmp_mat3x3, R_roll, RGyro);\n+        # double tmp_mat3x3[9];\n+        # matrixMultiply_3x3(R_yaw, R_pitch, tmp_mat3x3);\n+        # matrixMultiply_3x3(tmp_mat3x3, R_roll, RGyro);\n \n \n     def _construct_action_space(self):\n         # action_low = -1 * np.ones(12)\n"
                },
                {
                    "date": 1691985906426,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -227,8 +227,9 @@\n         \n         # double tmp_mat3x3[9];\n         # matrixMultiply_3x3(R_yaw, R_pitch, tmp_mat3x3);\n         # matrixMultiply_3x3(tmp_mat3x3, R_roll, RGyro);\n+        return rot_mat\n \n \n     def _construct_action_space(self):\n         # action_low = -1 * np.ones(12)\n"
                },
                {
                    "date": 1692151403142,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -192,8 +192,9 @@\n     def GetBodyRotationMatrix(self, roll, pitch, yaw):\n         rot_mat = np.zeros(9)\n         rot_mat[0] = np.cos(yaw)\n         rot_mat[1] = -np.sin(yaw)\n+        rot_mat[2] = 0.0\n         \n         \n         # R_yaw[2] = 0.0;\n         # R_yaw[3] = sin(Yaw);\n"
                },
                {
                    "date": 1692152152735,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -193,8 +193,16 @@\n         rot_mat = np.zeros(9)\n         rot_mat[0] = np.cos(yaw)\n         rot_mat[1] = -np.sin(yaw)\n         rot_mat[2] = 0.0\n+\n+        rot_mat[3] = np.sin(yaw)\n+        rot_mat[4] = np.cos(yaw)\n+        rot_mat[5] = 0.0\n+\n+        rot_mat[6] = 0.0\n+        rot_mat[7] = 0.0\n+        rot_mat[8] = 1.0\n         \n         \n         # R_yaw[2] = 0.0;\n         # R_yaw[3] = sin(Yaw);\n"
                },
                {
                    "date": 1692152258538,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -189,41 +189,43 @@\n         self.quaternion_goal[3] = z\n         # return np.array([w, x, y, z])\n \n     def GetBodyRotationMatrix(self, roll, pitch, yaw):\n-        rot_mat = np.zeros(9)\n-        rot_mat[0] = np.cos(yaw)\n-        rot_mat[1] = -np.sin(yaw)\n-        rot_mat[2] = 0.0\n+        R_yaw = np.zeros(9)\n+        R_pitch = np.zeros(9)\n+        R_roll = np.zeros(9)\n \n-        rot_mat[3] = np.sin(yaw)\n-        rot_mat[4] = np.cos(yaw)\n-        rot_mat[5] = 0.0\n+        R_yaw[0] = np.cos(yaw)\n+        R_yaw[1] = -np.sin(yaw)\n+        R_yaw[2] = 0.0\n+        R_yaw[3] = np.sin(yaw)\n+        R_yaw[4] = np.cos(yaw)\n+        R_yaw[5] = 0.0\n+        R_yaw[6] = 0.0\n+        R_yaw[7] = 0.0\n+        R_yaw[8] = 1.0\n \n-        rot_mat[6] = 0.0\n-        rot_mat[7] = 0.0\n-        rot_mat[8] = 1.0\n+        R_pitch[0] = np.cos(pitch)\n+        R_pitch[1] = 0.0\n+        R_pitch[2] = np.sin(pitch)\n+        R_pitch[3] = 0.0\n+        R_pitch[4] = 1.0\n+        R_pitch[5] = 0.0\n+        R_pitch[6] = -np.sin(pitch)\n+        R_pitch[7] = 0.0\n+        R_pitch[8] = np.cos(pitch)\n+\n+        R_roll[0] = np.cos(pitch)\n+        R_roll[1] = 0.0\n+        R_roll[2] = np.sin(pitch)\n+        R_roll[3] = 0.0\n+        R_roll[4] = 1.0\n+        R_roll[5] = 0.0\n+        R_roll[6] = -np.sin(pitch)\n+        R_roll[7] = 0.0\n+        R_roll[8] = np.cos(pitch)\n         \n-        \n-        # R_yaw[2] = 0.0;\n-        # R_yaw[3] = sin(Yaw);\n-        # R_yaw[4] = cos(Yaw);\n-        # R_yaw[5] = 0.0;\n-        # R_yaw[6] = 0.0;\n-        # R_yaw[7] = 0.0;\n-        # R_yaw[8] = 1.0;\n \n-        # double R_pitch[9];\n-        # R_pitch[0] = cos(Pitch);\n-        # R_pitch[1] = 0.0;\n-        # R_pitch[2] = sin(Pitch);\n-        # R_pitch[3] = 0.0;\n-        # R_pitch[4] = 1.0;\n-        # R_pitch[5] = 0.0;\n-        # R_pitch[6] = -sin(Pitch);\n-        # R_pitch[7] = 0.0;\n-        # R_pitch[8] = cos(Pitch);\n-\n         # double R_roll[9];\n         # R_roll[0] = 1.0;\n         # R_roll[1] = 0.0;\n         # R_roll[2] = 0.0;\n"
                },
                {
                    "date": 1692152299628,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -213,17 +213,17 @@\n         R_pitch[6] = -np.sin(pitch)\n         R_pitch[7] = 0.0\n         R_pitch[8] = np.cos(pitch)\n \n-        R_roll[0] = np.cos(pitch)\n+        R_roll[0] = 1.0\n         R_roll[1] = 0.0\n-        R_roll[2] = np.sin(pitch)\n+        R_roll[2] = 0.0\n         R_roll[3] = 0.0\n-        R_roll[4] = 1.0\n-        R_roll[5] = 0.0\n-        R_roll[6] = -np.sin(pitch)\n-        R_roll[7] = 0.0\n-        R_roll[8] = np.cos(pitch)\n+        R_roll[4] = np.cos(roll)\n+        R_roll[5] = -np.sin(roll)\n+        R_roll[6] = 0.0\n+        R_roll[7] = np.sin(roll)\n+        R_roll[8] = np.cos(roll)\n         \n \n         # double R_roll[9];\n         # R_roll[0] = 1.0;\n"
                },
                {
                    "date": 1692152309648,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -192,8 +192,9 @@\n     def GetBodyRotationMatrix(self, roll, pitch, yaw):\n         R_yaw = np.zeros(9)\n         R_pitch = np.zeros(9)\n         R_roll = np.zeros(9)\n+        tmp_mat3x3 = np.zeros(9)\n \n         R_yaw[0] = np.cos(yaw)\n         R_yaw[1] = -np.sin(yaw)\n         R_yaw[2] = 0.0\n@@ -223,20 +224,8 @@\n         R_roll[6] = 0.0\n         R_roll[7] = np.sin(roll)\n         R_roll[8] = np.cos(roll)\n         \n-\n-        # double R_roll[9];\n-        # R_roll[0] = 1.0;\n-        # R_roll[1] = 0.0;\n-        # R_roll[2] = 0.0;\n-        # R_roll[3] = 0.0;\n-        # R_roll[4] = cos(Roll);\n-        # R_roll[5] = -sin(Roll);\n-        # R_roll[6] = 0.0;\n-        # R_roll[7] = sin(Roll);\n-        # R_roll[8] = cos(Roll);\n-        \n         # double tmp_mat3x3[9];\n         # matrixMultiply_3x3(R_yaw, R_pitch, tmp_mat3x3);\n         # matrixMultiply_3x3(tmp_mat3x3, R_roll, RGyro);\n         return rot_mat\n"
                },
                {
                    "date": 1692152353446,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -223,10 +223,10 @@\n         R_roll[5] = -np.sin(roll)\n         R_roll[6] = 0.0\n         R_roll[7] = np.sin(roll)\n         R_roll[8] = np.cos(roll)\n-        \n-        # double tmp_mat3x3[9];\n+\n+        tmp_mat3x3 = \n         # matrixMultiply_3x3(R_yaw, R_pitch, tmp_mat3x3);\n         # matrixMultiply_3x3(tmp_mat3x3, R_roll, RGyro);\n         return rot_mat\n \n@@ -236,8 +236,10 @@\n         action_low =  -1 * np.ones(1)\n         action_high = np.ones(1) \n         return gym.spaces.Box(low=action_low, high=action_high, dtype=np.float64)\n     \n+    def matrixMultiply_3x3(self, mat_A, mat_B):\n+        \n     \n     def _construct_observation_space(self):\n         obs_low = -1 * np.ones(13) # X(6), X_goal(6), action(1) \n         obs_high = 1 * np.ones(13)\n"
                },
                {
                    "date": 1692152388011,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -237,8 +237,20 @@\n         action_high = np.ones(1) \n         return gym.spaces.Box(low=action_low, high=action_high, dtype=np.float64)\n     \n     def matrixMultiply_3x3(self, mat_A, mat_B):\n+        mat3 = np.zeros(9)\n+        mat3[0] = mat1[0] * mat2[0] + mat1[1] * mat2[3] + mat1[2] * mat2[6];\n+        mat3[1] = mat1[0] * mat2[1] + mat1[1] * mat2[4] + mat1[2] * mat2[7];\n+        mat3[2] = mat1[0] * mat2[2] + mat1[1] * mat2[5] + mat1[2] * mat2[8];\n+\n+        mat3[3] = mat1[3] * mat2[0] + mat1[4] * mat2[3] + mat1[5] * mat2[6];\n+        mat3[4] = mat1[3] * mat2[1] + mat1[4] * mat2[4] + mat1[5] * mat2[7];\n+        mat3[5] = mat1[3] * mat2[2] + mat1[4] * mat2[5] + mat1[5] * mat2[8];\n+\n+        mat3[6] = mat1[6] * mat2[0] + mat1[7] * mat2[3] + mat1[8] * mat2[6];\n+        mat3[7] = mat1[6] * mat2[1] + mat1[7] * mat2[4] + mat1[8] * mat2[7];\n+        mat3[8] = mat1[6] * mat2[2] + mat1[7] * mat2[5] + mat1[8] * mat2[8];\n         \n     \n     def _construct_observation_space(self):\n         obs_low = -1 * np.ones(13) # X(6), X_goal(6), action(1) \n"
                },
                {
                    "date": 1692152405153,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -238,19 +238,19 @@\n         return gym.spaces.Box(low=action_low, high=action_high, dtype=np.float64)\n     \n     def matrixMultiply_3x3(self, mat_A, mat_B):\n         mat3 = np.zeros(9)\n-        mat3[0] = mat1[0] * mat2[0] + mat1[1] * mat2[3] + mat1[2] * mat2[6];\n-        mat3[1] = mat1[0] * mat2[1] + mat1[1] * mat2[4] + mat1[2] * mat2[7];\n-        mat3[2] = mat1[0] * mat2[2] + mat1[1] * mat2[5] + mat1[2] * mat2[8];\n+        mat3[0] = mat_A[0] * mat_B[0] + mat_A[1] * mat_B[3] + mat_A[2] * mat_B[6];\n+        mat3[1] = mat_A[0] * mat_B[1] + mat_A[1] * mat_B[4] + mat_A[2] * mat_B[7];\n+        mat3[2] = mat_A[0] * mat_B[2] + mat_A[1] * mat_B[5] + mat_A[2] * mat_B[8];\n \n-        mat3[3] = mat1[3] * mat2[0] + mat1[4] * mat2[3] + mat1[5] * mat2[6];\n-        mat3[4] = mat1[3] * mat2[1] + mat1[4] * mat2[4] + mat1[5] * mat2[7];\n-        mat3[5] = mat1[3] * mat2[2] + mat1[4] * mat2[5] + mat1[5] * mat2[8];\n+        mat3[3] = mat_A[3] * mat_B[0] + mat_A[4] * mat_B[3] + mat_A[5] * mat_B[6];\n+        mat3[4] = mat_A[3] * mat_B[1] + mat_A[4] * mat_B[4] + mat_A[5] * mat_B[7];\n+        mat3[5] = mat_A[3] * mat_B[2] + mat_A[4] * mat_B[5] + mat_A[5] * mat_B[8];\n \n-        mat3[6] = mat1[6] * mat2[0] + mat1[7] * mat2[3] + mat1[8] * mat2[6];\n-        mat3[7] = mat1[6] * mat2[1] + mat1[7] * mat2[4] + mat1[8] * mat2[7];\n-        mat3[8] = mat1[6] * mat2[2] + mat1[7] * mat2[5] + mat1[8] * mat2[8];\n+        mat3[6] = mat_A[6] * mat_B[0] + mat_A[7] * mat_B[3] + mat_A[8] * mat_B[6];\n+        mat3[7] = mat_A[6] * mat_B[1] + mat_A[7] * mat_B[4] + mat_A[8] * mat_B[7];\n+        mat3[8] = mat_A[6] * mat_B[2] + mat_A[7] * mat_B[5] + mat_A[8] * mat_B[8];\n         \n     \n     def _construct_observation_space(self):\n         obs_low = -1 * np.ones(13) # X(6), X_goal(6), action(1) \n"
                },
                {
                    "date": 1692152483347,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -249,8 +249,9 @@\n \n         mat3[6] = mat_A[6] * mat_B[0] + mat_A[7] * mat_B[3] + mat_A[8] * mat_B[6];\n         mat3[7] = mat_A[6] * mat_B[1] + mat_A[7] * mat_B[4] + mat_A[8] * mat_B[7];\n         mat3[8] = mat_A[6] * mat_B[2] + mat_A[7] * mat_B[5] + mat_A[8] * mat_B[8];\n+        return mat3\n         \n     \n     def _construct_observation_space(self):\n         obs_low = -1 * np.ones(13) # X(6), X_goal(6), action(1) \n"
                },
                {
                    "date": 1692152549026,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -193,8 +193,9 @@\n         R_yaw = np.zeros(9)\n         R_pitch = np.zeros(9)\n         R_roll = np.zeros(9)\n         tmp_mat3x3 = np.zeros(9)\n+        rot_mat = np.zeros(9)\n \n         R_yaw[0] = np.cos(yaw)\n         R_yaw[1] = -np.sin(yaw)\n         R_yaw[2] = 0.0\n@@ -224,9 +225,10 @@\n         R_roll[6] = 0.0\n         R_roll[7] = np.sin(roll)\n         R_roll[8] = np.cos(roll)\n \n-        tmp_mat3x3 = \n+        tmp_mat3x3 = matrixMultiply_3x3(R_yaw, R_pitch)\n+        rot_mat = matrixMultiply_3x3(tmp_mat3x3, R_roll)\n         # matrixMultiply_3x3(R_yaw, R_pitch, tmp_mat3x3);\n         # matrixMultiply_3x3(tmp_mat3x3, R_roll, RGyro);\n         return rot_mat\n \n"
                },
                {
                    "date": 1692152556248,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -227,10 +227,8 @@\n         R_roll[8] = np.cos(roll)\n \n         tmp_mat3x3 = matrixMultiply_3x3(R_yaw, R_pitch)\n         rot_mat = matrixMultiply_3x3(tmp_mat3x3, R_roll)\n-        # matrixMultiply_3x3(R_yaw, R_pitch, tmp_mat3x3);\n-        # matrixMultiply_3x3(tmp_mat3x3, R_roll, RGyro);\n         return rot_mat\n \n \n     def _construct_action_space(self):\n"
                },
                {
                    "date": 1692152561993,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -263,9 +263,8 @@\n      \n     \n     def get_observation(self):\n         # self.sim_state = self.sim.get_state()\n-        \n         self.position[0] = self.X[0] / 0.6 # -0.6~0.6\n         self.position[1] = self.X[1] / 0.6 # -0.6~0.6\n         self.position[2] = self.X[2] -0.35 # 0.1~0.8\n         self.position[2] = self.position[2] / 0.45\n"
                },
                {
                    "date": 1692152597203,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -225,10 +225,10 @@\n         R_roll[6] = 0.0\n         R_roll[7] = np.sin(roll)\n         R_roll[8] = np.cos(roll)\n \n-        tmp_mat3x3 = matrixMultiply_3x3(R_yaw, R_pitch)\n-        rot_mat = matrixMultiply_3x3(tmp_mat3x3, R_roll)\n+        tmp_mat3x3 = self.matrixMultiply_3x3(R_yaw, R_pitch)\n+        rot_mat = self.matrixMultiply_3x3(tmp_mat3x3, R_roll)\n         return rot_mat\n \n \n     def _construct_action_space(self):\n"
                },
                {
                    "date": 1692152657286,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -111,8 +111,9 @@\n         # action = action + 0.003\n         self.controller.read(self.data.time, self.data.qpos[0:self.k], self.data.qvel[0:self.k], action)\n         self.controller.control_mujoco()\n         self.tmp_X = self.controller.state_controller()\n+        # GetBodyRotationMatrix\n         self.torque = self.controller.write()\n         for i in range(self.k):\n                 self.data.ctrl[i] = self.torque[i]\n \n"
                },
                {
                    "date": 1692152730108,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -111,13 +111,13 @@\n         # action = action + 0.003\n         self.controller.read(self.data.time, self.data.qpos[0:self.k], self.data.qvel[0:self.k], action)\n         self.controller.control_mujoco()\n         self.tmp_X = self.controller.state_controller()\n+        \n         # GetBodyRotationMatrix\n         self.torque = self.controller.write()\n         for i in range(self.k):\n                 self.data.ctrl[i] = self.torque[i]\n-\n         for i in range(6):\n             self.X_goal[i] = self.tmp_X[i]\n             # self.data.qpos[0:self.k]\n         for i in range(6):\n"
                },
                {
                    "date": 1692152998159,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -111,8 +111,12 @@\n         # action = action + 0.003\n         self.controller.read(self.data.time, self.data.qpos[0:self.k], self.data.qvel[0:self.k], action)\n         self.controller.control_mujoco()\n         self.tmp_X = self.controller.state_controller()\n+        self.rot_mat_pre = self.GetBodyRotationMatrix()\n+\n+\n+        rot_mat_pre\n         \n         # GetBodyRotationMatrix\n         self.torque = self.controller.write()\n         for i in range(self.k):\n@@ -122,9 +126,9 @@\n             # self.data.qpos[0:self.k]\n         for i in range(6):\n             self.X[i] = self.tmp_X[i+6]\n         for i in range(6):\n-            self.q[i] = self.tmp_X[i+6]\n+            self.q[i] = self.tmp_X[i+12]\n         self.euler_to_quaternion(self.X_goal[3],self.X_goal[4],self.X_goal[5])\n         # print(self.data.qpos)\n         # print(self.data.xpos[13,0])\n         # xpos랑 xmat을 건드려야함\n"
                },
                {
                    "date": 1692153053289,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -111,9 +111,10 @@\n         # action = action + 0.003\n         self.controller.read(self.data.time, self.data.qpos[0:self.k], self.data.qvel[0:self.k], action)\n         self.controller.control_mujoco()\n         self.tmp_X = self.controller.state_controller()\n-        self.rot_mat_pre = self.GetBodyRotationMatrix()\n+        self.rot_mat_pre = self.GetBodyRotationMatrix(self.tmp_X[9],self.tmp_X[10],self.tmp_X[11])\n+        self.rot_mat_goal = self.GetBodyRotationMatrix(self.tmp_X[9],self.tmp_X[10],self.tmp_X[11])\n \n \n         rot_mat_pre\n         \n@@ -125,10 +126,11 @@\n             self.X_goal[i] = self.tmp_X[i]\n             # self.data.qpos[0:self.k]\n         for i in range(6):\n             self.X[i] = self.tmp_X[i+6]\n-        for i in range(6):\n+        for i in range(7):\n             self.q[i] = self.tmp_X[i+12]\n+\n         self.euler_to_quaternion(self.X_goal[3],self.X_goal[4],self.X_goal[5])\n         # print(self.data.qpos)\n         # print(self.data.xpos[13,0])\n         # xpos랑 xmat을 건드려야함\n"
                },
                {
                    "date": 1692153076595,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -111,13 +111,10 @@\n         # action = action + 0.003\n         self.controller.read(self.data.time, self.data.qpos[0:self.k], self.data.qvel[0:self.k], action)\n         self.controller.control_mujoco()\n         self.tmp_X = self.controller.state_controller()\n+        self.rot_mat_goal = self.GetBodyRotationMatrix(self.tmp_X[3],self.tmp_X[4],self.tmp_X[5])\n         self.rot_mat_pre = self.GetBodyRotationMatrix(self.tmp_X[9],self.tmp_X[10],self.tmp_X[11])\n-        self.rot_mat_goal = self.GetBodyRotationMatrix(self.tmp_X[9],self.tmp_X[10],self.tmp_X[11])\n-\n-\n-        rot_mat_pre\n         \n         # GetBodyRotationMatrix\n         self.torque = self.controller.write()\n         for i in range(self.k):\n"
                },
                {
                    "date": 1692153419644,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -284,8 +284,36 @@\n \n         self.goal[3] = self.X_goal[3] / np.pi\n         self.goal[4] = self.X_goal[4] / np.pi\n         self.goal[5] = self.X_goal[5] / np.pi\n+\n+        self.q_pos[0] = self.q[0] / 2.8973\n+        self.q_pos[1] = self.q[1] / 1.7628\n+        self.q_pos[2] = self.q[2] / 2.8973\n+        self.q_pos[3] = self.q[3] + (1.5708+0.0698)\n+        self.q_pos[3] = self.q_pos[3] / 1.5708\n+        self.q_pos[4] = self.q[4] / 2.8973\n+        self.q_pos[5] = self.q[5] - (3.7525-1.885)\n+        self.q_pos[5] = self.q_pos[5] / 1.885\n+        self.q_pos[6] = self.q[6] / 2.8973\n+\n+\n+        self._min_joint_position[0] = -2.8973\n+        self._min_joint_position[1] = -1.7628\n+        self._min_joint_position[2] = -2.8973\n+        self._min_joint_position[3] = -3.0718\n+        self._min_joint_position[4] = -2.8973\n+        self._min_joint_position[5] = -0.0175\n+        self._min_joint_position[6] = -2.8973\n+\n+        self._max_joint_position[0] = 2.8973\n+        self._max_joint_position[1] = 1.7628\n+        self._max_joint_position[2] = 2.8973\n+        self._max_joint_position[3] = -0.0698\n+        self._max_joint_position[4] = 2.8973\n+        self._max_joint_position[5] = 3.7525\n+        self._max_joint_position[6] = 2.8973\n+\n         # goal = # controller로부터 받은 goal\n         # self.temp_action = self.temp_action * 10\n         # self.temp_action = self.temp_action - 1\n         return np.concatenate(\n"
                },
                {
                    "date": 1692153432388,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -295,32 +295,16 @@\n         self.q_pos[5] = self.q[5] - (3.7525-1.885)\n         self.q_pos[5] = self.q_pos[5] / 1.885\n         self.q_pos[6] = self.q[6] / 2.8973\n \n-\n-        self._min_joint_position[0] = -2.8973\n-        self._min_joint_position[1] = -1.7628\n-        self._min_joint_position[2] = -2.8973\n-        self._min_joint_position[3] = -3.0718\n-        self._min_joint_position[4] = -2.8973\n-        self._min_joint_position[5] = -0.0175\n-        self._min_joint_position[6] = -2.8973\n-\n-        self._max_joint_position[0] = 2.8973\n-        self._max_joint_position[1] = 1.7628\n-        self._max_joint_position[2] = 2.8973\n-        self._max_joint_position[3] = -0.0698\n-        self._max_joint_position[4] = 2.8973\n-        self._max_joint_position[5] = 3.7525\n-        self._max_joint_position[6] = 2.8973\n-\n         # goal = # controller로부터 받은 goal\n         # self.temp_action = self.temp_action * 10\n         # self.temp_action = self.temp_action - 1\n         return np.concatenate(\n         (\n             self.position, #6\n             self.goal, #6\n+            self.q_pos #7\n             self.temp_action, # 1\n         )\n         )\n         \n"
                },
                {
                    "date": 1692153453801,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -257,10 +257,10 @@\n         return mat3\n         \n     \n     def _construct_observation_space(self):\n-        obs_low = -1 * np.ones(13) # X(6), X_goal(6), action(1) \n-        obs_high = 1 * np.ones(13)\n+        obs_low = -1 * np.ones(20) # X(6), X_goal(6), action(1) \n+        obs_high = 1 * np.ones(20)\n         # obs_low = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0,0,0, -1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1]\n         # obs_high = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0,0,0, 1,1,1,1,1,1,1,1,1,1,1,1]\n         return gym.spaces.Box(obs_low, obs_high, dtype=np.float64)\n     \n@@ -302,9 +302,9 @@\n         return np.concatenate(\n         (\n             self.position, #6\n             self.goal, #6\n-            self.q_pos #7\n+            self.q_pos, #7\n             self.temp_action, # 1\n         )\n         )\n         \n"
                },
                {
                    "date": 1692153533475,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -34,8 +34,9 @@\n         self.quaternion_goal = np.zeros(4)\n \n         self.position = np.zeros(6)\n         self.goal = np.zeros(6)\n+        self.q_pos = np.zeros(7)\n         # self.reward_arr = np.zeros(2) #shape=(2,), dtype=np.float64\n         self.reward_arr = [0,0,0]\n         self.cnt = 1\n         self.temp_action = np.zeros(1)\n@@ -442,8 +443,11 @@\n \n     def reward(self):\n         #Reaching Goal Poses (Cost Function)\n         tmp_reward = 0\n+\n+        self.rot_mat_goal\n+        self.rot_mat_pre\n         for i in range(6):\n             if self.X_goal[i] == self.X[i]:  # 분모가 0인 경우 처리\n                 tmp_reward += 1000\n             else:\n"
                },
                {
                    "date": 1692153587915,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -32,9 +32,16 @@\n         self.rot_mat_goal = np.zeros(9)\n         self.rot_mat_pre = np.zeros(9)\n         self.quaternion_goal = np.zeros(4)\n \n-        self.position = np.zeros(6)\n+        self.x_pos\n+x_pos\n+x_pos\n+x_pos\n+x_pos\n+x_pos\n+x_pos\n+x_pos = np.zeros(6)\n         self.goal = np.zeros(6)\n         self.q_pos = np.zeros(7)\n         # self.reward_arr = np.zeros(2) #shape=(2,), dtype=np.float64\n         self.reward_arr = [0,0,0]\n@@ -268,16 +275,16 @@\n      \n     \n     def get_observation(self):\n         # self.sim_state = self.sim.get_state()\n-        self.position[0] = self.X[0] / 0.6 # -0.6~0.6\n-        self.position[1] = self.X[1] / 0.6 # -0.6~0.6\n-        self.position[2] = self.X[2] -0.35 # 0.1~0.8\n-        self.position[2] = self.position[2] / 0.45\n+        self.x_pos[0] = self.X[0] / 0.6 # -0.6~0.6\n+        self.x_pos[1] = self.X[1] / 0.6 # -0.6~0.6\n+        self.x_pos[2] = self.X[2] -0.35 # 0.1~0.8\n+        self.x_pos[2] = self.x_pos[2] / 0.45\n \n-        self.position[3] = self.X[3] / np.pi # -np.pi~np.pi\n-        self.position[4] = self.X[4] / np.pi #      \"\n-        self.position[5] = self.X[5] / np.pi #      \"\n+        self.x_pos[3] = self.X[3] / np.pi # -np.pi~np.pi\n+        self.x_pos[4] = self.X[4] / np.pi #      \"\n+        self.x_pos[5] = self.X[5] / np.pi #      \"\n \n         self.goal[0] = self.X_goal[0] / 0.6\n         self.goal[1] = self.X_goal[1] / 0.6\n         self.goal[2] = self.X_goal[2] -0.35\n"
                },
                {
                    "date": 1692153599234,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -32,17 +32,11 @@\n         self.rot_mat_goal = np.zeros(9)\n         self.rot_mat_pre = np.zeros(9)\n         self.quaternion_goal = np.zeros(4)\n \n-        self.x_pos\n-x_pos\n-x_pos\n-x_pos\n-x_pos\n-x_pos\n-x_pos\n-x_pos = np.zeros(6)\n+        #self.x_pos = np.zeros(6)\n         self.goal = np.zeros(6)\n+        self.x_pos = np.zeros(6)\n         self.q_pos = np.zeros(7)\n         # self.reward_arr = np.zeros(2) #shape=(2,), dtype=np.float64\n         self.reward_arr = [0,0,0]\n         self.cnt = 1\n"
                },
                {
                    "date": 1692153694444,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -274,20 +274,32 @@\n         self.x_pos[1] = self.X[1] / 0.6 # -0.6~0.6\n         self.x_pos[2] = self.X[2] -0.35 # 0.1~0.8\n         self.x_pos[2] = self.x_pos[2] / 0.45\n \n-        self.x_pos[3] = self.X[3] / np.pi # -np.pi~np.pi\n-        self.x_pos[4] = self.X[4] / np.pi #      \"\n-        self.x_pos[5] = self.X[5] / np.pi #      \"\n+        # self.x_pos[3] = self.X[3] / np.pi # -np.pi~np.pi\n+        # self.x_pos[4] = self.X[4] / np.pi #      \"\n+        # self.x_pos[5] = self.X[5] / np.pi #      \"\n+        self.x_pos[3] = self.rot_mat_pre[0]\n+        self.x_pos[4] = self.rot_mat_pre[1]\n+        self.x_pos[5] = self.rot_mat_pre[3]\n+        self.x_pos[6] = self.rot_mat_pre[4]\n+        self.x_pos[7] = self.rot_mat_pre[6]\n+        self.x_pos[8] = self.rot_mat_pre[7]\n \n         self.goal[0] = self.X_goal[0] / 0.6\n         self.goal[1] = self.X_goal[1] / 0.6\n         self.goal[2] = self.X_goal[2] -0.35\n         self.goal[2] = self.goal[2] / 0.45\n \n-        self.goal[3] = self.X_goal[3] / np.pi\n-        self.goal[4] = self.X_goal[4] / np.pi\n-        self.goal[5] = self.X_goal[5] / np.pi\n+        # self.goal[3] = self.X_goal[3] / np.pi\n+        # self.goal[4] = self.X_goal[4] / np.pi\n+        # self.goal[5] = self.X_goal[5] / np.pi\n+        self.goal[3] = self.rot_mat_goal[0]\n+        self.goal[4] = self.rot_mat_goal[1]\n+        self.goal[5] = self.rot_mat_goal[3]\n+        self.goal[6] = self.rot_mat_goal[4]\n+        self.goal[7] = self.rot_mat_goal[6]\n+        self.goal[8] = self.rot_mat_goal[7]\n \n         self.q_pos[0] = self.q[0] / 2.8973\n         self.q_pos[1] = self.q[1] / 1.7628\n         self.q_pos[2] = self.q[2] / 2.8973\n"
                },
                {
                    "date": 1692153703907,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -314,10 +314,10 @@\n         # self.temp_action = self.temp_action * 10\n         # self.temp_action = self.temp_action - 1\n         return np.concatenate(\n         (\n-            self.position, #6\n-            self.goal, #6\n+            self.x_pos, #9\n+            self.goal, #9\n             self.q_pos, #7\n             self.temp_action, # 1\n         )\n         )\n"
                },
                {
                    "date": 1692153709933,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -259,10 +259,10 @@\n         return mat3\n         \n     \n     def _construct_observation_space(self):\n-        obs_low = -1 * np.ones(20) # X(6), X_goal(6), action(1) \n-        obs_high = 1 * np.ones(20)\n+        obs_low = -1 * np.ones(24) # X(6), X_goal(6), action(1) \n+        obs_high = 1 * np.ones(24)\n         # obs_low = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0,0,0, -1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1]\n         # obs_high = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0,0,0, 1,1,1,1,1,1,1,1,1,1,1,1]\n         return gym.spaces.Box(obs_low, obs_high, dtype=np.float64)\n     \n"
                },
                {
                    "date": 1692153728834,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -33,10 +33,10 @@\n         self.rot_mat_pre = np.zeros(9)\n         self.quaternion_goal = np.zeros(4)\n \n         #self.x_pos = np.zeros(6)\n-        self.goal = np.zeros(6)\n-        self.x_pos = np.zeros(6)\n+        self.goal = np.zeros(9)\n+        self.x_pos = np.zeros(9)\n         self.q_pos = np.zeros(7)\n         # self.reward_arr = np.zeros(2) #shape=(2,), dtype=np.float64\n         self.reward_arr = [0,0,0]\n         self.cnt = 1\n"
                },
                {
                    "date": 1692153746536,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -459,9 +459,9 @@\n         tmp_reward = 0\n \n         self.rot_mat_goal\n         self.rot_mat_pre\n-        for i in range(6):\n+        for i in range(3):\n             if self.X_goal[i] == self.X[i]:  # 분모가 0인 경우 처리\n                 tmp_reward += 1000\n             else:\n                 tmp_reward += 1 / (self.X_goal[i] - self.X[i])\n"
                },
                {
                    "date": 1692153757766,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -464,8 +464,10 @@\n             if self.X_goal[i] == self.X[i]:  # 분모가 0인 경우 처리\n                 tmp_reward += 1000\n             else:\n                 tmp_reward += 1 / (self.X_goal[i] - self.X[i])\n+\n+        tmp_reward2 = 0\n         reward = tmp_reward\n         return reward\n     \n     def seed(self, seed=None):\n"
                },
                {
                    "date": 1692153778219,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -466,8 +466,13 @@\n             else:\n                 tmp_reward += 1 / (self.X_goal[i] - self.X[i])\n \n         tmp_reward2 = 0\n+        for i in range(6):\n+            if self.X_goal[i+3] == self.X[i]:  # 분모가 0인 경우 처리\n+                tmp_reward += 1000\n+            else:\n+                tmp_reward += 1 / (self.X_goal[i] - self.X[i])\n         reward = tmp_reward\n         return reward\n     \n     def seed(self, seed=None):\n"
                },
                {
                    "date": 1692153799486,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -467,9 +467,9 @@\n                 tmp_reward += 1 / (self.X_goal[i] - self.X[i])\n \n         tmp_reward2 = 0\n         for i in range(6):\n-            if self.X_goal[i+3] == self.X[i]:  # 분모가 0인 경우 처리\n+            if self.rot_mat_goal[i] == self.rot_mat_pre[i]:  # 분모가 0인 경우 처리\n                 tmp_reward += 1000\n             else:\n                 tmp_reward += 1 / (self.X_goal[i] - self.X[i])\n         reward = tmp_reward\n"
                },
                {
                    "date": 1692153817643,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -468,12 +468,12 @@\n \n         tmp_reward2 = 0\n         for i in range(6):\n             if self.rot_mat_goal[i] == self.rot_mat_pre[i]:  # 분모가 0인 경우 처리\n-                tmp_reward += 1000\n+                tmp_reward2 += 1000\n             else:\n-                tmp_reward += 1 / (self.X_goal[i] - self.X[i])\n-        reward = tmp_reward\n+                tmp_reward2 += 1 / (self.X_goal[i] - self.X[i])\n+        reward = tmp_reward + tmp_reward2\n         return reward\n     \n     def seed(self, seed=None):\n         return \n\\ No newline at end of file\n"
                },
                {
                    "date": 1692153826024,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -456,18 +456,16 @@\n \n     def reward(self):\n         #Reaching Goal Poses (Cost Function)\n         tmp_reward = 0\n-\n-        self.rot_mat_goal\n-        self.rot_mat_pre\n+        tmp_reward2 = 0\n         for i in range(3):\n             if self.X_goal[i] == self.X[i]:  # 분모가 0인 경우 처리\n                 tmp_reward += 1000\n             else:\n                 tmp_reward += 1 / (self.X_goal[i] - self.X[i])\n \n-        tmp_reward2 = 0\n+        \n         for i in range(6):\n             if self.rot_mat_goal[i] == self.rot_mat_pre[i]:  # 분모가 0인 경우 처리\n                 tmp_reward2 += 1000\n             else:\n"
                },
                {
                    "date": 1692153834467,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -457,8 +457,9 @@\n     def reward(self):\n         #Reaching Goal Poses (Cost Function)\n         tmp_reward = 0\n         tmp_reward2 = 0\n+\n         for i in range(3):\n             if self.X_goal[i] == self.X[i]:  # 분모가 0인 경우 처리\n                 tmp_reward += 1000\n             else:\n@@ -468,9 +469,9 @@\n         for i in range(6):\n             if self.rot_mat_goal[i] == self.rot_mat_pre[i]:  # 분모가 0인 경우 처리\n                 tmp_reward2 += 1000\n             else:\n-                tmp_reward2 += 1 / (self.X_goal[i] - self.X[i])\n+                tmp_reward2 += 1 / (self.rot_mat_goal[i] - self.rot_mat_pre[i])\n         reward = tmp_reward + tmp_reward2\n         return reward\n     \n     def seed(self, seed=None):\n"
                },
                {
                    "date": 1692153849552,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -465,9 +465,9 @@\n             else:\n                 tmp_reward += 1 / (self.X_goal[i] - self.X[i])\n \n         \n-        for i in range(6):\n+        for i in range(9):\n             if self.rot_mat_goal[i] == self.rot_mat_pre[i]:  # 분모가 0인 경우 처리\n                 tmp_reward2 += 1000\n             else:\n                 tmp_reward2 += 1 / (self.rot_mat_goal[i] - self.rot_mat_pre[i])\n"
                },
                {
                    "date": 1692153903366,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -466,13 +466,18 @@\n                 tmp_reward += 1 / (self.X_goal[i] - self.X[i])\n \n         \n         for i in range(9):\n-            if self.rot_mat_goal[i] == self.rot_mat_pre[i]:  # 분모가 0인 경우 처리\n+            if i == 2 or i == 5 or i == 8:\n+\n+            else :\n+                if self.rot_mat_goal[i] == self.rot_mat_pre[i]:  # 분모가 0인 경우 처리\n                 tmp_reward2 += 1000\n-            else:\n-                tmp_reward2 += 1 / (self.rot_mat_goal[i] - self.rot_mat_pre[i])\n-        reward = tmp_reward + tmp_reward2\n+                else:\n+                    tmp_reward2 += 1 / (self.rot_mat_goal[i] - self.rot_mat_pre[i])\n+            \n+            \n+        reward = tmp_reward + tmp_reward2            \n         return reward\n     \n     def seed(self, seed=None):\n         return \n\\ No newline at end of file\n"
                },
                {
                    "date": 1692153924229,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -467,9 +467,9 @@\n \n         \n         for i in range(9):\n             if i == 2 or i == 5 or i == 8:\n-\n+                tmp_reward2 = 0\n             else :\n                 if self.rot_mat_goal[i] == self.rot_mat_pre[i]:  # 분모가 0인 경우 처리\n                 tmp_reward2 += 1000\n                 else:\n"
                },
                {
                    "date": 1692153933337,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -470,9 +470,9 @@\n             if i == 2 or i == 5 or i == 8:\n                 tmp_reward2 = 0\n             else :\n                 if self.rot_mat_goal[i] == self.rot_mat_pre[i]:  # 분모가 0인 경우 처리\n-                tmp_reward2 += 1000\n+                    tmp_reward2 += 1000\n                 else:\n                     tmp_reward2 += 1 / (self.rot_mat_goal[i] - self.rot_mat_pre[i])\n             \n             \n"
                },
                {
                    "date": 1692153945663,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -464,19 +464,17 @@\n                 tmp_reward += 1000\n             else:\n                 tmp_reward += 1 / (self.X_goal[i] - self.X[i])\n \n-        \n         for i in range(9):\n             if i == 2 or i == 5 or i == 8:\n                 tmp_reward2 = 0\n             else :\n                 if self.rot_mat_goal[i] == self.rot_mat_pre[i]:  # 분모가 0인 경우 처리\n                     tmp_reward2 += 1000\n                 else:\n                     tmp_reward2 += 1 / (self.rot_mat_goal[i] - self.rot_mat_pre[i])\n-            \n-            \n+    \n         reward = tmp_reward + tmp_reward2            \n         return reward\n     \n     def seed(self, seed=None):\n"
                },
                {
                    "date": 1692153979442,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -92,13 +92,13 @@\n         # self.sim_state = self.sim.get_state()\n         \n         #######################################################\n         # TODO (0814)\n-        # rotation정보 matrix로 처리하기\n+        # rotation정보 matrix로 처리하기 OK\n         # 속도와 관련된 reward 만들지 생각해 보기\n-        # action이 dt_cuda[2]의 delta를 구하는 것으로 바꿔서도 해보기  \n+        # action이 dt_cuda[2]의 delta를 구하는 것으로 바꿔서도 해보기 OK\n         # done 기준 rotation에는 조금 더 여유롭게 정해주기 (cuda.cu와 비슷한 구조?)\n-        # obs에 q정보도 넣어주기\n+        # obs에 q정보도 넣어주기 OK\n         ########################################################\n \n     \n     def step(self, action, render=True):\n"
                },
                {
                    "date": 1692153998052,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -153,9 +153,9 @@\n                 sum_reward -= 1000\n                 return obs, sum_reward, done, dict(), {}\n \n         if self.data.time > self.end_time:\n-            for i in range(6) :\n+            for i in range(3) :\n                 if (self.X_goal[i] - self.X[i]) >= 0.01:\n                     self.reset()\n                     sum_reward -= 1000\n                     return obs, sum_reward, done, dict(), {}\n"
                },
                {
                    "date": 1692154029212,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -153,9 +153,9 @@\n                 sum_reward -= 1000\n                 return obs, sum_reward, done, dict(), {}\n \n         if self.data.time > self.end_time:\n-            for i in range(3) :\n+            for i in range(9) :\n                 if (self.X_goal[i] - self.X[i]) >= 0.01:\n                     self.reset()\n                     sum_reward -= 1000\n                     return obs, sum_reward, done, dict(), {}\n"
                },
                {
                    "date": 1692154051715,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -154,12 +154,13 @@\n                 return obs, sum_reward, done, dict(), {}\n \n         if self.data.time > self.end_time:\n             for i in range(9) :\n-                if (self.X_goal[i] - self.X[i]) >= 0.01:\n-                    self.reset()\n-                    sum_reward -= 1000\n-                    return obs, sum_reward, done, dict(), {}\n+                if i<3 :\n+                    if (self.X_goal[i] - self.X[i]) >= 0.01:\n+                        self.reset()\n+                        sum_reward -= 1000\n+                        return obs, sum_reward, done, dict(), {}\n                 else:\n                     if i == 5 :\n                         sum_reward += 1000\n                         # self.reset()\n"
                },
                {
                    "date": 1692154118693,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -155,14 +155,15 @@\n \n         if self.data.time > self.end_time:\n             for i in range(9) :\n                 if i<3 :\n-                    if (self.X_goal[i] - self.X[i]) >= 0.01:\n+                    if (self.X_goal[i] - self.X[i]) >= 0.001:\n                         self.reset()\n                         sum_reward -= 1000\n                         return obs, sum_reward, done, dict(), {}\n+                if(self.rot_mat_goal[i] - self.rot_mat_pre[i]) >= 0.01\n                 else:\n-                    if i == 5 :\n+                    if (self.X_goal[i] - self.X[i]) >= 0.01:\n                         sum_reward += 1000\n                         # self.reset()\n                         self.controller.reset_goal()\n                         self.reset_time()\n"
                },
                {
                    "date": 1692154150714,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -155,13 +155,14 @@\n \n         if self.data.time > self.end_time:\n             for i in range(9) :\n                 if i<3 :\n-                    if (self.X_goal[i] - self.X[i]) >= 0.001:\n+                    if abs(self.X_goal[i] - self.X[i]) >= 0.001:\n                         self.reset()\n                         sum_reward -= 1000\n                         return obs, sum_reward, done, dict(), {}\n-                if(self.rot_mat_goal[i] - self.rot_mat_pre[i]) >= 0.01\n+                if abs(self.rot_mat_goal[i] - self.rot_mat_pre[i]) >= 0.01:\n+\n                 else:\n                     if (self.X_goal[i] - self.X[i]) >= 0.01:\n                         sum_reward += 1000\n                         # self.reset()\n"
                },
                {
                    "date": 1692154163490,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -153,9 +153,9 @@\n                 sum_reward -= 1000\n                 return obs, sum_reward, done, dict(), {}\n \n         if self.data.time > self.end_time:\n-            for i in range(9) :\n+            for i in range(9):\n                 if i<3 :\n                     if abs(self.X_goal[i] - self.X[i]) >= 0.001:\n                         self.reset()\n                         sum_reward -= 1000\n"
                },
                {
                    "date": 1692154197871,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -160,8 +160,11 @@\n                         self.reset()\n                         sum_reward -= 1000\n                         return obs, sum_reward, done, dict(), {}\n                 if abs(self.rot_mat_goal[i] - self.rot_mat_pre[i]) >= 0.01:\n+                    self.reset()\n+                    sum_reward -= 1000\n+                    return obs, sum_reward, done, dict(), {}\n \n                 else:\n                     if (self.X_goal[i] - self.X[i]) >= 0.01:\n                         sum_reward += 1000\n"
                },
                {
                    "date": 1692154255141,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -151,20 +151,18 @@\n             if(self.data.qpos[i] < self._min_joint_position[i]) :\n                 self.reset()\n                 sum_reward -= 1000\n                 return obs, sum_reward, done, dict(), {}\n-\n+        # if abs(self.rot_mat_goal[i] - self.rot_mat_pre[i]) >= 0.01:\n         if self.data.time > self.end_time:\n-            for i in range(9):\n+            for i in range(9) :\n                 if i<3 :\n                     if abs(self.X_goal[i] - self.X[i]) >= 0.001:\n                         self.reset()\n                         sum_reward -= 1000\n                         return obs, sum_reward, done, dict(), {}\n-                if abs(self.rot_mat_goal[i] - self.rot_mat_pre[i]) >= 0.01:\n-                    self.reset()\n-                    sum_reward -= 1000\n-                    return obs, sum_reward, done, dict(), {}\n+                if\n+                \n \n                 else:\n                     if (self.X_goal[i] - self.X[i]) >= 0.01:\n                         sum_reward += 1000\n"
                },
                {
                    "date": 1692154263183,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -159,11 +159,8 @@\n                     if abs(self.X_goal[i] - self.X[i]) >= 0.001:\n                         self.reset()\n                         sum_reward -= 1000\n                         return obs, sum_reward, done, dict(), {}\n-                if\n-                \n-\n                 else:\n                     if (self.X_goal[i] - self.X[i]) >= 0.01:\n                         sum_reward += 1000\n                         # self.reset()\n"
                },
                {
                    "date": 1692154290276,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -153,16 +153,16 @@\n                 sum_reward -= 1000\n                 return obs, sum_reward, done, dict(), {}\n         # if abs(self.rot_mat_goal[i] - self.rot_mat_pre[i]) >= 0.01:\n         if self.data.time > self.end_time:\n-            for i in range(9) :\n+            for i in range(3) :\n                 if i<3 :\n                     if abs(self.X_goal[i] - self.X[i]) >= 0.001:\n                         self.reset()\n                         sum_reward -= 1000\n                         return obs, sum_reward, done, dict(), {}\n                 else:\n-                    if (self.X_goal[i] - self.X[i]) >= 0.01:\n+                        self.position_state = 1\n                         sum_reward += 1000\n                         # self.reset()\n                         self.controller.reset_goal()\n                         self.reset_time()\n"
                },
                {
                    "date": 1692154315244,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -151,18 +151,17 @@\n             if(self.data.qpos[i] < self._min_joint_position[i]) :\n                 self.reset()\n                 sum_reward -= 1000\n                 return obs, sum_reward, done, dict(), {}\n-        # if abs(self.rot_mat_goal[i] - self.rot_mat_pre[i]) >= 0.01:\n+\n         if self.data.time > self.end_time:\n-            for i in range(3) :\n-                if i<3 :\n-                    if abs(self.X_goal[i] - self.X[i]) >= 0.001:\n-                        self.reset()\n-                        sum_reward -= 1000\n-                        return obs, sum_reward, done, dict(), {}\n+            for i in range(6) :\n+                if (self.X_goal[i] - self.X[i]) >= 0.01:\n+                    self.reset()\n+                    sum_reward -= 1000\n+                    return obs, sum_reward, done, dict(), {}\n                 else:\n-                        self.position_state = 1\n+                    if i == 5 :\n                         sum_reward += 1000\n                         # self.reset()\n                         self.controller.reset_goal()\n                         self.reset_time()\n"
                },
                {
                    "date": 1692154321384,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -153,15 +153,15 @@\n                 sum_reward -= 1000\n                 return obs, sum_reward, done, dict(), {}\n \n         if self.data.time > self.end_time:\n-            for i in range(6) :\n+            for i in range(3) :\n                 if (self.X_goal[i] - self.X[i]) >= 0.01:\n                     self.reset()\n                     sum_reward -= 1000\n                     return obs, sum_reward, done, dict(), {}\n                 else:\n-                    if i == 5 :\n+                    if i == 2:\n                         sum_reward += 1000\n                         # self.reset()\n                         self.controller.reset_goal()\n                         self.reset_time()\n"
                },
                {
                    "date": 1692154327145,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -154,9 +154,9 @@\n                 return obs, sum_reward, done, dict(), {}\n \n         if self.data.time > self.end_time:\n             for i in range(3) :\n-                if (self.X_goal[i] - self.X[i]) >= 0.01:\n+                if abs(self.X_goal[i] - self.X[i]) >= 0.01:\n                     self.reset()\n                     sum_reward -= 1000\n                     return obs, sum_reward, done, dict(), {}\n                 else:\n"
                },
                {
                    "date": 1692154344254,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -154,21 +154,15 @@\n                 return obs, sum_reward, done, dict(), {}\n \n         if self.data.time > self.end_time:\n             for i in range(3) :\n-                if abs(self.X_goal[i] - self.X[i]) >= 0.01:\n+                if abs(self.X_goal[i] - self.X[i]) >= 0.001:\n                     self.reset()\n                     sum_reward -= 1000\n                     return obs, sum_reward, done, dict(), {}\n                 else:\n                     if i == 2:\n-                        sum_reward += 1000\n-                        # self.reset()\n-                        self.controller.reset_goal()\n-                        self.reset_time()\n-                        # print(\"!!??\")\n-                        # mujoco.mj_step(self.model, self.data)\n-                        return obs, sum_reward, done, dict(), {}\n+                        self.position_state = 1\n         \n         return obs, sum_reward, done, dict(), {}\n     # def render(self):\n     #     if self.viewer is None:\n"
                },
                {
                    "date": 1692154455700,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -153,16 +153,28 @@\n                 sum_reward -= 1000\n                 return obs, sum_reward, done, dict(), {}\n \n         if self.data.time > self.end_time:\n-            for i in range(3) :\n+            for i in range(3):\n                 if abs(self.X_goal[i] - self.X[i]) >= 0.001:\n                     self.reset()\n                     sum_reward -= 1000\n                     return obs, sum_reward, done, dict(), {}\n                 else:\n                     if i == 2:\n                         self.position_state = 1\n+\n+            for i in range(9):\n+                if i == 2 or i == 5 or i == 8:\n+                    sum_reward += 0\n+                else:\n+                    if abs(self.rot_mat_goal[i] - self.X[i]) >= 0.01:\n+                        self.reset()\n+                        sum_reward -= 1000\n+                        return obs, sum_reward, done, dict(), {}\n+                    else:\n+                        if i == 2:\n+                            self.position_state = 1\n         \n         return obs, sum_reward, done, dict(), {}\n     # def render(self):\n     #     if self.viewer is None:\n"
                },
                {
                    "date": 1692154461802,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -171,9 +171,9 @@\n                         self.reset()\n                         sum_reward -= 1000\n                         return obs, sum_reward, done, dict(), {}\n                     else:\n-                        if i == 2:\n+                        if i == 7:\n                             self.position_state = 1\n         \n         return obs, sum_reward, done, dict(), {}\n     # def render(self):\n"
                },
                {
                    "date": 1692154469630,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -162,10 +162,10 @@\n                 else:\n                     if i == 2:\n                         self.position_state = 1\n \n-            for i in range(9):\n-                if i == 2 or i == 5 or i == 8:\n+            for i in range(8):\n+                if i == 2 or i == 5:\n                     sum_reward += 0\n                 else:\n                     if abs(self.rot_mat_goal[i] - self.X[i]) >= 0.01:\n                         self.reset()\n"
                },
                {
                    "date": 1692154478565,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -172,9 +172,9 @@\n                         sum_reward -= 1000\n                         return obs, sum_reward, done, dict(), {}\n                     else:\n                         if i == 7:\n-                            self.position_state = 1\n+                            self.rotate_state = 1\n         \n         return obs, sum_reward, done, dict(), {}\n     # def render(self):\n     #     if self.viewer is None:\n"
                },
                {
                    "date": 1692154559845,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -152,8 +152,9 @@\n                 self.reset()\n                 sum_reward -= 1000\n                 return obs, sum_reward, done, dict(), {}\n \n+\n         if self.data.time > self.end_time:\n             for i in range(3):\n                 if abs(self.X_goal[i] - self.X[i]) >= 0.001:\n                     self.reset()\n@@ -173,8 +174,33 @@\n                         return obs, sum_reward, done, dict(), {}\n                     else:\n                         if i == 7:\n                             self.rotate_state = 1\n+            if self.position_state == 1 and self.rotate_state == 1:\n+                sum_reward += 1000\n+                # self.reset()\n+                self.controller.reset_goal()\n+                self.reset_time()\n+                # print(\"!!??\")\n+                # mujoco.mj_step(self.model, self.data)\n+                return obs, sum_reward, done, dict(), {}\n+\n+\n+        if self.data.time > self.end_time:\n+            for i in range(3) :\n+                if (self.X_goal[i] - self.X[i]) >= 0.01:\n+                    self.reset()\n+                    sum_reward -= 1000\n+                    return obs, sum_reward, done, dict(), {}\n+                else:\n+                    if i == 5 :\n+                        sum_reward += 1000\n+                        # self.reset()\n+                        self.controller.reset_goal()\n+                        self.reset_time()\n+                        # print(\"!!??\")\n+                        # mujoco.mj_step(self.model, self.data)\n+                        return obs, sum_reward, done, dict(), {}\n         \n         return obs, sum_reward, done, dict(), {}\n     # def render(self):\n     #     if self.viewer is None:\n"
                },
                {
                    "date": 1692154566349,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -181,27 +181,9 @@\n                 self.controller.reset_goal()\n                 self.reset_time()\n                 # print(\"!!??\")\n                 # mujoco.mj_step(self.model, self.data)\n-                return obs, sum_reward, done, dict(), {}\n-\n-\n-        if self.data.time > self.end_time:\n-            for i in range(3) :\n-                if (self.X_goal[i] - self.X[i]) >= 0.01:\n-                    self.reset()\n-                    sum_reward -= 1000\n-                    return obs, sum_reward, done, dict(), {}\n-                else:\n-                    if i == 5 :\n-                        sum_reward += 1000\n-                        # self.reset()\n-                        self.controller.reset_goal()\n-                        self.reset_time()\n-                        # print(\"!!??\")\n-                        # mujoco.mj_step(self.model, self.data)\n-                        return obs, sum_reward, done, dict(), {}\n-        \n+                return obs, sum_reward, done, dict(), {}        \n         return obs, sum_reward, done, dict(), {}\n     # def render(self):\n     #     if self.viewer is None:\n     #         self.viewer = viewer.launch_passive(model=self.model, data=self.data)\n"
                },
                {
                    "date": 1692154576537,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -167,9 +167,9 @@\n             for i in range(8):\n                 if i == 2 or i == 5:\n                     sum_reward += 0\n                 else:\n-                    if abs(self.rot_mat_goal[i] - self.X[i]) >= 0.01:\n+                    if abs(self.rot_mat_goal[i] - self.rot_mat_pre[i]) >= 0.01:\n                         self.reset()\n                         sum_reward -= 1000\n                         return obs, sum_reward, done, dict(), {}\n                     else:\n"
                },
                {
                    "date": 1692154601877,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -27,8 +27,10 @@\n         self.tmp_X = np.zeros(19)\n         self.X_goal = np.zeros(6)\n         self.X = np.zeros(6)\n         self.q = np.zeros(7)\n+        self.position_state = 0\n+        self.rotate_state = 0\n \n         self.rot_mat_goal = np.zeros(9)\n         self.rot_mat_pre = np.zeros(9)\n         self.quaternion_goal = np.zeros(4)\n"
                },
                {
                    "date": 1692154619664,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -106,8 +106,10 @@\n     def step(self, action, render=True):\n         self.viewer.sync()\n         # print('step')\n         sum_reward = 0\n+        self.position_state = 0\n+        self.rotate_state = 0\n         done = False\n         self.temp_action = action\n         action = action + 1.0\n         action = action / 2.0\n"
                },
                {
                    "date": 1692154627114,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -106,10 +106,10 @@\n     def step(self, action, render=True):\n         self.viewer.sync()\n         # print('step')\n         sum_reward = 0\n-        self.position_state = 0\n-        self.rotate_state = 0\n+        position_state = 0\n+        rotate_state = 0\n         done = False\n         self.temp_action = action\n         action = action + 1.0\n         action = action / 2.0\n"
                },
                {
                    "date": 1692154646938,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -27,10 +27,8 @@\n         self.tmp_X = np.zeros(19)\n         self.X_goal = np.zeros(6)\n         self.X = np.zeros(6)\n         self.q = np.zeros(7)\n-        self.position_state = 0\n-        self.rotate_state = 0\n \n         self.rot_mat_goal = np.zeros(9)\n         self.rot_mat_pre = np.zeros(9)\n         self.quaternion_goal = np.zeros(4)\n@@ -165,9 +163,9 @@\n                     sum_reward -= 1000\n                     return obs, sum_reward, done, dict(), {}\n                 else:\n                     if i == 2:\n-                        self.position_state = 1\n+                        position_state = 1\n \n             for i in range(8):\n                 if i == 2 or i == 5:\n                     sum_reward += 0\n@@ -177,10 +175,10 @@\n                         sum_reward -= 1000\n                         return obs, sum_reward, done, dict(), {}\n                     else:\n                         if i == 7:\n-                            self.rotate_state = 1\n-            if self.position_state == 1 and self.rotate_state == 1:\n+                            rotate_state = 1\n+            if position_state == 1 and rotate_state == 1:\n                 sum_reward += 1000\n                 # self.reset()\n                 self.controller.reset_goal()\n                 self.reset_time()\n"
                },
                {
                    "date": 1692154739867,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -275,10 +275,10 @@\n         return mat3\n         \n     \n     def _construct_observation_space(self):\n-        obs_low = -1 * np.ones(24) # X(6), X_goal(6), action(1) \n-        obs_high = 1 * np.ones(24)\n+        obs_low = -1 * np.ones(26) # X(6), X_goal(6), action(1) \n+        obs_high = 1 * np.ones(26)\n         # obs_low = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0,0,0, -1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1]\n         # obs_high = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0,0,0, 1,1,1,1,1,1,1,1,1,1,1,1]\n         return gym.spaces.Box(obs_low, obs_high, dtype=np.float64)\n     \n"
                },
                {
                    "date": 1692154966284,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -117,8 +117,9 @@\n         self.controller.control_mujoco()\n         self.tmp_X = self.controller.state_controller()\n         self.rot_mat_goal = self.GetBodyRotationMatrix(self.tmp_X[3],self.tmp_X[4],self.tmp_X[5])\n         self.rot_mat_pre = self.GetBodyRotationMatrix(self.tmp_X[9],self.tmp_X[10],self.tmp_X[11])\n+        print(self.rot_mat_goal)\n         \n         # GetBodyRotationMatrix\n         self.torque = self.controller.write()\n         for i in range(self.k):\n"
                },
                {
                    "date": 1692154991926,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -117,9 +117,11 @@\n         self.controller.control_mujoco()\n         self.tmp_X = self.controller.state_controller()\n         self.rot_mat_goal = self.GetBodyRotationMatrix(self.tmp_X[3],self.tmp_X[4],self.tmp_X[5])\n         self.rot_mat_pre = self.GetBodyRotationMatrix(self.tmp_X[9],self.tmp_X[10],self.tmp_X[11])\n-        print(self.rot_mat_goal)\n+        print(\"goal_ratoate : \",self.rot_mat_goal)\n+        print(\"pre_ratoate : \",self.rot_mat_goal)\n+        # print(self.rot_mat_pre)\n         \n         # GetBodyRotationMatrix\n         self.torque = self.controller.write()\n         for i in range(self.k):\n"
                },
                {
                    "date": 1692155012608,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -117,10 +117,12 @@\n         self.controller.control_mujoco()\n         self.tmp_X = self.controller.state_controller()\n         self.rot_mat_goal = self.GetBodyRotationMatrix(self.tmp_X[3],self.tmp_X[4],self.tmp_X[5])\n         self.rot_mat_pre = self.GetBodyRotationMatrix(self.tmp_X[9],self.tmp_X[10],self.tmp_X[11])\n-        print(\"goal_ratoate : \",self.rot_mat_goal)\n-        print(\"pre_ratoate : \",self.rot_mat_goal)\n+        print(\"goal_rotate : \",self.rot_mat_goal)\n+        print(\"pre_rotate : \",self.rot_mat_goal)\n+        print(\"goal_pos : \",self.rot_mat_goal)\n+        print(\"pre_po : \",self.rot_mat_goal)\n         # print(self.rot_mat_pre)\n         \n         # GetBodyRotationMatrix\n         self.torque = self.controller.write()\n"
                },
                {
                    "date": 1692155026257,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -117,12 +117,9 @@\n         self.controller.control_mujoco()\n         self.tmp_X = self.controller.state_controller()\n         self.rot_mat_goal = self.GetBodyRotationMatrix(self.tmp_X[3],self.tmp_X[4],self.tmp_X[5])\n         self.rot_mat_pre = self.GetBodyRotationMatrix(self.tmp_X[9],self.tmp_X[10],self.tmp_X[11])\n-        print(\"goal_rotate : \",self.rot_mat_goal)\n-        print(\"pre_rotate : \",self.rot_mat_goal)\n-        print(\"goal_pos : \",self.rot_mat_goal)\n-        print(\"pre_po : \",self.rot_mat_goal)\n+        \n         # print(self.rot_mat_pre)\n         \n         # GetBodyRotationMatrix\n         self.torque = self.controller.write()\n@@ -135,8 +132,12 @@\n             self.X[i] = self.tmp_X[i+6]\n         for i in range(7):\n             self.q[i] = self.tmp_X[i+12]\n \n+        print(\"goal_rotate : \",self.rot_mat_goal)\n+        print(\"pre_rotate : \",self.rot_mat_goal)\n+        print(\"goal_pos : \",self.X_goal)\n+        print(\"pre_pos : \",self.X)\n         self.euler_to_quaternion(self.X_goal[3],self.X_goal[4],self.X_goal[5])\n         # print(self.data.qpos)\n         # print(self.data.xpos[13,0])\n         # xpos랑 xmat을 건드려야함\n"
                },
                {
                    "date": 1692155062721,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -133,11 +133,12 @@\n         for i in range(7):\n             self.q[i] = self.tmp_X[i+12]\n \n         print(\"goal_rotate : \",self.rot_mat_goal)\n-        print(\"pre_rotate : \",self.rot_mat_goal)\n+        print(\"pre_rotate : \",self.rot_mat_pre)\n         print(\"goal_pos : \",self.X_goal)\n         print(\"pre_pos : \",self.X)\n+\n         self.euler_to_quaternion(self.X_goal[3],self.X_goal[4],self.X_goal[5])\n         # print(self.data.qpos)\n         # print(self.data.xpos[13,0])\n         # xpos랑 xmat을 건드려야함\n"
                },
                {
                    "date": 1692155077624,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -133,10 +133,13 @@\n         for i in range(7):\n             self.q[i] = self.tmp_X[i+12]\n \n         print(\"goal_rotate : \",self.rot_mat_goal)\n+        print(\"\")\n         print(\"pre_rotate : \",self.rot_mat_pre)\n+        print(\"\")\n         print(\"goal_pos : \",self.X_goal)\n+        print(\"\")\n         print(\"pre_pos : \",self.X)\n \n         self.euler_to_quaternion(self.X_goal[3],self.X_goal[4],self.X_goal[5])\n         # print(self.data.qpos)\n"
                },
                {
                    "date": 1692155124556,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -132,15 +132,15 @@\n             self.X[i] = self.tmp_X[i+6]\n         for i in range(7):\n             self.q[i] = self.tmp_X[i+12]\n \n-        print(\"goal_rotate : \",self.rot_mat_goal)\n-        print(\"\")\n-        print(\"pre_rotate : \",self.rot_mat_pre)\n-        print(\"\")\n-        print(\"goal_pos : \",self.X_goal)\n-        print(\"\")\n-        print(\"pre_pos : \",self.X)\n+        # print(\"goal_rotate : \",self.rot_mat_goal)\n+        # print(\"\")\n+        # print(\"pre_rotate : \",self.rot_mat_pre)\n+        # print(\"\")\n+        # print(\"goal_pos : \",self.X_goal)\n+        # print(\"\")\n+        # print(\"pre_pos : \",self.X)\n \n         self.euler_to_quaternion(self.X_goal[3],self.X_goal[4],self.X_goal[5])\n         # print(self.data.qpos)\n         # print(self.data.xpos[13,0])\n"
                },
                {
                    "date": 1692155195180,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -132,15 +132,21 @@\n             self.X[i] = self.tmp_X[i+6]\n         for i in range(7):\n             self.q[i] = self.tmp_X[i+12]\n \n-        # print(\"goal_rotate : \",self.rot_mat_goal)\n-        # print(\"\")\n-        # print(\"pre_rotate : \",self.rot_mat_pre)\n-        # print(\"\")\n-        # print(\"goal_pos : \",self.X_goal)\n-        # print(\"\")\n-        # print(\"pre_pos : \",self.X)\n+        print(\"goal_rotate\")\n+        print(self.rot_mat_goal[0],self.rot_mat_goal[1],self.rot_mat_goal[2])\n+        print(self.rot_mat_goal[3],self.rot_mat_goal[4],self.rot_mat_goal[5])\n+        print(self.rot_mat_goal[6],self.rot_mat_goal[7],self.rot_mat_goal[8])\n+        print(\"\")\n+        print(\"pre_rotate\")\n+        print(self.rot_mat_pre[0],self.rot_mat_pre[1],self.rot_mat_pre[2])\n+        print(self.rot_mat_pre[3],self.rot_mat_pre[4],self.rot_mat_pre[5])\n+        print(self.rot_mat_pre[6],self.rot_mat_pre[7],self.rot_mat_pre[8])\n+        print(\"\")\n+        print(\"goal_pos : \",self.X_goal)\n+        print(\"\")\n+        print(\"pre_pos : \",self.X)\n \n         self.euler_to_quaternion(self.X_goal[3],self.X_goal[4],self.X_goal[5])\n         # print(self.data.qpos)\n         # print(self.data.xpos[13,0])\n"
                },
                {
                    "date": 1692155278893,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -142,12 +142,16 @@\n         print(self.rot_mat_pre[0],self.rot_mat_pre[1],self.rot_mat_pre[2])\n         print(self.rot_mat_pre[3],self.rot_mat_pre[4],self.rot_mat_pre[5])\n         print(self.rot_mat_pre[6],self.rot_mat_pre[7],self.rot_mat_pre[8])\n         print(\"\")\n-        print(\"goal_pos : \",self.X_goal)\n+        print(\"goal_pos\")\n+        print(self.X_goal[0],self.X_goal[1],self.X_goal[2])\n         print(\"\")\n         print(\"pre_pos : \",self.X)\n+        print(self.X[0],self.X[1],self.X[2])\n+        print(\"\")\n \n+\n         self.euler_to_quaternion(self.X_goal[3],self.X_goal[4],self.X_goal[5])\n         # print(self.data.qpos)\n         # print(self.data.xpos[13,0])\n         # xpos랑 xmat을 건드려야함\n"
                },
                {
                    "date": 1692155327659,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -145,9 +145,9 @@\n         print(\"\")\n         print(\"goal_pos\")\n         print(self.X_goal[0],self.X_goal[1],self.X_goal[2])\n         print(\"\")\n-        print(\"pre_pos : \",self.X)\n+        print(\"pre_pos\")\n         print(self.X[0],self.X[1],self.X[2])\n         print(\"\")\n \n \n"
                },
                {
                    "date": 1692162929939,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -132,24 +132,24 @@\n             self.X[i] = self.tmp_X[i+6]\n         for i in range(7):\n             self.q[i] = self.tmp_X[i+12]\n \n-        print(\"goal_rotate\")\n-        print(self.rot_mat_goal[0],self.rot_mat_goal[1],self.rot_mat_goal[2])\n-        print(self.rot_mat_goal[3],self.rot_mat_goal[4],self.rot_mat_goal[5])\n-        print(self.rot_mat_goal[6],self.rot_mat_goal[7],self.rot_mat_goal[8])\n-        print(\"\")\n-        print(\"pre_rotate\")\n-        print(self.rot_mat_pre[0],self.rot_mat_pre[1],self.rot_mat_pre[2])\n-        print(self.rot_mat_pre[3],self.rot_mat_pre[4],self.rot_mat_pre[5])\n-        print(self.rot_mat_pre[6],self.rot_mat_pre[7],self.rot_mat_pre[8])\n-        print(\"\")\n-        print(\"goal_pos\")\n-        print(self.X_goal[0],self.X_goal[1],self.X_goal[2])\n-        print(\"\")\n-        print(\"pre_pos\")\n-        print(self.X[0],self.X[1],self.X[2])\n-        print(\"\")\n+        # print(\"goal_rotate\")\n+        # print(self.rot_mat_goal[0],self.rot_mat_goal[1],self.rot_mat_goal[2])\n+        # print(self.rot_mat_goal[3],self.rot_mat_goal[4],self.rot_mat_goal[5])\n+        # print(self.rot_mat_goal[6],self.rot_mat_goal[7],self.rot_mat_goal[8])\n+        # print(\"\")\n+        # print(\"pre_rotate\")\n+        # print(self.rot_mat_pre[0],self.rot_mat_pre[1],self.rot_mat_pre[2])\n+        # print(self.rot_mat_pre[3],self.rot_mat_pre[4],self.rot_mat_pre[5])\n+        # print(self.rot_mat_pre[6],self.rot_mat_pre[7],self.rot_mat_pre[8])\n+        # print(\"\")\n+        # print(\"goal_pos\")\n+        # print(self.X_goal[0],self.X_goal[1],self.X_goal[2])\n+        # print(\"\")\n+        # print(\"pre_pos\")\n+        # print(self.X[0],self.X[1],self.X[2])\n+        # print(\"\")\n \n \n         self.euler_to_quaternion(self.X_goal[3],self.X_goal[4],self.X_goal[5])\n         # print(self.data.qpos)\n"
                },
                {
                    "date": 1692166713933,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -177,9 +177,9 @@\n \n \n         if self.data.time > self.end_time:\n             for i in range(3):\n-                if abs(self.X_goal[i] - self.X[i]) >= 0.001:\n+                if abs(self.X_goal[i] - self.X[i]) >= 0.05:\n                     self.reset()\n                     sum_reward -= 1000\n                     return obs, sum_reward, done, dict(), {}\n                 else:\n@@ -189,9 +189,9 @@\n             for i in range(8):\n                 if i == 2 or i == 5:\n                     sum_reward += 0\n                 else:\n-                    if abs(self.rot_mat_goal[i] - self.rot_mat_pre[i]) >= 0.01:\n+                    if abs(self.rot_mat_goal[i] - self.rot_mat_pre[i]) >= 0.1:\n                         self.reset()\n                         sum_reward -= 1000\n                         return obs, sum_reward, done, dict(), {}\n                     else:\n"
                },
                {
                    "date": 1692256053825,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -274,10 +274,10 @@\n \n \n     def _construct_action_space(self):\n         # action_low = -1 * np.ones(12)\n-        action_low =  -1 * np.ones(1)\n-        action_high = np.ones(1) \n+        action_low =  -1 * np.ones(2)\n+        action_high = np.ones(2) \n         return gym.spaces.Box(low=action_low, high=action_high, dtype=np.float64)\n     \n     def matrixMultiply_3x3(self, mat_A, mat_B):\n         mat3 = np.zeros(9)\n"
                },
                {
                    "date": 1692256062120,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -23,9 +23,9 @@\n         self.framerate = 10  # (Hz)\n \n         self.controller = controller.CController()\n         self.torque = np.zeros(self.k)\n-        self.tmp_X = np.zeros(19)\n+        self.tmp_X = np.zeros(20)\n         self.X_goal = np.zeros(6)\n         self.X = np.zeros(6)\n         self.q = np.zeros(7)\n \n"
                },
                {
                    "date": 1692256074903,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -295,9 +295,9 @@\n         return mat3\n         \n     \n     def _construct_observation_space(self):\n-        obs_low = -1 * np.ones(26) # X(6), X_goal(6), action(1) \n+        obs_low = -1 * np.ones(26) # X(6), X_goal(6), action(2) \n         obs_high = 1 * np.ones(26)\n         # obs_low = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0,0,0, -1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1]\n         # obs_high = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0,0,0, 1,1,1,1,1,1,1,1,1,1,1,1]\n         return gym.spaces.Box(obs_low, obs_high, dtype=np.float64)\n"
                },
                {
                    "date": 1692256080641,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -295,9 +295,9 @@\n         return mat3\n         \n     \n     def _construct_observation_space(self):\n-        obs_low = -1 * np.ones(26) # X(6), X_goal(6), action(2) \n+        obs_low = -1 * np.ones(26) # X(6), X_goal(6), q(7), action(2) \n         obs_high = 1 * np.ones(26)\n         # obs_low = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0,0,0, -1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1]\n         # obs_high = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0,0,0, 1,1,1,1,1,1,1,1,1,1,1,1]\n         return gym.spaces.Box(obs_low, obs_high, dtype=np.float64)\n"
                },
                {
                    "date": 1692256089543,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -295,10 +295,10 @@\n         return mat3\n         \n     \n     def _construct_observation_space(self):\n-        obs_low = -1 * np.ones(26) # X(6), X_goal(6), q(7), action(2) \n-        obs_high = 1 * np.ones(26)\n+        obs_low = -1 * np.ones(27) # X(6), X_goal(6), q(7), action(2) \n+        obs_high = 1 * np.ones(27)\n         # obs_low = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0,0,0, -1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1]\n         # obs_high = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0,0,0, 1,1,1,1,1,1,1,1,1,1,1,1]\n         return gym.spaces.Box(obs_low, obs_high, dtype=np.float64)\n     \n"
                },
                {
                    "date": 1692256099423,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -295,9 +295,9 @@\n         return mat3\n         \n     \n     def _construct_observation_space(self):\n-        obs_low = -1 * np.ones(27) # X(6), X_goal(6), q(7), action(2) \n+        obs_low = -1 * np.ones(27) # X(10), X_goal(10), q(7), action(2) \n         obs_high = 1 * np.ones(27)\n         # obs_low = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0,0,0, -1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1]\n         # obs_high = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0,0,0, 1,1,1,1,1,1,1,1,1,1,1,1]\n         return gym.spaces.Box(obs_low, obs_high, dtype=np.float64)\n"
                },
                {
                    "date": 1692256110387,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -295,9 +295,9 @@\n         return mat3\n         \n     \n     def _construct_observation_space(self):\n-        obs_low = -1 * np.ones(27) # X(10), X_goal(10), q(7), action(2) \n+        obs_low = -1 * np.ones(27) # X(9), X_goal(9), q(7), action(2) \n         obs_high = 1 * np.ones(27)\n         # obs_low = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0,0,0, -1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1]\n         # obs_high = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0,0,0, 1,1,1,1,1,1,1,1,1,1,1,1]\n         return gym.spaces.Box(obs_low, obs_high, dtype=np.float64)\n"
                },
                {
                    "date": 1692256120365,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -23,9 +23,9 @@\n         self.framerate = 10  # (Hz)\n \n         self.controller = controller.CController()\n         self.torque = np.zeros(self.k)\n-        self.tmp_X = np.zeros(20)\n+        self.tmp_X = np.zeros(19)\n         self.X_goal = np.zeros(6)\n         self.X = np.zeros(6)\n         self.q = np.zeros(7)\n \n"
                },
                {
                    "date": 1692256488903,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -39,9 +39,9 @@\n         self.q_pos = np.zeros(7)\n         # self.reward_arr = np.zeros(2) #shape=(2,), dtype=np.float64\n         self.reward_arr = [0,0,0]\n         self.cnt = 1\n-        self.temp_action = np.zeros(1)\n+        self.temp_action = np.zeros(2)\n         self._max_joint_position = np.zeros(self.k)\n         self._min_joint_position = np.zeros(self.k)\n         self.timestep = 0\n \n"
                },
                {
                    "date": 1692256507878,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -109,11 +109,11 @@\n         rotate_state = 0\n         done = False\n         self.temp_action = action\n         action = action + 1.0\n-        action = action / 2.0\n-        action = action + 0.003\n+        # action = action / 2.0\n         # action = action + 0.003\n+        # action = action + 0.003\n         self.controller.read(self.data.time, self.data.qpos[0:self.k], self.data.qvel[0:self.k], action)\n         self.controller.control_mujoco()\n         self.tmp_X = self.controller.state_controller()\n         self.rot_mat_goal = self.GetBodyRotationMatrix(self.tmp_X[3],self.tmp_X[4],self.tmp_X[5])\n"
                },
                {
                    "date": 1692256516603,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -109,9 +109,9 @@\n         rotate_state = 0\n         done = False\n         self.temp_action = action\n         action = action + 1.0\n-        # action = action / 2.0\n+        action = action / 2.0\n         # action = action + 0.003\n         # action = action + 0.003\n         self.controller.read(self.data.time, self.data.qpos[0:self.k], self.data.qvel[0:self.k], action)\n         self.controller.control_mujoco()\n"
                },
                {
                    "date": 1692256592583,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -271,9 +271,8 @@\n         tmp_mat3x3 = self.matrixMultiply_3x3(R_yaw, R_pitch)\n         rot_mat = self.matrixMultiply_3x3(tmp_mat3x3, R_roll)\n         return rot_mat\n \n-\n     def _construct_action_space(self):\n         # action_low = -1 * np.ones(12)\n         action_low =  -1 * np.ones(2)\n         action_high = np.ones(2) \n@@ -292,19 +291,16 @@\n         mat3[6] = mat_A[6] * mat_B[0] + mat_A[7] * mat_B[3] + mat_A[8] * mat_B[6];\n         mat3[7] = mat_A[6] * mat_B[1] + mat_A[7] * mat_B[4] + mat_A[8] * mat_B[7];\n         mat3[8] = mat_A[6] * mat_B[2] + mat_A[7] * mat_B[5] + mat_A[8] * mat_B[8];\n         return mat3\n-        \n-    \n+         \n     def _construct_observation_space(self):\n         obs_low = -1 * np.ones(27) # X(9), X_goal(9), q(7), action(2) \n         obs_high = 1 * np.ones(27)\n         # obs_low = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0,0,0, -1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1]\n         # obs_high = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0,0,0, 1,1,1,1,1,1,1,1,1,1,1,1]\n         return gym.spaces.Box(obs_low, obs_high, dtype=np.float64)\n     \n-     \n-    \n     def get_observation(self):\n         # self.sim_state = self.sim.get_state()\n         self.x_pos[0] = self.X[0] / 0.6 # -0.6~0.6\n         self.x_pos[1] = self.X[1] / 0.6 # -0.6~0.6\n@@ -353,9 +349,9 @@\n         (\n             self.x_pos, #9\n             self.goal, #9\n             self.q_pos, #7\n-            self.temp_action, # 1\n+            self.temp_action, # 2\n         )\n         )\n         \n     def reset(self, seed=None):\n"
                },
                {
                    "date": 1692256649682,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -384,10 +384,8 @@\n     \n     def reset_time(self):\n         self.end_time = self.data.time + self.motion_time\n     \n-    \n-    \n     def set_state(self, qpos, qvel):\n         old_state = self.sim.get_state()\n         new_state = mujoco.MjSimState(\n             old_state.time, qpos, qvel, old_state.act, old_state.udd_state\n@@ -443,11 +441,9 @@\n             # original image is upside-down, so flip it\n             return data[::-1, :]\n         elif mode == \"human\":\n             self._get_viewer(mode).render()\n-            \n-            \n-            \n+                   \n     def viewer_setup(self):\n         \"\"\"\n         This method is called when the viewer is initialized.\n         Optionally implement this method, if you need to tinker with camera position\n"
                },
                {
                    "date": 1692257043280,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -114,11 +114,14 @@\n         # action = action + 0.003\n         # action = action + 0.003\n         self.controller.read(self.data.time, self.data.qpos[0:self.k], self.data.qvel[0:self.k], action)\n         self.controller.control_mujoco()\n+        print(\"1\")\n         self.tmp_X = self.controller.state_controller()\n+        print(\"2\")\n         self.rot_mat_goal = self.GetBodyRotationMatrix(self.tmp_X[3],self.tmp_X[4],self.tmp_X[5])\n         self.rot_mat_pre = self.GetBodyRotationMatrix(self.tmp_X[9],self.tmp_X[10],self.tmp_X[11])\n+        print(\"3\")\n         \n         # print(self.rot_mat_pre)\n         \n         # GetBodyRotationMatrix\n"
                },
                {
                    "date": 1692257050244,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -125,8 +125,9 @@\n         # print(self.rot_mat_pre)\n         \n         # GetBodyRotationMatrix\n         self.torque = self.controller.write()\n+        print(\"4\")\n         for i in range(self.k):\n                 self.data.ctrl[i] = self.torque[i]\n         for i in range(6):\n             self.X_goal[i] = self.tmp_X[i]\n@@ -135,9 +136,9 @@\n             self.X[i] = self.tmp_X[i+6]\n         for i in range(7):\n             self.q[i] = self.tmp_X[i+12]\n \n-        # print(\"goal_rotate\")\n+        print(\"goal_rotate\")\n         # print(self.rot_mat_goal[0],self.rot_mat_goal[1],self.rot_mat_goal[2])\n         # print(self.rot_mat_goal[3],self.rot_mat_goal[4],self.rot_mat_goal[5])\n         # print(self.rot_mat_goal[6],self.rot_mat_goal[7],self.rot_mat_goal[8])\n         # print(\"\")\n"
                },
                {
                    "date": 1692257066517,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -114,20 +114,16 @@\n         # action = action + 0.003\n         # action = action + 0.003\n         self.controller.read(self.data.time, self.data.qpos[0:self.k], self.data.qvel[0:self.k], action)\n         self.controller.control_mujoco()\n-        print(\"1\")\n         self.tmp_X = self.controller.state_controller()\n-        print(\"2\")\n         self.rot_mat_goal = self.GetBodyRotationMatrix(self.tmp_X[3],self.tmp_X[4],self.tmp_X[5])\n         self.rot_mat_pre = self.GetBodyRotationMatrix(self.tmp_X[9],self.tmp_X[10],self.tmp_X[11])\n-        print(\"3\")\n         \n         # print(self.rot_mat_pre)\n         \n         # GetBodyRotationMatrix\n         self.torque = self.controller.write()\n-        print(\"4\")\n         for i in range(self.k):\n                 self.data.ctrl[i] = self.torque[i]\n         for i in range(6):\n             self.X_goal[i] = self.tmp_X[i]\n@@ -136,9 +132,9 @@\n             self.X[i] = self.tmp_X[i+6]\n         for i in range(7):\n             self.q[i] = self.tmp_X[i+12]\n \n-        print(\"goal_rotate\")\n+        # print(\"goal_rotate\")\n         # print(self.rot_mat_goal[0],self.rot_mat_goal[1],self.rot_mat_goal[2])\n         # print(self.rot_mat_goal[3],self.rot_mat_goal[4],self.rot_mat_goal[5])\n         # print(self.rot_mat_goal[6],self.rot_mat_goal[7],self.rot_mat_goal[8])\n         # print(\"\")\n"
                },
                {
                    "date": 1692257920705,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -162,10 +162,11 @@\n         obs =self.get_observation()\n         mujoco.mj_step(self.model, self.data)\n         sum_reward = self.reward()\n         # sum_reward = 0.0\n-        # print(self.data.time)\n+        print(\"응애\")\n \n+\n         for i in range(self.k) : \n             if(self.data.qpos[i] > self._max_joint_position[i]) :\n                 self.reset()\n                 sum_reward -= 1000\n"
                },
                {
                    "date": 1692258929054,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -162,9 +162,9 @@\n         obs =self.get_observation()\n         mujoco.mj_step(self.model, self.data)\n         sum_reward = self.reward()\n         # sum_reward = 0.0\n-        print(\"응애\")\n+        # print(\"응애\")\n \n \n         for i in range(self.k) : \n             if(self.data.qpos[i] > self._max_joint_position[i]) :\n"
                },
                {
                    "date": 1692261789659,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -169,21 +169,21 @@\n         for i in range(self.k) : \n             if(self.data.qpos[i] > self._max_joint_position[i]) :\n                 self.reset()\n                 sum_reward -= 1000\n-                return obs, sum_reward, done, dict(), {}\n+                return obs, sum_reward, done, dict()\n             if(self.data.qpos[i] < self._min_joint_position[i]) :\n                 self.reset()\n                 sum_reward -= 1000\n-                return obs, sum_reward, done, dict(), {}\n+                return obs, sum_reward, done, dict()\n \n \n         if self.data.time > self.end_time:\n             for i in range(3):\n                 if abs(self.X_goal[i] - self.X[i]) >= 0.05:\n                     self.reset()\n                     sum_reward -= 1000\n-                    return obs, sum_reward, done, dict(), {}\n+                    return obs, sum_reward, done, dict()\n                 else:\n                     if i == 2:\n                         position_state = 1\n \n@@ -193,9 +193,9 @@\n                 else:\n                     if abs(self.rot_mat_goal[i] - self.rot_mat_pre[i]) >= 0.1:\n                         self.reset()\n                         sum_reward -= 1000\n-                        return obs, sum_reward, done, dict(), {}\n+                        return obs, sum_reward, done, dict()\n                     else:\n                         if i == 7:\n                             rotate_state = 1\n             if position_state == 1 and rotate_state == 1:\n@@ -204,10 +204,10 @@\n                 self.controller.reset_goal()\n                 self.reset_time()\n                 # print(\"!!??\")\n                 # mujoco.mj_step(self.model, self.data)\n-                return obs, sum_reward, done, dict(), {}        \n-        return obs, sum_reward, done, dict(), {}\n+                return obs, sum_reward, done, dict()        \n+        return obs, sum_reward, done, dict()\n     # def render(self):\n     #     if self.viewer is None:\n     #         self.viewer = viewer.launch_passive(model=self.model, data=self.data)\n     #     else:\n"
                },
                {
                    "date": 1692261797762,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -169,21 +169,21 @@\n         for i in range(self.k) : \n             if(self.data.qpos[i] > self._max_joint_position[i]) :\n                 self.reset()\n                 sum_reward -= 1000\n-                return obs, sum_reward, done, dict()\n+                return obs, sum_reward, done, dict(), {}\n             if(self.data.qpos[i] < self._min_joint_position[i]) :\n                 self.reset()\n                 sum_reward -= 1000\n-                return obs, sum_reward, done, dict()\n+                return obs, sum_reward, done, dict(), {}\n \n \n         if self.data.time > self.end_time:\n             for i in range(3):\n                 if abs(self.X_goal[i] - self.X[i]) >= 0.05:\n                     self.reset()\n                     sum_reward -= 1000\n-                    return obs, sum_reward, done, dict()\n+                    return obs, sum_reward, done, dict(), {}\n                 else:\n                     if i == 2:\n                         position_state = 1\n \n@@ -193,9 +193,9 @@\n                 else:\n                     if abs(self.rot_mat_goal[i] - self.rot_mat_pre[i]) >= 0.1:\n                         self.reset()\n                         sum_reward -= 1000\n-                        return obs, sum_reward, done, dict()\n+                        return obs, sum_reward, done, dict(), {}\n                     else:\n                         if i == 7:\n                             rotate_state = 1\n             if position_state == 1 and rotate_state == 1:\n@@ -204,10 +204,10 @@\n                 self.controller.reset_goal()\n                 self.reset_time()\n                 # print(\"!!??\")\n                 # mujoco.mj_step(self.model, self.data)\n-                return obs, sum_reward, done, dict()        \n-        return obs, sum_reward, done, dict()\n+                return obs, sum_reward, done, dict(), {}        \n+        return obs, sum_reward, done, dict(), {}\n     # def render(self):\n     #     if self.viewer is None:\n     #         self.viewer = viewer.launch_passive(model=self.model, data=self.data)\n     #     else:\n"
                },
                {
                    "date": 1692320695448,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -380,9 +380,9 @@\n         #     'initial_state': self.get_observation,  # 초기 상태 정보\n         #     'custom_info': 'This is custom environment info'  # 기타 정보\n         # }\n         \n-        return self.get_observation(), {}\n+        return self.get_observation()#, {}\n     \n     def reset_time(self):\n         self.end_time = self.data.time + self.motion_time\n     \n"
                },
                {
                    "date": 1692321433786,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -380,9 +380,10 @@\n         #     'initial_state': self.get_observation,  # 초기 상태 정보\n         #     'custom_info': 'This is custom environment info'  # 기타 정보\n         # }\n         \n-        return self.get_observation()#, {}\n+        return self.get_observation(), {}\n+        # return self.get_observation()#, {}\n     \n     def reset_time(self):\n         self.end_time = self.data.time + self.motion_time\n     \n"
                },
                {
                    "date": 1692322852262,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -302,12 +302,12 @@\n         return gym.spaces.Box(obs_low, obs_high, dtype=np.float64)\n     \n     def get_observation(self):\n         # self.sim_state = self.sim.get_state()\n-        self.x_pos[0] = self.X[0] / 0.6 # -0.6~0.6\n-        self.x_pos[1] = self.X[1] / 0.6 # -0.6~0.6\n-        self.x_pos[2] = self.X[2] -0.35 # 0.1~0.8\n-        self.x_pos[2] = self.x_pos[2] / 0.45\n+        self.x_pos[0] = self.X[0] / 0.5 # -0.6~0.6\n+        self.x_pos[1] = self.X[1] / 0.5 # -0.6~0.6\n+        self.x_pos[2] = self.X[2] -0.3 # -0.1~0.7\n+        self.x_pos[2] = self.x_pos[2] / 0.4\n \n         # self.x_pos[3] = self.X[3] / np.pi # -np.pi~np.pi\n         # self.x_pos[4] = self.X[4] / np.pi #      \"\n         # self.x_pos[5] = self.X[5] / np.pi #      \"\n"
                },
                {
                    "date": 1692322858716,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -317,12 +317,12 @@\n         self.x_pos[6] = self.rot_mat_pre[4]\n         self.x_pos[7] = self.rot_mat_pre[6]\n         self.x_pos[8] = self.rot_mat_pre[7]\n \n-        self.goal[0] = self.X_goal[0] / 0.6\n-        self.goal[1] = self.X_goal[1] / 0.6\n-        self.goal[2] = self.X_goal[2] -0.35\n-        self.goal[2] = self.goal[2] / 0.45\n+        self.goal[0] = self.X_goal[0] / 0.5\n+        self.goal[1] = self.X_goal[1] / 0.5\n+        self.goal[2] = self.X_goal[2] -0.3\n+        self.goal[2] = self.goal[2] / 0.4\n \n         # self.goal[3] = self.X_goal[3] / np.pi\n         # self.goal[4] = self.X_goal[4] / np.pi\n         # self.goal[5] = self.X_goal[5] / np.pi\n"
                },
                {
                    "date": 1692324448223,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -168,14 +168,12 @@\n \n         for i in range(self.k) : \n             if(self.data.qpos[i] > self._max_joint_position[i]) :\n                 self.reset()\n-                sum_reward -= 1000\n-                return obs, sum_reward, done, dict(), {}\n+                sum_reward -= 50\n             if(self.data.qpos[i] < self._min_joint_position[i]) :\n                 self.reset()\n-                sum_reward -= 1000\n-                return obs, sum_reward, done, dict(), {}\n+                sum_reward -= 50\n \n \n         if self.data.time > self.end_time:\n             for i in range(3):\n"
                },
                {
                    "date": 1692324604322,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -167,13 +167,15 @@\n \n \n         for i in range(self.k) : \n             if(self.data.qpos[i] > self._max_joint_position[i]) :\n-                self.reset()\n                 sum_reward -= 50\n+                # self.reset()\n+                # return obs, sum_reward, done, dict(), {}\n             if(self.data.qpos[i] < self._min_joint_position[i]) :\n-                self.reset()\n                 sum_reward -= 50\n+                # self.reset()\n+                # return obs, sum_reward, done, dict(), {}\n \n \n         if self.data.time > self.end_time:\n             for i in range(3):\n"
                },
                {
                    "date": 1692325612498,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -485,27 +485,27 @@\n         return np.concatenate([self.sim.data.qpos.flat, self.sim.data.qvel.flat])\n \n     def reward(self):\n         #Reaching Goal Poses (Cost Function)\n-        tmp_reward = 0\n-        tmp_reward2 = 0\n+        # tmp_reward = 0\n+        # tmp_reward2 = 0\n \n-        for i in range(3):\n-            if self.X_goal[i] == self.X[i]:  # 분모가 0인 경우 처리\n-                tmp_reward += 1000\n-            else:\n-                tmp_reward += 1 / (self.X_goal[i] - self.X[i])\n+        # for i in range(3):\n+        #     if self.X_goal[i] == self.X[i]:  # 분모가 0인 경우 처리\n+        #         tmp_reward += 1000\n+        #     else:\n+        #         tmp_reward += 1 / (self.X_goal[i] - self.X[i])\n \n-        for i in range(9):\n-            if i == 2 or i == 5 or i == 8:\n-                tmp_reward2 = 0\n-            else :\n-                if self.rot_mat_goal[i] == self.rot_mat_pre[i]:  # 분모가 0인 경우 처리\n-                    tmp_reward2 += 1000\n-                else:\n-                    tmp_reward2 += 1 / (self.rot_mat_goal[i] - self.rot_mat_pre[i])\n+        # for i in range(9):\n+        #     if i == 2 or i == 5 or i == 8:\n+        #         tmp_reward2 = 0\n+        #     else :\n+        #         if self.rot_mat_goal[i] == self.rot_mat_pre[i]:  # 분모가 0인 경우 처리\n+        #             tmp_reward2 += 1000\n+        #         else:\n+        #             tmp_reward2 += 1 / (self.rot_mat_goal[i] - self.rot_mat_pre[i])\n     \n-        reward = tmp_reward + tmp_reward2            \n+        reward = 0        \n         return reward\n     \n     def seed(self, seed=None):\n         return \n\\ No newline at end of file\n"
                },
                {
                    "date": 1692334632044,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -162,9 +162,9 @@\n         obs =self.get_observation()\n         mujoco.mj_step(self.model, self.data)\n         sum_reward = self.reward()\n         # sum_reward = 0.0\n-        # print(\"응애\")\n+        print(\"응애\")\n \n \n         for i in range(self.k) : \n             if(self.data.qpos[i] > self._max_joint_position[i]) :\n"
                },
                {
                    "date": 1692334651695,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -162,9 +162,9 @@\n         obs =self.get_observation()\n         mujoco.mj_step(self.model, self.data)\n         sum_reward = self.reward()\n         # sum_reward = 0.0\n-        print(\"응애\")\n+        # print(\"응애\")\n \n \n         for i in range(self.k) : \n             if(self.data.qpos[i] > self._max_joint_position[i]) :\n"
                },
                {
                    "date": 1692334684458,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -162,18 +162,20 @@\n         obs =self.get_observation()\n         mujoco.mj_step(self.model, self.data)\n         sum_reward = self.reward()\n         # sum_reward = 0.0\n-        # print(\"응애\")\n+        print(\"응애\")\n \n \n         for i in range(self.k) : \n             if(self.data.qpos[i] > self._max_joint_position[i]) :\n                 sum_reward -= 50\n+                print(\"응애1\")\n                 # self.reset()\n                 # return obs, sum_reward, done, dict(), {}\n             if(self.data.qpos[i] < self._min_joint_position[i]) :\n                 sum_reward -= 50\n+                print(\"응애2\")\n                 # self.reset()\n                 # return obs, sum_reward, done, dict(), {}\n \n \n@@ -181,29 +183,35 @@\n             for i in range(3):\n                 if abs(self.X_goal[i] - self.X[i]) >= 0.05:\n                     self.reset()\n                     sum_reward -= 1000\n+                    print(\"응애3\")\n                     return obs, sum_reward, done, dict(), {}\n                 else:\n                     if i == 2:\n+                        print(\"응애4\")\n                         position_state = 1\n \n             for i in range(8):\n                 if i == 2 or i == 5:\n                     sum_reward += 0\n+                    print(\"응애5\")\n                 else:\n                     if abs(self.rot_mat_goal[i] - self.rot_mat_pre[i]) >= 0.1:\n                         self.reset()\n                         sum_reward -= 1000\n+                        print(\"응애6\")\n                         return obs, sum_reward, done, dict(), {}\n                     else:\n                         if i == 7:\n                             rotate_state = 1\n+                            print(\"응애7\")\n             if position_state == 1 and rotate_state == 1:\n                 sum_reward += 1000\n                 # self.reset()\n                 self.controller.reset_goal()\n                 self.reset_time()\n+                print(\"응애8\")\n                 # print(\"!!??\")\n                 # mujoco.mj_step(self.model, self.data)\n                 return obs, sum_reward, done, dict(), {}        \n         return obs, sum_reward, done, dict(), {}\n"
                },
                {
                    "date": 1692334700355,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -212,9 +212,10 @@\n                 self.reset_time()\n                 print(\"응애8\")\n                 # print(\"!!??\")\n                 # mujoco.mj_step(self.model, self.data)\n-                return obs, sum_reward, done, dict(), {}        \n+                return obs, sum_reward, done, dict(), {}\n+        print(\"응애9\")      \n         return obs, sum_reward, done, dict(), {}\n     # def render(self):\n     #     if self.viewer is None:\n     #         self.viewer = viewer.launch_passive(model=self.model, data=self.data)\n"
                },
                {
                    "date": 1692334725450,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -162,20 +162,16 @@\n         obs =self.get_observation()\n         mujoco.mj_step(self.model, self.data)\n         sum_reward = self.reward()\n         # sum_reward = 0.0\n-        print(\"응애\")\n \n-\n         for i in range(self.k) : \n             if(self.data.qpos[i] > self._max_joint_position[i]) :\n                 sum_reward -= 50\n-                print(\"응애1\")\n                 # self.reset()\n                 # return obs, sum_reward, done, dict(), {}\n             if(self.data.qpos[i] < self._min_joint_position[i]) :\n                 sum_reward -= 50\n-                print(\"응애2\")\n                 # self.reset()\n                 # return obs, sum_reward, done, dict(), {}\n \n \n@@ -183,39 +179,32 @@\n             for i in range(3):\n                 if abs(self.X_goal[i] - self.X[i]) >= 0.05:\n                     self.reset()\n                     sum_reward -= 1000\n-                    print(\"응애3\")\n                     return obs, sum_reward, done, dict(), {}\n                 else:\n                     if i == 2:\n-                        print(\"응애4\")\n                         position_state = 1\n \n             for i in range(8):\n                 if i == 2 or i == 5:\n                     sum_reward += 0\n-                    print(\"응애5\")\n                 else:\n                     if abs(self.rot_mat_goal[i] - self.rot_mat_pre[i]) >= 0.1:\n                         self.reset()\n                         sum_reward -= 1000\n-                        print(\"응애6\")\n                         return obs, sum_reward, done, dict(), {}\n                     else:\n                         if i == 7:\n                             rotate_state = 1\n-                            print(\"응애7\")\n             if position_state == 1 and rotate_state == 1:\n                 sum_reward += 1000\n                 # self.reset()\n                 self.controller.reset_goal()\n                 self.reset_time()\n-                print(\"응애8\")\n                 # print(\"!!??\")\n                 # mujoco.mj_step(self.model, self.data)\n                 return obs, sum_reward, done, dict(), {}\n-        print(\"응애9\")      \n         return obs, sum_reward, done, dict(), {}\n     # def render(self):\n     #     if self.viewer is None:\n     #         self.viewer = viewer.launch_passive(model=self.model, data=self.data)\n"
                },
                {
                    "date": 1692334731034,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -203,9 +203,9 @@\n                 self.reset_time()\n                 # print(\"!!??\")\n                 # mujoco.mj_step(self.model, self.data)\n                 return obs, sum_reward, done, dict(), {}\n-        return obs, sum_reward, done, dict(), {}\n+        return obs, sum_reward, done, dict()\n     # def render(self):\n     #     if self.viewer is None:\n     #         self.viewer = viewer.launch_passive(model=self.model, data=self.data)\n     #     else:\n"
                },
                {
                    "date": 1692334736537,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -203,9 +203,9 @@\n                 self.reset_time()\n                 # print(\"!!??\")\n                 # mujoco.mj_step(self.model, self.data)\n                 return obs, sum_reward, done, dict(), {}\n-        return obs, sum_reward, done, dict()\n+        return obs, sum_reward, done, dict(), {}\n     # def render(self):\n     #     if self.viewer is None:\n     #         self.viewer = viewer.launch_passive(model=self.model, data=self.data)\n     #     else:\n"
                },
                {
                    "date": 1692334765105,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -203,9 +203,9 @@\n                 self.reset_time()\n                 # print(\"!!??\")\n                 # mujoco.mj_step(self.model, self.data)\n                 return obs, sum_reward, done, dict(), {}\n-        return obs, sum_reward, done, dict(), {}\n+        return obs, sum_reward, done, 0, {}\n     # def render(self):\n     #     if self.viewer is None:\n     #         self.viewer = viewer.launch_passive(model=self.model, data=self.data)\n     #     else:\n"
                },
                {
                    "date": 1692334801836,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -168,20 +168,23 @@\n             if(self.data.qpos[i] > self._max_joint_position[i]) :\n                 sum_reward -= 50\n                 # self.reset()\n                 # return obs, sum_reward, done, dict(), {}\n+                # return obs, sum_reward, done, 0, {}\n             if(self.data.qpos[i] < self._min_joint_position[i]) :\n                 sum_reward -= 50\n                 # self.reset()\n                 # return obs, sum_reward, done, dict(), {}\n+                # return obs, sum_reward, done, 0, {}\n \n \n         if self.data.time > self.end_time:\n             for i in range(3):\n                 if abs(self.X_goal[i] - self.X[i]) >= 0.05:\n                     self.reset()\n                     sum_reward -= 1000\n-                    return obs, sum_reward, done, dict(), {}\n+                    # return obs, sum_reward, done, dict(), {}\n+                    return obs, sum_reward, done, 0, {}\n                 else:\n                     if i == 2:\n                         position_state = 1\n \n@@ -191,9 +194,10 @@\n                 else:\n                     if abs(self.rot_mat_goal[i] - self.rot_mat_pre[i]) >= 0.1:\n                         self.reset()\n                         sum_reward -= 1000\n-                        return obs, sum_reward, done, dict(), {}\n+                        # return obs, sum_reward, done, dict(), {}\n+                        return obs, sum_reward, done, 0, {}\n                     else:\n                         if i == 7:\n                             rotate_state = 1\n             if position_state == 1 and rotate_state == 1:\n@@ -202,10 +206,14 @@\n                 self.controller.reset_goal()\n                 self.reset_time()\n                 # print(\"!!??\")\n                 # mujoco.mj_step(self.model, self.data)\n-                return obs, sum_reward, done, dict(), {}\n+                # return obs, sum_reward, done, dict(), {}\n+                return obs, sum_reward, done, 0, {}\n+            \n+        # return obs, sum_reward, done, dict(), {}\n         return obs, sum_reward, done, 0, {}\n+    \n     # def render(self):\n     #     if self.viewer is None:\n     #         self.viewer = viewer.launch_passive(model=self.model, data=self.data)\n     #     else:\n"
                },
                {
                    "date": 1692338619627,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -182,9 +182,9 @@\n                 if abs(self.X_goal[i] - self.X[i]) >= 0.05:\n                     self.reset()\n                     sum_reward -= 1000\n                     # return obs, sum_reward, done, dict(), {}\n-                    return obs, sum_reward, done, 0, {}\n+                    return obs, sum_reward, True, 0, {}\n                 else:\n                     if i == 2:\n                         position_state = 1\n \n@@ -195,9 +195,9 @@\n                     if abs(self.rot_mat_goal[i] - self.rot_mat_pre[i]) >= 0.1:\n                         self.reset()\n                         sum_reward -= 1000\n                         # return obs, sum_reward, done, dict(), {}\n-                        return obs, sum_reward, done, 0, {}\n+                        return obs, sum_reward, True, 0, {}\n                     else:\n                         if i == 7:\n                             rotate_state = 1\n             if position_state == 1 and rotate_state == 1:\n@@ -207,12 +207,12 @@\n                 self.reset_time()\n                 # print(\"!!??\")\n                 # mujoco.mj_step(self.model, self.data)\n                 # return obs, sum_reward, done, dict(), {}\n-                return obs, sum_reward, done, 0, {}\n+                return obs, sum_reward, True, 0, {}\n             \n         # return obs, sum_reward, done, dict(), {}\n-        return obs, sum_reward, done, 0, {}\n+        return obs, sum_reward, True, 0, {}\n     \n     # def render(self):\n     #     if self.viewer is None:\n     #         self.viewer = viewer.launch_passive(model=self.model, data=self.data)\n"
                },
                {
                    "date": 1692338668356,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -182,9 +182,10 @@\n                 if abs(self.X_goal[i] - self.X[i]) >= 0.05:\n                     self.reset()\n                     sum_reward -= 1000\n                     # return obs, sum_reward, done, dict(), {}\n-                    return obs, sum_reward, True, 0, {}\n+                    done = True\n+                    return obs, sum_reward, done, 0, {}\n                 else:\n                     if i == 2:\n                         position_state = 1\n \n@@ -195,9 +196,10 @@\n                     if abs(self.rot_mat_goal[i] - self.rot_mat_pre[i]) >= 0.1:\n                         self.reset()\n                         sum_reward -= 1000\n                         # return obs, sum_reward, done, dict(), {}\n-                        return obs, sum_reward, True, 0, {}\n+                        done = True\n+                        return obs, sum_reward, done, 0, {}\n                     else:\n                         if i == 7:\n                             rotate_state = 1\n             if position_state == 1 and rotate_state == 1:\n@@ -207,12 +209,14 @@\n                 self.reset_time()\n                 # print(\"!!??\")\n                 # mujoco.mj_step(self.model, self.data)\n                 # return obs, sum_reward, done, dict(), {}\n-                return obs, sum_reward, True, 0, {}\n+                done = True\n+                return obs, sum_reward, done, 0, {}\n             \n         # return obs, sum_reward, done, dict(), {}\n-        return obs, sum_reward, True, 0, {}\n+        done = True\n+        return obs, sum_reward, done, 0, {}\n     \n     # def render(self):\n     #     if self.viewer is None:\n     #         self.viewer = viewer.launch_passive(model=self.model, data=self.data)\n"
                },
                {
                    "date": 1692338692224,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -213,9 +213,9 @@\n                 done = True\n                 return obs, sum_reward, done, 0, {}\n             \n         # return obs, sum_reward, done, dict(), {}\n-        done = True\n+        # done = True\n         return obs, sum_reward, done, 0, {}\n     \n     # def render(self):\n     #     if self.viewer is None:\n"
                },
                {
                    "date": 1692341885310,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -209,9 +209,9 @@\n                 self.reset_time()\n                 # print(\"!!??\")\n                 # mujoco.mj_step(self.model, self.data)\n                 # return obs, sum_reward, done, dict(), {}\n-                done = True\n+                # done = True\n                 return obs, sum_reward, done, 0, {}\n             \n         # return obs, sum_reward, done, dict(), {}\n         # done = True\n"
                },
                {
                    "date": 1692341905244,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -165,14 +165,14 @@\n         # sum_reward = 0.0\n \n         for i in range(self.k) : \n             if(self.data.qpos[i] > self._max_joint_position[i]) :\n-                sum_reward -= 50\n+                sum_reward -= 5\n                 # self.reset()\n                 # return obs, sum_reward, done, dict(), {}\n                 # return obs, sum_reward, done, 0, {}\n             if(self.data.qpos[i] < self._min_joint_position[i]) :\n-                sum_reward -= 50\n+                sum_reward -= 5\n                 # self.reset()\n                 # return obs, sum_reward, done, dict(), {}\n                 # return obs, sum_reward, done, 0, {}\n \n"
                },
                {
                    "date": 1692580132301,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -165,14 +165,14 @@\n         # sum_reward = 0.0\n \n         for i in range(self.k) : \n             if(self.data.qpos[i] > self._max_joint_position[i]) :\n-                sum_reward -= 5\n+                sum_reward -= 0.01\n                 # self.reset()\n                 # return obs, sum_reward, done, dict(), {}\n                 # return obs, sum_reward, done, 0, {}\n             if(self.data.qpos[i] < self._min_joint_position[i]) :\n-                sum_reward -= 5\n+                sum_reward -= 0.01\n                 # self.reset()\n                 # return obs, sum_reward, done, dict(), {}\n                 # return obs, sum_reward, done, 0, {}\n \n@@ -390,9 +390,9 @@\n         #     'initial_state': self.get_observation,  # 초기 상태 정보\n         #     'custom_info': 'This is custom environment info'  # 기타 정보\n         # }\n         \n-        return self.get_observation(), {}\n+        return self.get_observation()#, {}\n         # return self.get_observation()#, {}\n     \n     def reset_time(self):\n         self.end_time = self.data.time + self.motion_time\n"
                },
                {
                    "date": 1692580261508,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -163,19 +163,19 @@\n         mujoco.mj_step(self.model, self.data)\n         sum_reward = self.reward()\n         # sum_reward = 0.0\n \n-        for i in range(self.k) : \n-            if(self.data.qpos[i] > self._max_joint_position[i]) :\n-                sum_reward -= 0.01\n-                # self.reset()\n-                # return obs, sum_reward, done, dict(), {}\n-                # return obs, sum_reward, done, 0, {}\n-            if(self.data.qpos[i] < self._min_joint_position[i]) :\n-                sum_reward -= 0.01\n-                # self.reset()\n-                # return obs, sum_reward, done, dict(), {}\n-                # return obs, sum_reward, done, 0, {}\n+        # for i in range(self.k) : \n+        #     if(self.data.qpos[i] > self._max_joint_position[i]) :\n+        #         sum_reward -= 0.01\n+        #         # self.reset()\n+        #         # return obs, sum_reward, done, dict(), {}\n+        #         # return obs, sum_reward, done, 0, {}\n+        #     if(self.data.qpos[i] < self._min_joint_position[i]) :\n+        #         sum_reward -= 0.01\n+        #         # self.reset()\n+        #         # return obs, sum_reward, done, dict(), {}\n+        #         # return obs, sum_reward, done, 0, {}\n \n \n         if self.data.time > self.end_time:\n             for i in range(3):\n"
                },
                {
                    "date": 1692580282598,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -163,8 +163,10 @@\n         mujoco.mj_step(self.model, self.data)\n         sum_reward = self.reward()\n         # sum_reward = 0.0\n \n+\n+        #########   Joint Limit    ###############\n         # for i in range(self.k) : \n         #     if(self.data.qpos[i] > self._max_joint_position[i]) :\n         #         sum_reward -= 0.01\n         #         # self.reset()\n@@ -174,8 +176,9 @@\n         #         sum_reward -= 0.01\n         #         # self.reset()\n         #         # return obs, sum_reward, done, dict(), {}\n         #         # return obs, sum_reward, done, 0, {}\n+        ############################################\n \n \n         if self.data.time > self.end_time:\n             for i in range(3):\n"
                },
                {
                    "date": 1692580371249,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -393,9 +393,10 @@\n         #     'initial_state': self.get_observation,  # 초기 상태 정보\n         #     'custom_info': 'This is custom environment info'  # 기타 정보\n         # }\n         \n-        return self.get_observation()#, {}\n+        return self.get_observation(), {}\n+        ### Predict 확인시 사용 ####################\n         # return self.get_observation()#, {}\n     \n     def reset_time(self):\n         self.end_time = self.data.time + self.motion_time\n"
                },
                {
                    "date": 1692580390636,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -393,11 +393,12 @@\n         #     'initial_state': self.get_observation,  # 초기 상태 정보\n         #     'custom_info': 'This is custom environment info'  # 기타 정보\n         # }\n         \n-        return self.get_observation(), {}\n+        #학습시 시용\n+        # return self.get_observation(), {}\n         ### Predict 확인시 사용 ####################\n-        # return self.get_observation()#, {}\n+        return self.get_observation()#, {}\n     \n     def reset_time(self):\n         self.end_time = self.data.time + self.motion_time\n     \n"
                },
                {
                    "date": 1692581168244,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -287,10 +287,10 @@\n         return rot_mat\n \n     def _construct_action_space(self):\n         # action_low = -1 * np.ones(12)\n-        action_low =  -1 * np.ones(2)\n-        action_high = np.ones(2) \n+        action_low =  -1 * np.ones(3)\n+        action_high = np.ones(3) \n         return gym.spaces.Box(low=action_low, high=action_high, dtype=np.float64)\n     \n     def matrixMultiply_3x3(self, mat_A, mat_B):\n         mat3 = np.zeros(9)\n"
                },
                {
                    "date": 1692581217206,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -110,8 +110,9 @@\n         done = False\n         self.temp_action = action\n         action = action + 1.0\n         action = action / 2.0\n+        action[2] = action[2] + 0.003\n         # action = action + 0.003\n         # action = action + 0.003\n         self.controller.read(self.data.time, self.data.qpos[0:self.k], self.data.qvel[0:self.k], action)\n         self.controller.control_mujoco()\n"
                },
                {
                    "date": 1692581246914,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -108,11 +108,11 @@\n         position_state = 0\n         rotate_state = 0\n         done = False\n         self.temp_action = action\n-        action = action + 1.0\n-        action = action / 2.0\n-        action[2] = action[2] + 0.003\n+        action = action + 1.0 # 범위가 0 ~ 2\n+        action = action / 2.0 # 범위가 0 ~ 1\n+        action[2] = action[2] + 0.003 # 범위가 0.003 ~ 1.003\n         # action = action + 0.003\n         # action = action + 0.003\n         self.controller.read(self.data.time, self.data.qpos[0:self.k], self.data.qvel[0:self.k], action)\n         self.controller.control_mujoco()\n"
                },
                {
                    "date": 1692581264152,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -308,10 +308,10 @@\n         mat3[8] = mat_A[6] * mat_B[2] + mat_A[7] * mat_B[5] + mat_A[8] * mat_B[8];\n         return mat3\n          \n     def _construct_observation_space(self):\n-        obs_low = -1 * np.ones(27) # X(9), X_goal(9), q(7), action(2) \n-        obs_high = 1 * np.ones(27)\n+        obs_low = -1 * np.ones(28) # X(9), X_goal(9), q(7), action(3) \n+        obs_high = 1 * np.ones(28)\n         # obs_low = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0,0,0, -1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1]\n         # obs_high = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0,0,0, 1,1,1,1,1,1,1,1,1,1,1,1]\n         return gym.spaces.Box(obs_low, obs_high, dtype=np.float64)\n     \n"
                },
                {
                    "date": 1692581312647,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -39,9 +39,9 @@\n         self.q_pos = np.zeros(7)\n         # self.reward_arr = np.zeros(2) #shape=(2,), dtype=np.float64\n         self.reward_arr = [0,0,0]\n         self.cnt = 1\n-        self.temp_action = np.zeros(2)\n+        self.temp_action = np.zeros(3)\n         self._max_joint_position = np.zeros(self.k)\n         self._min_joint_position = np.zeros(self.k)\n         self.timestep = 0\n \n@@ -364,9 +364,9 @@\n         (\n             self.x_pos, #9\n             self.goal, #9\n             self.q_pos, #7\n-            self.temp_action, # 2\n+            self.temp_action, # 3\n         )\n         )\n         \n     def reset(self, seed=None):\n"
                },
                {
                    "date": 1692581421361,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -395,11 +395,11 @@\n         #     'custom_info': 'This is custom environment info'  # 기타 정보\n         # }\n         \n         #학습시 시용\n-        # return self.get_observation(), {}\n+        return self.get_observation(), {}\n         ### Predict 확인시 사용 ####################\n-        return self.get_observation()#, {}\n+        # return self.get_observation()#, {}\n     \n     def reset_time(self):\n         self.end_time = self.data.time + self.motion_time\n     \n"
                },
                {
                    "date": 1692594952686,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -110,9 +110,10 @@\n         done = False\n         self.temp_action = action\n         action = action + 1.0 # 범위가 0 ~ 2\n         action = action / 2.0 # 범위가 0 ~ 1\n-        action[2] = action[2] + 0.003 # 범위가 0.003 ~ 1.003\n+        action[2] = action[2] * 2 / 3 # 범위가 0 ~ 0.666\n+        action[2] = action[2] + 0.003 # 범위가 0.003 ~ 0.669\n         # action = action + 0.003\n         # action = action + 0.003\n         self.controller.read(self.data.time, self.data.qpos[0:self.k], self.data.qvel[0:self.k], action)\n         self.controller.control_mujoco()\n"
                },
                {
                    "date": 1692605783371,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -114,8 +114,9 @@\n         action[2] = action[2] * 2 / 3 # 범위가 0 ~ 0.666\n         action[2] = action[2] + 0.003 # 범위가 0.003 ~ 0.669\n         # action = action + 0.003\n         # action = action + 0.003\n+        for i in range(10):\n         self.controller.read(self.data.time, self.data.qpos[0:self.k], self.data.qvel[0:self.k], action)\n         self.controller.control_mujoco()\n         self.tmp_X = self.controller.state_controller()\n         self.rot_mat_goal = self.GetBodyRotationMatrix(self.tmp_X[3],self.tmp_X[4],self.tmp_X[5])\n@@ -162,8 +163,10 @@\n         for i in range(4):\n             self.data.qpos[12+i] = self.quaternion_goal[i]\n         obs =self.get_observation()\n         mujoco.mj_step(self.model, self.data)\n+\n+\n         sum_reward = self.reward()\n         # sum_reward = 0.0\n \n \n"
                },
                {
                    "date": 1692605788435,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -115,56 +115,56 @@\n         action[2] = action[2] + 0.003 # 범위가 0.003 ~ 0.669\n         # action = action + 0.003\n         # action = action + 0.003\n         for i in range(10):\n-        self.controller.read(self.data.time, self.data.qpos[0:self.k], self.data.qvel[0:self.k], action)\n-        self.controller.control_mujoco()\n-        self.tmp_X = self.controller.state_controller()\n-        self.rot_mat_goal = self.GetBodyRotationMatrix(self.tmp_X[3],self.tmp_X[4],self.tmp_X[5])\n-        self.rot_mat_pre = self.GetBodyRotationMatrix(self.tmp_X[9],self.tmp_X[10],self.tmp_X[11])\n-        \n-        # print(self.rot_mat_pre)\n-        \n-        # GetBodyRotationMatrix\n-        self.torque = self.controller.write()\n-        for i in range(self.k):\n-                self.data.ctrl[i] = self.torque[i]\n-        for i in range(6):\n-            self.X_goal[i] = self.tmp_X[i]\n-            # self.data.qpos[0:self.k]\n-        for i in range(6):\n-            self.X[i] = self.tmp_X[i+6]\n-        for i in range(7):\n-            self.q[i] = self.tmp_X[i+12]\n+            self.controller.read(self.data.time, self.data.qpos[0:self.k], self.data.qvel[0:self.k], action)\n+            self.controller.control_mujoco()\n+            self.tmp_X = self.controller.state_controller()\n+            self.rot_mat_goal = self.GetBodyRotationMatrix(self.tmp_X[3],self.tmp_X[4],self.tmp_X[5])\n+            self.rot_mat_pre = self.GetBodyRotationMatrix(self.tmp_X[9],self.tmp_X[10],self.tmp_X[11])\n+            \n+            # print(self.rot_mat_pre)\n+            \n+            # GetBodyRotationMatrix\n+            self.torque = self.controller.write()\n+            for i in range(self.k):\n+                    self.data.ctrl[i] = self.torque[i]\n+            for i in range(6):\n+                self.X_goal[i] = self.tmp_X[i]\n+                # self.data.qpos[0:self.k]\n+            for i in range(6):\n+                self.X[i] = self.tmp_X[i+6]\n+            for i in range(7):\n+                self.q[i] = self.tmp_X[i+12]\n \n-        # print(\"goal_rotate\")\n-        # print(self.rot_mat_goal[0],self.rot_mat_goal[1],self.rot_mat_goal[2])\n-        # print(self.rot_mat_goal[3],self.rot_mat_goal[4],self.rot_mat_goal[5])\n-        # print(self.rot_mat_goal[6],self.rot_mat_goal[7],self.rot_mat_goal[8])\n-        # print(\"\")\n-        # print(\"pre_rotate\")\n-        # print(self.rot_mat_pre[0],self.rot_mat_pre[1],self.rot_mat_pre[2])\n-        # print(self.rot_mat_pre[3],self.rot_mat_pre[4],self.rot_mat_pre[5])\n-        # print(self.rot_mat_pre[6],self.rot_mat_pre[7],self.rot_mat_pre[8])\n-        # print(\"\")\n-        # print(\"goal_pos\")\n-        # print(self.X_goal[0],self.X_goal[1],self.X_goal[2])\n-        # print(\"\")\n-        # print(\"pre_pos\")\n-        # print(self.X[0],self.X[1],self.X[2])\n-        # print(\"\")\n+            # print(\"goal_rotate\")\n+            # print(self.rot_mat_goal[0],self.rot_mat_goal[1],self.rot_mat_goal[2])\n+            # print(self.rot_mat_goal[3],self.rot_mat_goal[4],self.rot_mat_goal[5])\n+            # print(self.rot_mat_goal[6],self.rot_mat_goal[7],self.rot_mat_goal[8])\n+            # print(\"\")\n+            # print(\"pre_rotate\")\n+            # print(self.rot_mat_pre[0],self.rot_mat_pre[1],self.rot_mat_pre[2])\n+            # print(self.rot_mat_pre[3],self.rot_mat_pre[4],self.rot_mat_pre[5])\n+            # print(self.rot_mat_pre[6],self.rot_mat_pre[7],self.rot_mat_pre[8])\n+            # print(\"\")\n+            # print(\"goal_pos\")\n+            # print(self.X_goal[0],self.X_goal[1],self.X_goal[2])\n+            # print(\"\")\n+            # print(\"pre_pos\")\n+            # print(self.X[0],self.X[1],self.X[2])\n+            # print(\"\")\n \n \n-        self.euler_to_quaternion(self.X_goal[3],self.X_goal[4],self.X_goal[5])\n-        # print(self.data.qpos)\n-        # print(self.data.xpos[13,0])\n-        # xpos랑 xmat을 건드려야함\n-        for i in range(3):\n-            self.data.qpos[9+i] = self.X_goal[i]\n-        for i in range(4):\n-            self.data.qpos[12+i] = self.quaternion_goal[i]\n-        obs =self.get_observation()\n-        mujoco.mj_step(self.model, self.data)\n+            self.euler_to_quaternion(self.X_goal[3],self.X_goal[4],self.X_goal[5])\n+            # print(self.data.qpos)\n+            # print(self.data.xpos[13,0])\n+            # xpos랑 xmat을 건드려야함\n+            for i in range(3):\n+                self.data.qpos[9+i] = self.X_goal[i]\n+            for i in range(4):\n+                self.data.qpos[12+i] = self.quaternion_goal[i]\n+            obs =self.get_observation()\n+            mujoco.mj_step(self.model, self.data)\n \n \n         sum_reward = self.reward()\n         # sum_reward = 0.0\n"
                },
                {
                    "date": 1692605917106,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -117,56 +117,35 @@\n         # action = action + 0.003\n         for i in range(10):\n             self.controller.read(self.data.time, self.data.qpos[0:self.k], self.data.qvel[0:self.k], action)\n             self.controller.control_mujoco()\n-            self.tmp_X = self.controller.state_controller()\n-            self.rot_mat_goal = self.GetBodyRotationMatrix(self.tmp_X[3],self.tmp_X[4],self.tmp_X[5])\n-            self.rot_mat_pre = self.GetBodyRotationMatrix(self.tmp_X[9],self.tmp_X[10],self.tmp_X[11])\n-            \n-            # print(self.rot_mat_pre)\n-            \n-            # GetBodyRotationMatrix\n             self.torque = self.controller.write()\n-            for i in range(self.k):\n-                    self.data.ctrl[i] = self.torque[i]\n-            for i in range(6):\n-                self.X_goal[i] = self.tmp_X[i]\n-                # self.data.qpos[0:self.k]\n-            for i in range(6):\n-                self.X[i] = self.tmp_X[i+6]\n-            for i in range(7):\n-                self.q[i] = self.tmp_X[i+12]\n \n-            # print(\"goal_rotate\")\n-            # print(self.rot_mat_goal[0],self.rot_mat_goal[1],self.rot_mat_goal[2])\n-            # print(self.rot_mat_goal[3],self.rot_mat_goal[4],self.rot_mat_goal[5])\n-            # print(self.rot_mat_goal[6],self.rot_mat_goal[7],self.rot_mat_goal[8])\n-            # print(\"\")\n-            # print(\"pre_rotate\")\n-            # print(self.rot_mat_pre[0],self.rot_mat_pre[1],self.rot_mat_pre[2])\n-            # print(self.rot_mat_pre[3],self.rot_mat_pre[4],self.rot_mat_pre[5])\n-            # print(self.rot_mat_pre[6],self.rot_mat_pre[7],self.rot_mat_pre[8])\n-            # print(\"\")\n-            # print(\"goal_pos\")\n-            # print(self.X_goal[0],self.X_goal[1],self.X_goal[2])\n-            # print(\"\")\n-            # print(\"pre_pos\")\n-            # print(self.X[0],self.X[1],self.X[2])\n-            # print(\"\")\n-\n-\n-            self.euler_to_quaternion(self.X_goal[3],self.X_goal[4],self.X_goal[5])\n-            # print(self.data.qpos)\n-            # print(self.data.xpos[13,0])\n-            # xpos랑 xmat을 건드려야함\n-            for i in range(3):\n-                self.data.qpos[9+i] = self.X_goal[i]\n-            for i in range(4):\n-                self.data.qpos[12+i] = self.quaternion_goal[i]\n+            \n             obs =self.get_observation()\n             mujoco.mj_step(self.model, self.data)\n \n-\n+        self.tmp_X = self.controller.state_controller()\n+        self.rot_mat_goal = self.GetBodyRotationMatrix(self.tmp_X[3],self.tmp_X[4],self.tmp_X[5])\n+        self.rot_mat_pre = self.GetBodyRotationMatrix(self.tmp_X[9],self.tmp_X[10],self.tmp_X[11])\n+        for i in range(self.k):\n+                self.data.ctrl[i] = self.torque[i]\n+        for i in range(6):\n+            self.X_goal[i] = self.tmp_X[i]\n+            # self.data.qpos[0:self.k]\n+        for i in range(6):\n+            self.X[i] = self.tmp_X[i+6]\n+        for i in range(7):\n+            self.q[i] = self.tmp_X[i+12]\n+        self.euler_to_quaternion(self.X_goal[3],self.X_goal[4],self.X_goal[5])\n+        # print(self.data.qpos)\n+        # print(self.data.xpos[13,0])\n+        # xpos랑 xmat을 건드려야함\n+        for i in range(3):\n+            self.data.qpos[9+i] = self.X_goal[i]\n+        for i in range(4):\n+            self.data.qpos[12+i] = self.quaternion_goal[i]\n+            \n         sum_reward = self.reward()\n         # sum_reward = 0.0\n \n \n"
                },
                {
                    "date": 1692605928955,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -143,10 +143,10 @@\n         for i in range(3):\n             self.data.qpos[9+i] = self.X_goal[i]\n         for i in range(4):\n             self.data.qpos[12+i] = self.quaternion_goal[i]\n-            \n-        sum_reward = self.reward()\n+\n+        # sum_reward = self.reward()\n         # sum_reward = 0.0\n \n \n         #########   Joint Limit    ###############\n"
                },
                {
                    "date": 1692605949181,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -118,11 +118,8 @@\n         for i in range(10):\n             self.controller.read(self.data.time, self.data.qpos[0:self.k], self.data.qvel[0:self.k], action)\n             self.controller.control_mujoco()\n             self.torque = self.controller.write()\n-\n-            \n-            obs =self.get_observation()\n             mujoco.mj_step(self.model, self.data)\n \n         self.tmp_X = self.controller.state_controller()\n         self.rot_mat_goal = self.GetBodyRotationMatrix(self.tmp_X[3],self.tmp_X[4],self.tmp_X[5])\n@@ -143,8 +140,9 @@\n         for i in range(3):\n             self.data.qpos[9+i] = self.X_goal[i]\n         for i in range(4):\n             self.data.qpos[12+i] = self.quaternion_goal[i]\n+        obs =self.get_observation()\n \n         # sum_reward = self.reward()\n         # sum_reward = 0.0\n \n"
                },
                {
                    "date": 1692605963559,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -124,9 +124,9 @@\n         self.tmp_X = self.controller.state_controller()\n         self.rot_mat_goal = self.GetBodyRotationMatrix(self.tmp_X[3],self.tmp_X[4],self.tmp_X[5])\n         self.rot_mat_pre = self.GetBodyRotationMatrix(self.tmp_X[9],self.tmp_X[10],self.tmp_X[11])\n         for i in range(self.k):\n-                self.data.ctrl[i] = self.torque[i]\n+            self.data.ctrl[i] = self.torque[i]\n         for i in range(6):\n             self.X_goal[i] = self.tmp_X[i]\n             # self.data.qpos[0:self.k]\n         for i in range(6):\n"
                },
                {
                    "date": 1692605977518,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -118,15 +118,16 @@\n         for i in range(10):\n             self.controller.read(self.data.time, self.data.qpos[0:self.k], self.data.qvel[0:self.k], action)\n             self.controller.control_mujoco()\n             self.torque = self.controller.write()\n+            for i in range(self.k):\n+                self.data.ctrl[i] = self.torque[i]\n             mujoco.mj_step(self.model, self.data)\n \n         self.tmp_X = self.controller.state_controller()\n         self.rot_mat_goal = self.GetBodyRotationMatrix(self.tmp_X[3],self.tmp_X[4],self.tmp_X[5])\n         self.rot_mat_pre = self.GetBodyRotationMatrix(self.tmp_X[9],self.tmp_X[10],self.tmp_X[11])\n-        for i in range(self.k):\n-            self.data.ctrl[i] = self.torque[i]\n+        \n         for i in range(6):\n             self.X_goal[i] = self.tmp_X[i]\n             # self.data.qpos[0:self.k]\n         for i in range(6):\n"
                },
                {
                    "date": 1692605991117,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -128,13 +128,13 @@\n         self.rot_mat_pre = self.GetBodyRotationMatrix(self.tmp_X[9],self.tmp_X[10],self.tmp_X[11])\n         \n         for i in range(6):\n             self.X_goal[i] = self.tmp_X[i]\n-            # self.data.qpos[0:self.k]\n         for i in range(6):\n             self.X[i] = self.tmp_X[i+6]\n         for i in range(7):\n             self.q[i] = self.tmp_X[i+12]\n+        \n         self.euler_to_quaternion(self.X_goal[3],self.X_goal[4],self.X_goal[5])\n         # print(self.data.qpos)\n         # print(self.data.xpos[13,0])\n         # xpos랑 xmat을 건드려야함\n"
                },
                {
                    "date": 1692607159856,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -114,9 +114,9 @@\n         action[2] = action[2] * 2 / 3 # 범위가 0 ~ 0.666\n         action[2] = action[2] + 0.003 # 범위가 0.003 ~ 0.669\n         # action = action + 0.003\n         # action = action + 0.003\n-        for i in range(10):\n+        for i in range(5):\n             self.controller.read(self.data.time, self.data.qpos[0:self.k], self.data.qvel[0:self.k], action)\n             self.controller.control_mujoco()\n             self.torque = self.controller.write()\n             for i in range(self.k):\n"
                },
                {
                    "date": 1692668544812,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -108,11 +108,12 @@\n         position_state = 0\n         rotate_state = 0\n         done = False\n         self.temp_action = action\n-        action = action + 1.0 # 범위가 0 ~ 2\n-        action = action / 2.0 # 범위가 0 ~ 1\n-        action[2] = action[2] * 2 / 3 # 범위가 0 ~ 0.666\n+        # action = action + 1.0 # 범위가 0 ~ 2\n+        # action = action / 2.0 # 범위가 0 ~ 1\n+        action[2] = action[2] + 1.0 # 범위가 0 ~ 2\n+        action[2] = action[2] / 3 # 범위가 0 ~ 0.666\n         action[2] = action[2] + 0.003 # 범위가 0.003 ~ 0.669\n         # action = action + 0.003\n         # action = action + 0.003\n         for i in range(5):\n"
                },
                {
                    "date": 1692671807482,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -111,10 +111,10 @@\n         self.temp_action = action\n         # action = action + 1.0 # 범위가 0 ~ 2\n         # action = action / 2.0 # 범위가 0 ~ 1\n         action[2] = action[2] + 1.0 # 범위가 0 ~ 2\n-        action[2] = action[2] / 3 # 범위가 0 ~ 0.666\n-        action[2] = action[2] + 0.003 # 범위가 0.003 ~ 0.669\n+        action[2] = action[2] / 20 # 범위가 0 ~ 0.1\n+        action[2] = action[2] + 0.003 # 범위가 0.003 ~ 0.103\n         # action = action + 0.003\n         # action = action + 0.003\n         for i in range(5):\n             self.controller.read(self.data.time, self.data.qpos[0:self.k], self.data.qvel[0:self.k], action)\n"
                },
                {
                    "date": 1692672745297,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -111,10 +111,10 @@\n         self.temp_action = action\n         # action = action + 1.0 # 범위가 0 ~ 2\n         # action = action / 2.0 # 범위가 0 ~ 1\n         action[2] = action[2] + 1.0 # 범위가 0 ~ 2\n-        action[2] = action[2] / 20 # 범위가 0 ~ 0.1\n-        action[2] = action[2] + 0.003 # 범위가 0.003 ~ 0.103\n+        action[2] = action[2] / 10 # 범위가 0 ~ 0.1\n+        action[2] = action[2] + 0.003 # 범위가 0.003 ~ 0.203\n         # action = action + 0.003\n         # action = action + 0.003\n         for i in range(5):\n             self.controller.read(self.data.time, self.data.qpos[0:self.k], self.data.qvel[0:self.k], action)\n"
                },
                {
                    "date": 1692679192526,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -111,10 +111,10 @@\n         self.temp_action = action\n         # action = action + 1.0 # 범위가 0 ~ 2\n         # action = action / 2.0 # 범위가 0 ~ 1\n         action[2] = action[2] + 1.0 # 범위가 0 ~ 2\n-        action[2] = action[2] / 10 # 범위가 0 ~ 0.1\n-        action[2] = action[2] + 0.003 # 범위가 0.003 ~ 0.203\n+        action[2] = action[2] / 20 # 범위가 0 ~ 0.1\n+        action[2] = action[2] + 0.003 # 범위가 0.003 ~ 0.103\n         # action = action + 0.003\n         # action = action + 0.003\n         for i in range(5):\n             self.controller.read(self.data.time, self.data.qpos[0:self.k], self.data.qvel[0:self.k], action)\n"
                },
                {
                    "date": 1692679225428,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -70,9 +70,9 @@\n         \n         self.observation_space = self._construct_observation_space()\n         self.action_space = self._construct_action_space()\n         self.start_time = 0\n-        self.motion_time = 10\n+        self.motion_time = 15\n         self.end_time = self.start_time + self.motion_time\n \n \n         for i in range(self.k):\n"
                }
            ],
            "date": 1691482703701,
            "name": "Commit-0",
            "content": "from tokenize import Double\nimport gym\nimport numpy as np\n\nfrom mujoco_py import load_model_from_path, MjSim, MjViewer\nimport mujoco_py\n\nfrom gym import ObservationWrapper, spaces\nimport controller\n\nDEFAULT_SIZE = 300\nDEG2RAD = 3.14 / 180.0\n\nclass FrankaEnv(gym.Env):\n    def __init__(self):\n        self.model_path = 'model/fr3.xml'\n        self.frame_skip = 1\n        self.model = load_model_from_path(self.model_path)\n        self.sim = mujoco_py.MjSim(self.model)\n         \n        self.data = self.sim.data\n        self.viewer = None\n        self._viewers = {}\n        self.controller = controller.CController()\n        self.torque = np.ones(15)\n        # self.reward_arr = np.zeros(2) #shape=(2,), dtype=np.float64\n        self.reward_arr = [0,0,0]\n        self.cnt = 1\n        self.temp_action = np.zeros(12)\n        self._max_joint_position = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n        self._min_joint_position = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n        self.timestep = 0\n\n        self._max_joint_position[0] = 0.15\n        self._min_joint_position[0] = -0.15\n        self._max_joint_position[1] = 50.0 * DEG2RAD\n        self._min_joint_position[1] = -160.0 * DEG2RAD\n        self._max_joint_position[2] = 170.0 * DEG2RAD\n        self._min_joint_position[2] = -30.0 * DEG2RAD\n        self._max_joint_position[3] = 180.0 * DEG2RAD # 몸 안쪽 회전\n        self._min_joint_position[3] = -150.0 * DEG2RAD # 몸 바깥쪽 회전\n        self._max_joint_position[4] = 160.0 * DEG2RAD\n        self._min_joint_position[4] = -30.0 * DEG2RAD\n        self._max_joint_position[5] = 170.0 * DEG2RAD\n        self._min_joint_position[5] = -170.0 * DEG2RAD\n        self._max_joint_position[6] = 90.0 * DEG2RAD\n        self._min_joint_position[6] = -90.0 * DEG2RAD\n        self._max_joint_position[7] = 45.0 * DEG2RAD\n        self._min_joint_position[7] = -45.0 * DEG2RAD\n        self._max_joint_position[8] = 160.0 * DEG2RAD\n        self._min_joint_position[8] = -50.0 * DEG2RAD\n        self._max_joint_position[9] = 30.0 * DEG2RAD\n        self._min_joint_position[9] = -150.0 * DEG2RAD\n        self._max_joint_position[10] = 150.0 * DEG2RAD # 몸 바깥쪽 회전\n        self._min_joint_position[10] = -150.0 * DEG2RAD # 몸 안쪽 회전\n        self._max_joint_position[11] = 30.0 * DEG2RAD\n        self._min_joint_position[11] = -160.0 * DEG2RAD\n        self._max_joint_position[12] = 170.0 * DEG2RAD\n        self._min_joint_position[12] = -170.0 * DEG2RAD\n        self._max_joint_position[13] = 90.0 * DEG2RAD\n        self._min_joint_position[13] = -90.0 * DEG2RAD\n        self._max_joint_position[14] = 45.0 * DEG2RAD\n        self._min_joint_position[14] = -45.0 * DEG2RAD\n\n        self._max_joint_position_hat = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n        self._min_joint_position_hat = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n        for i in range(15):\n            self._max_joint_position_hat[i] = self._max_joint_position[i] - 0.1*(self._max_joint_position[i] - self._min_joint_position[i])\n            self._min_joint_position_hat[i] = self._min_joint_position[i] + 0.1*(self._max_joint_position[i] - self._min_joint_position[i])    \n        \n        self.observation_space = self._construct_observation_space()\n        self.action_space = self._construct_action_space()\n        self.metadata = {\n            \"render.modes\": [\"human\", \"rgb_array\", \"depth_array\"],\n            \"video.frames_per_second\": int(np.round(1.0 / self.dt)),\n            # \"video.frames_per_second\": int(1000),\n        }\n        \n        self.sim_state = self.sim.get_state()\n        \n        \n    def step(self, action, render=True):\n        \n        # print('step')\n        sum_reward = 0\n        done = False\n        motion_cnt = self.controller.cnt()\n        # print(action)\n        self.controller.read(self.sim_state.time, self.sim_state.qpos, self.sim_state.qvel)\n        self.temp_action = action\n        # self.controller.slack(action)\n        self.controller.slack(action)\n        # print(self.temp_action)\n        # print(self.sim_state.time)\n        \n        self.controller.control_mujoco()\n        self.torque = self.controller.write()\n        self.do_simulation(self.torque, self.frame_skip)\n        obs =self.get_observation()\n        # print(obs)\n        sum_reward = self.reward()\n        if sum_reward == 21498098 :\n            done = True\n            self.controller.init_controller()\n            # print(action)\n            return obs, 0, done, dict()\n        # print(\"sum_reward : \")\n        # print(sum_reward)\n\n        if render:\n            self.render()\n\n        for i in range(15):\n            # print(i)\n            # print(\":\")\n            # print(self.sim_state.qpos[i])\n            # print(self._min_joint_position[i])\n            if self.sim_state.qpos[i] >=  self._max_joint_position[i] :\n                done = True\n                sum_reward = sum_reward - 10000\n                self.controller.init_controller()\n                # print('joint{%.1i} > max val'% (i))\n                # print(action)\n                return obs, sum_reward, done, dict()\n            \n            elif self.sim_state.qpos[i] <= self._min_joint_position[i] :\n                done = True\n                sum_reward = sum_reward - 10000\n                self.controller.init_controller()\n                # print('joint{%.1i} < min val'% (i))\n                # print(action)\n                return obs, sum_reward, done, dict()\n\n        # if abs(self.sim_state.qpos[0]) > 0.1 :\n        #     done = True\n        #     sum_reward = sum_reward -100000\n        #     self.controller.init_controller()\n        #     return obs, sum_reward, done, dict()\n        \n        # if motion_cnt[0] > 6:\n        #     done = True\n        #     self.controller.init_controller()\n        #     return obs, sum_reward, done, dict()\n\n\n        return obs, sum_reward, done, dict()\n    \n        \n    def _construct_action_space(self):\n        # action_low = -1 * np.ones(12)\n        action_low =  np.zeros(15)\n        action_high = np.ones(15) \n        return gym.spaces.Box(low=action_low, high=action_high, dtype=np.float64)\n    \n    \n    def _construct_observation_space(self):\n        obs_low = -1 * np.ones(39)\n        obs_high = 1 * np.ones(39)\n        # obs_low = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0,0,0, -1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1]\n        # obs_high = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0,0,0, 1,1,1,1,1,1,1,1,1,1,1,1]\n        return gym.spaces.Box(obs_low, obs_high)\n    \n     \n    \n    def get_observation(self):\n        self.sim_state = self.sim.get_state()\n        \n        joint_position = self.sim_state.qpos\n        \n        # joint_velocity = self.sim_state.qvel\n        xr_des = self.controller.xr_des()\n        # print(xr_des)\n        # joint_velocity = joint_velocity / 15.0\n        # for i in range(15) :    \n        #     if joint_velocity[i]>=1 :\n        #         print(\"joint_velocity[%.1i] >= 1\"%(i))\n        #         print(joint_velocity[i])\n        #     elif joint_velocity[i]<= -1 :\n        #         print(\"joint_velocity[%.1i] <= -1\"%(i))\n        #         print(joint_velocity[i])\n        \n\n        joint_position = self.sim_state.qpos / 3.141592\n        joint_position[0] = joint_position[0] * 3.141592\n        xr_des[0] = xr_des[0] / 0.9\n        xr_des[1] = xr_des[1] / 0.9\n        xr_des[2] = xr_des[2] / 1.7\n        \n        xr_des[3] = xr_des[3] / 0.9\n        xr_des[4] = xr_des[4] / 0.9\n        xr_des[5] = xr_des[5] / 1.7\n        \n        xr_des[6] = xr_des[6] / 3.141592\n        xr_des[7] = xr_des[7] / 3.141592\n        xr_des[8] = xr_des[8] / 3.141592\n        \n        xr_des[9] = xr_des[9] / 3.141592\n        xr_des[10] = xr_des[10] / 3.141592\n        xr_des[11] = xr_des[11] / 3.141592\n\n        self.temp_action = self.temp_action * 2\n        self.temp_action = self.temp_action - 1\n        return np.concatenate(\n        (\n            joint_position, #15\n            # joint_velocity, #15\n            xr_des, # 12\n            self.temp_action, # 12\n            # self.get_body_com(\"endEffector\"),# 3\n        )\n        )\n        \n    def reset(self):\n        qpos = np.zeros(15)\n        qvel = np.zeros(15)\n        self.set_state(qpos, qvel)\n        self.timestep = 0\n        self.temp_action = np.zeros(12)\n        \n        \n        return self.get_observation()\n    \n    \n    \n    def set_state(self, qpos, qvel):\n        old_state = self.sim.get_state()\n        new_state = mujoco_py.MjSimState(\n            old_state.time, qpos, qvel, old_state.act, old_state.udd_state\n        )\n        self.sim.set_state(new_state)\n        self.sim.forward()\n    \n    @property\n    def dt(self):\n        return self.model.opt.timestep * self.frame_skip\n\n    def do_simulation(self, ctrl, n_frames):\n        self.sim.data.ctrl[:] = ctrl\n        self.sim.step()\n        # for _ in range(n_frames):\n        #     self.sim.step()\n    \n    \n    def render(\n        self,\n        mode=\"human\",\n        width=DEFAULT_SIZE,\n        height=DEFAULT_SIZE,\n        camera_id=None,\n        camera_name=None,\n    ):\n        if mode == \"rgb_array\" or mode == \"depth_array\":\n            if camera_id is not None and camera_name is not None:\n                raise ValueError(\n                    \"Both `camera_id` and `camera_name` cannot be\"\n                    \" specified at the same time.\"\n                )\n\n            no_camera_specified = camera_name is None and camera_id is None\n            if no_camera_specified:\n                camera_name = \"track\"\n\n            if camera_id is None and camera_name in self.model._camera_name2id:\n                camera_id = self.model.camera_name2id(camera_name)\n\n            self._get_viewer(mode).render(width, height, camera_id=camera_id)\n\n        if mode == \"rgb_array\":\n            # window size used for old mujoco-py:\n            data = self._get_viewer(mode).read_pixels(width, height, depth=False)\n            # original image is upside-down, so flip it\n            return data[::-1, :, :]\n        elif mode == \"depth_array\":\n            self._get_viewer(mode).render(width, height)\n            # window size used for old mujoco-py:\n            # Extract depth part of the read_pixels() tuple\n            data = self._get_viewer(mode).read_pixels(width, height, depth=True)[1]\n            # original image is upside-down, so flip it\n            return data[::-1, :]\n        elif mode == \"human\":\n            self._get_viewer(mode).render()\n            \n            \n            \n    def viewer_setup(self):\n        \"\"\"\n        This method is called when the viewer is initialized.\n        Optionally implement this method, if you need to tinker with camera position\n        and so forth.\n        \"\"\"\n        self.viewer.cam.trackbodyid = 1   #id of the body to track()\n        self.viewer.cam.distance = self.model.stat.extent * 1.5 #how much zoom in\n        self.viewer.cam.lookat[0] -= 0 #offset x\n        self.viewer.cam.lookat[1] -= 0 #offset y\n        self.viewer.cam.lookat[2] += 0 #offset z\n        self.viewer.cam.elevation = 0   #cam rotation around the axis in the plane going throug the frame origin\n\n        pass\n\n    def close(self):\n        if self.viewer is not None:\n            # self.viewer.finish()\n            self.viewer = None\n            self._viewers = {}\n\n    def _get_viewer(self, mode):\n        self.viewer = self._viewers.get(mode)\n        if self.viewer is None:\n            if mode == \"human\":\n                self.viewer = mujoco_py.MjViewer(self.sim)\n            elif mode == \"rgb_array\" or mode == \"depth_array\":\n                self.viewer = mujoco_py.MjRenderContextOffscreen(self.sim, -1)\n\n            self.viewer_setup()\n            self._viewers[mode] = self.viewer\n        return self.viewer\n\n    def get_body_com(self, body_name):\n        return self.data.get_body_xpos(body_name)\n\n    def state_vector(self):\n        return np.concatenate([self.sim.data.qpos.flat, self.sim.data.qvel.flat])\n\n    def reward(self):\n        #Reaching Goal Poses (Cost Function)\n        # self.timestep = self.timestep + 1\n        self.reward_arr = self.controller.error()\n        if self.reward_arr[3] == 1:\n            return 21498098\n        reward1 = -self.reward_arr[0] - self.reward_arr[1]\n        alpha = 1\n        reward1 = reward1 * alpha\n\n        #Joint Limit Avoidance (Cost Function)\n        reward2 = 0.0\n        alpha2 = 1.0\n        for i in range(15) :\n            if self._min_joint_position_hat[i] > self.sim_state.qpos[i]:\n                if i == 0:\n                    reward2 = reward2 - abs(self.sim_state.qpos[i] - self._min_joint_position_hat[i]) * alpha2\n                else :\n                    reward2 = reward2 - abs(self.sim_state.qpos[i] - self._min_joint_position_hat[i])\n            if self._max_joint_position_hat[i] < self.sim_state.qpos[i]:\n                if i == 0:\n                    reward2 = reward2 - abs(self._max_joint_position_hat[i] - self.sim_state.qpos[i]) * alpha2\n                else :\n                    reward2 = reward2 - abs(self._max_joint_position_hat[i] - self.sim_state.qpos[i])\n        reward2 = reward2 * alpha2\n        #Avoiding Cartesian Local Minima (Cost Function)\n        # reward3 = self.reward_arr[2]\n        # if reward3 != 0 :\n        #     reward3 = np.sqrt(reward3)            \n        # alpha3 =  -1.0\n        # if reward3 < 0.01 :\n        #     reward3 = 1 - reward3\n        #     reward3 = reward3 * alpha3 \n            # reward3 = reward3 * 5.0\n        # else :\n        #     reward3 = 0.0\n        # print(\"reward1 : %.3f, reward2 : %.3f, reward3 : %.3f\"%(reward1,reward2,reward3))\n        # print(reward1)\n        # print(\"reward2 : %.3f\"%reward2)\n        # print(reward2)\n        # print(\"reward3 : %.3f\"%reward3)\n        # print(reward3)\n        # reward = reward1 + reward2 + reward3 + 2\n        reward = reward1 + reward2 + 1\n        # print(\"reward : %.3f\"%reward)\n        # print(reward)\n        return reward"
        }
    ]
}
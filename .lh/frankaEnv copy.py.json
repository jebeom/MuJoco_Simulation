{
    "sourceFile": "frankaEnv copy.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 0,
            "patches": [
                {
                    "date": 1691650282642,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                }
            ],
            "date": 1691650282642,
            "name": "Commit-0",
            "content": "from tokenize import Double\nimport gymnasium as gym\nimport numpy as np\nfrom gymnasium import spaces\nimport mujoco\n\nfrom mujoco import viewer\nimport controller\n\nDEFAULT_SIZE = 300\nDEG2RAD = 3.14 / 180.0\n\nclass FrankaEnv(gym.Env):\n    def __init__(self):\n        # self.frame_skip = 1\n        self.k = 7  # for jacobian calculation\n        self.model_path = 'model/fr3.xml'\n        self.model = mujoco.MjModel.from_xml_path(self.model_path)\n        self.data = mujoco.MjData(self.model)\n\n        self.viewer = viewer.launch_passive(model=self.model, data=self.data)\n        self.duration = 380  # (seconds)\n        self.framerate = 10  # (Hz)\n\n        self.controller = controller.CController()\n        self.torque = np.zeros(self.k)\n        self.tmp_X = np.zeros(12)\n        self.X_goal = np.zeros(6)\n        self.X = np.zeros(6)\n        self.quaternion_goal = np.zeros(4)\n\n        self.position = np.zeros(6)\n        self.goal = np.zeros(6)\n        # self.reward_arr = np.zeros(2) #shape=(2,), dtype=np.float64\n        self.reward_arr = [0,0,0]\n        self.cnt = 1\n        self.temp_action = np.zeros(1)\n        self._max_joint_position = np.zeros(self.k)\n        self._min_joint_position = np.zeros(self.k)\n        self.timestep = 0\n\n        self._min_joint_position[0] = -2.8973\n        self._min_joint_position[1] = -1.7628\n        self._min_joint_position[2] = -2.8973\n        self._min_joint_position[3] = -3.0718\n        self._min_joint_position[4] = -2.8973\n        self._min_joint_position[5] = -0.0175\n        self._min_joint_position[6] = -2.8973\n\n        self._max_joint_position[0] = 2.8973\n        self._max_joint_position[1] = 1.7628\n        self._max_joint_position[2] = 2.8973\n        self._max_joint_position[3] = -0.0698\n        self._max_joint_position[4] = 2.8973\n        self._max_joint_position[5] = 3.7525\n        self._max_joint_position[6] = 2.8973\n\n        self._max_joint_position_hat = np.zeros(self.k)\n        self._min_joint_position_hat = np.zeros(self.k)\n\n        for i in range(self.k):\n            self._max_joint_position_hat[i] = self._max_joint_position[i] - 0.1*(self._max_joint_position[i] - self._min_joint_position[i])\n            self._min_joint_position_hat[i] = self._min_joint_position[i] + 0.1*(self._max_joint_position[i] - self._min_joint_position[i])    \n        \n        self.observation_space = self._construct_observation_space()\n        self.action_space = self._construct_action_space()\n        self.start_time = 0\n        self.motion_time = 10\n        self.end_time = self.start_time + self.motion_time\n\n\n        for i in range(self.k):\n            self.data.qpos[i] = 0.0\n            self.data.qvel[i] = 0.0\n\n        self.data.qpos[3] = -1.1\n        self.data.qpos[5] = 30.0 * DEG2RAD\n        mujoco.mj_step(self.model, self.data)\n        \n        # self.metadata = {\n        #     \"render.modes\": [\"human\", \"rgb_array\", \"depth_array\"],\n        #     \"video.frames_per_second\": int(np.round(1.0 / self.dt)),\n        #     # \"video.frames_per_second\": int(1000),\n        # }\n        \n        # self.sim_state = self.sim.get_state()\n        \n        \n    def step(self, action, render=True):\n        self.viewer.sync()\n        # print('step')\n        sum_reward = 0\n        done = False\n        self.temp_action = action\n        action = action * 10\n        self.controller.read(self.data.time, self.data.qpos[0:self.k], self.data.qvel[0:self.k], action)\n        self.controller.control_mujoco()\n        self.tmp_X = self.controller.state_controller()\n        self.torque = self.controller.write()\n        for i in range(self.k):\n                self.data.ctrl[i] = self.torque[i]\n\n        for i in range(6):\n            self.X_goal[i] = self.tmp_X[i]\n            # self.data.qpos[0:self.k]\n        for i in range(6):\n            self.X[i] = self.tmp_X[i+6]\n        self.euler_to_quaternion(self.X_goal[3],self.X_goal[4],self.X_goal[5])\n        # print(self.data.qpos)\n        # print(self.data.xpos[13,0])\n        # xpos랑 xmat을 건드려야함\n        for i in range(3):\n            self.data.qpos[9+i] = self.X_goal[i]\n        for i in range(4):\n            self.data.qpos[12+i] = self.quaternion_goal[i]\n        obs =self.get_observation()\n        mujoco.mj_step(self.model, self.data)\n        sum_reward += self.reward()\n        # sum_reward = 0.0\n        # print(self.data.time)\n\n        for i in range(self.k) : \n            if(self.data.qpos[i] > self._max_joint_position[i]) :\n                self.reset()\n                sum_reward -= 1000\n                return obs, sum_reward, done, dict()\n            if(self.data.qpos[i] < self._min_joint_position[i]) :\n                self.reset()\n                sum_reward -= 1000\n                return obs, sum_reward, done, dict()\n\n        if self.data.time > self.end_time:\n            for i in range(6) :\n                if (self.X_goal[i] - self.X[i]) > 0.01:\n                    self.reset()\n                    sum_reward -= 1000\n                    return obs, sum_reward, done, dict()\n                else:\n                    if i == 5 :\n                        sum_reward += 1000\n                        # self.reset()\n                        self.controller.reset_goal()\n                        self.reset_time()\n                        # print(\"!!??\")\n                        # mujoco.mj_step(self.model, self.data)\n                        return obs, sum_reward, done, dict()\n        \n        return obs, sum_reward, done, dict()\n    \n    def euler_to_quaternion(self, roll, pitch, yaw):\n        cy = np.cos(yaw * 0.5)\n        sy = np.sin(yaw * 0.5)\n        cp = np.cos(pitch * 0.5)\n        sp = np.sin(pitch * 0.5)\n        cr = np.cos(roll * 0.5)\n        sr = np.sin(roll * 0.5)\n        \n        w = cr * cp * cy + sr * sp * sy\n        x = sr * cp * cy - cr * sp * sy\n        y = cr * sp * cy + sr * cp * sy\n        z = cr * cp * sy - sr * sp * cy\n        self.quaternion_goal[0] = w\n        self.quaternion_goal[1] = x\n        self.quaternion_goal[2] = y\n        self.quaternion_goal[3] = z\n        # return np.array([w, x, y, z])\n\n    def _construct_action_space(self):\n        # action_low = -1 * np.ones(12)\n        action_low =  np.zeros(1)\n        action_high = np.ones(1) \n        return gym.spaces.Box(low=action_low, high=action_high, dtype=np.float64)\n    \n    \n    def _construct_observation_space(self):\n        obs_low = -1 * np.ones(13) # X(6), X_goal(6), action(1) \n        obs_high = 1 * np.ones(13)\n        # obs_low = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0,0,0, -1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1]\n        # obs_high = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,0,0,0,0,0, 1,1,1,1,1,1,1,1,1,1,1,1]\n        return gym.spaces.Box(obs_low, obs_high, dtype=np.float64)\n    \n     \n    \n    def get_observation(self):\n        # self.sim_state = self.sim.get_state()\n        \n        self.position[0] = self.X[0] / 0.75\n        self.position[1] = self.X[1] / 0.75\n        self.position[2] = self.X[2] / 0.75\n        self.position[3] = self.X[3] / 3.14\n        self.position[4] = self.X[4] / 3.14\n        self.position[5] = self.X[5] / 3.14\n\n        self.goal[0] = self.X_goal[0] / 0.75\n        self.goal[1] = self.X_goal[1] / 0.75\n        self.goal[2] = self.X_goal[2] / 0.75\n        self.goal[3] = self.X_goal[3] / 3.14\n        self.goal[4] = self.X_goal[4] / 3.14\n        self.goal[5] = self.X_goal[5] / 3.14\n        # goal = # controller로부터 받은 goal\n        # self.temp_action = self.temp_action * 10\n        # self.temp_action = self.temp_action - 1\n        return np.concatenate(\n        (\n            self.position, #6\n            self.goal, #6\n            self.temp_action, # 20\n        )\n        )\n        \n    def reset(self, seed=None):\n        self.seedNum = seed\n        # self.qpos = np.zeros(self.k)\n        # qvel = np.zeros(self.k)\n        # self.sim_state.qpos[0:self.k], self.sim_state.qvel[0:self.k]\n        self.controller.initialize()\n        self.end_time = self.data.time + self.motion_time\n\n        for i in range(self.k):\n            self.data.qpos[i] = 0.0\n            self.data.qvel[i] = 0.0\n\n        self.data.qpos[3] = -1.1\n        self.data.qpos[5] = 30.0 * DEG2RAD\n\n        self.timestep = 0\n        # self.temp_action = np.zeros(1)\n\n        mujoco.mj_step(self.model, self.data)\n        \n        return self.get_observation()\n    \n    def reset_time(self):\n        self.end_time = self.data.time + self.motion_time\n    \n    \n    \n    def set_state(self, qpos, qvel):\n        old_state = self.sim.get_state()\n        new_state = mujoco.MjSimState(\n            old_state.time, qpos, qvel, old_state.act, old_state.udd_state\n        )\n        self.sim.set_state(new_state)\n        self.sim.forward()\n    \n    @property\n    def dt(self):\n        return self.model.opt.timestep * self.frame_skip\n\n    # def do_simulation(self, ctrl, n_frames):\n    #     self.sim.data.ctrl[:] = ctrl\n    #     self.sim.step()\n    #     # for _ in range(n_frames):\n    #     #     self.sim.step()\n    \n    \n    def render(\n        self,\n        mode=\"human\",\n        width=DEFAULT_SIZE,\n        height=DEFAULT_SIZE,\n        camera_id=None,\n        camera_name=None,\n    ):\n        if mode == \"rgb_array\" or mode == \"depth_array\":\n            if camera_id is not None and camera_name is not None:\n                raise ValueError(\n                    \"Both `camera_id` and `camera_name` cannot be\"\n                    \" specified at the same time.\"\n                )\n\n            no_camera_specified = camera_name is None and camera_id is None\n            if no_camera_specified:\n                camera_name = \"track\"\n\n            if camera_id is None and camera_name in self.model._camera_name2id:\n                camera_id = self.model.camera_name2id(camera_name)\n\n            self._get_viewer(mode).render(width, height, camera_id=camera_id)\n\n        if mode == \"rgb_array\":\n            # window size used for old mujoco-py:\n            data = self._get_viewer(mode).read_pixels(width, height, depth=False)\n            # original image is upside-down, so flip it\n            return data[::-1, :, :]\n        elif mode == \"depth_array\":\n            self._get_viewer(mode).render(width, height)\n            # window size used for old mujoco-py:\n            # Extract depth part of the read_pixels() tuple\n            data = self._get_viewer(mode).read_pixels(width, height, depth=True)[1]\n            # original image is upside-down, so flip it\n            return data[::-1, :]\n        elif mode == \"human\":\n            self._get_viewer(mode).render()\n            \n            \n            \n    def viewer_setup(self):\n        \"\"\"\n        This method is called when the viewer is initialized.\n        Optionally implement this method, if you need to tinker with camera position\n        and so forth.\n        \"\"\"\n        self.viewer.cam.trackbodyid = 1   #id of the body to track()\n        self.viewer.cam.distance = self.model.stat.extent * 1.5 #how much zoom in\n        self.viewer.cam.lookat[0] -= 0 #offset x\n        self.viewer.cam.lookat[1] -= 0 #offset y\n        self.viewer.cam.lookat[2] += 0 #offset z\n        self.viewer.cam.elevation = 0   #cam rotation around the axis in the plane going throug the frame origin\n\n        pass\n\n    def close(self):\n        if self.viewer is not None:\n            # self.viewer.finish()\n            self.viewer = None\n            self._viewers = {}\n\n    def _get_viewer(self, mode):\n        self.viewer = self._viewers.get(mode)\n        if self.viewer is None:\n            if mode == \"human\":\n                self.viewer = mujoco.MjViewer(self.sim)\n            elif mode == \"rgb_array\" or mode == \"depth_array\":\n                self.viewer = mujoco.MjRenderContextOffscreen(self.sim, -1)\n\n            self.viewer_setup()\n            self._viewers[mode] = self.viewer\n        return self.viewer\n\n    def get_body_com(self, body_name):\n        return self.data.get_body_xpos(body_name)\n\n    def state_vector(self):\n        return np.concatenate([self.sim.data.qpos.flat, self.sim.data.qvel.flat])\n\n    def reward(self):\n        #Reaching Goal Poses (Cost Function)\n        tmp_reward = 0\n        for i in range(6):\n            if self.X_goal[i] != self.X[i]:  # 분모가 0인 경우 처리\n                tmp_reward += 1 / (self.X_goal[i] - self.X[i])\n        reward = tmp_reward * 100\n        return reward\n    \n    def seed(self, seed=None):\n        return "
        }
    ]
}
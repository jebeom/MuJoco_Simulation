{
    "sourceFile": "train.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 191,
            "patches": [
                {
                    "date": 1691627759878,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1691635974050,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -8,8 +8,10 @@\n from stable_baselines3.common.env_util import make_vec_env\n # from stable_baselines3.common.callbacks import EvalCallback\n from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n import frankaEnv\n+\n+\n # device = th.device('cuda:0')\n device = th.device('cuda:0')\n # th.cuda.is_available()\n # Parallel environments\n@@ -22,10 +24,11 @@\n     env = frankaEnv.FrankaEnv()\n \n     n_step = 8000 #1000\n     total_timestep = 8000 * 5000\n-\n+    print(\"1\")\n     model = PPO(\"MlpPolicy\", env, n_steps=n_step, policy_kwargs=policy_kwargs, verbose=1)\n+    print(\"1\")\n     # model.to(device)\n     # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n     # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n     eval_callback = EvalCallback(env, best_model_save_path=\"./logs/\",\n"
                },
                {
                    "date": 1691636074200,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -24,11 +24,11 @@\n     env = frankaEnv.FrankaEnv()\n \n     n_step = 8000 #1000\n     total_timestep = 8000 * 5000\n-    print(\"1\")\n+    # print(\"1\")\n     model = PPO(\"MlpPolicy\", env, n_steps=n_step, policy_kwargs=policy_kwargs, verbose=1)\n-    print(\"1\")\n+    # print(\"2\")\n     # model.to(device)\n     # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n     # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n     eval_callback = EvalCallback(env, best_model_save_path=\"./logs/\",\n"
                },
                {
                    "date": 1691639946925,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -25,9 +25,9 @@\n \n     n_step = 8000 #1000\n     total_timestep = 8000 * 5000\n     # print(\"1\")\n-    model = PPO(\"MlpPolicy\", env, n_steps=n_step, policy_kwargs=policy_kwargs, verbose=1)\n+    model = PPO(\"MlpPolicy\", env, n_steps=n_step, policy_kwargs=policy_kwargs, verbose=0)\n     # print(\"2\")\n     # model.to(device)\n     # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n     # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n"
                },
                {
                    "date": 1691640800371,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -22,10 +22,10 @@\n def main():\n     # Parallel environments\n     env = frankaEnv.FrankaEnv()\n \n-    n_step = 8000 #1000\n-    total_timestep = 8000 * 5000\n+    n_step = 10000 #1000\n+    total_timestep = 10000 * 5000\n     # print(\"1\")\n     model = PPO(\"MlpPolicy\", env, n_steps=n_step, policy_kwargs=policy_kwargs, verbose=0)\n     # print(\"2\")\n     # model.to(device)\n"
                },
                {
                    "date": 1691640900437,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -32,11 +32,11 @@\n     # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n     # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n     eval_callback = EvalCallback(env, best_model_save_path=\"./logs/\",\n                              log_path=\"./logs/\", eval_freq=1,\n-                             deterministic=False, verbose=8000)\n+                             deterministic=False, verbose=0)\n \n-    model.learn(total_timesteps=total_timestep, callback=eval_callback)\n+    model.learn(total_timesteps=total_timestep, callback=eval_callback, verbose=1)\n     model.save(\"ppo_padna_mppi_0810\")\n \n     # del model # remove to demonstrate saving and loading\n \n"
                },
                {
                    "date": 1691640911707,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -34,9 +34,9 @@\n     eval_callback = EvalCallback(env, best_model_save_path=\"./logs/\",\n                              log_path=\"./logs/\", eval_freq=1,\n                              deterministic=False, verbose=0)\n \n-    model.learn(total_timesteps=total_timestep, callback=eval_callback, verbose=1)\n+    model.learn(total_timesteps=total_timestep, callback=eval_callback)\n     model.save(\"ppo_padna_mppi_0810\")\n \n     # del model # remove to demonstrate saving and loading\n \n"
                },
                {
                    "date": 1691643101913,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -25,9 +25,9 @@\n \n     n_step = 10000 #1000\n     total_timestep = 10000 * 5000\n     # print(\"1\")\n-    model = PPO(\"MlpPolicy\", env, n_steps=n_step, policy_kwargs=policy_kwargs, verbose=0)\n+    model = PPO(\"MlpPolicy\", env, n_steps=n_step, policy_kwargs=policy_kwargs, verbose=1)\n     # print(\"2\")\n     # model.to(device)\n     # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n     # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n"
                },
                {
                    "date": 1691643132359,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -25,19 +25,19 @@\n \n     n_step = 10000 #1000\n     total_timestep = 10000 * 5000\n     # print(\"1\")\n-    model = PPO(\"MlpPolicy\", env, n_steps=n_step, policy_kwargs=policy_kwargs, verbose=1)\n+    model = PPO(\"MlpPolicy\", env, n_steps=n_step, policy_kwargs=policy_kwargs, verbose=1).learn(1000)\n     # print(\"2\")\n     # model.to(device)\n     # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n     # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n-    eval_callback = EvalCallback(env, best_model_save_path=\"./logs/\",\n-                             log_path=\"./logs/\", eval_freq=1,\n-                             deterministic=False, verbose=0)\n+    # eval_callback = EvalCallback(env, best_model_save_path=\"./logs/\",\n+    #                          log_path=\"./logs/\", eval_freq=1,\n+    #                          deterministic=False, verbose=0)\n \n-    model.learn(total_timesteps=total_timestep, callback=eval_callback)\n-    model.save(\"ppo_padna_mppi_0810\")\n+    # model.learn(total_timesteps=total_timestep, callback=eval_callback)\n+    # model.save(\"ppo_padna_mppi_0810\")\n \n     # del model # remove to demonstrate saving and loading\n \n     # model = PPO.load(\"./logs/best_model.zip\")\n"
                },
                {
                    "date": 1691643238587,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -25,19 +25,19 @@\n \n     n_step = 10000 #1000\n     total_timestep = 10000 * 5000\n     # print(\"1\")\n-    model = PPO(\"MlpPolicy\", env, n_steps=n_step, policy_kwargs=policy_kwargs, verbose=1).learn(1000)\n+    model = PPO(\"MlpPolicy\", env, n_steps=n_step, policy_kwargs=policy_kwargs, verbose=1)\n     # print(\"2\")\n     # model.to(device)\n     # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n     # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n-    # eval_callback = EvalCallback(env, best_model_save_path=\"./logs/\",\n-    #                          log_path=\"./logs/\", eval_freq=1,\n-    #                          deterministic=False, verbose=0)\n+    eval_callback = EvalCallback(env, best_model_save_path=\"./logs/\",\n+                             log_path=\"./logs/\", eval_freq=1,\n+                             deterministic=False, verbose=0)\n \n-    # model.learn(total_timesteps=total_timestep, callback=eval_callback)\n-    # model.save(\"ppo_padna_mppi_0810\")\n+    model.learn(total_timesteps=total_timestep, callback=eval_callback)\n+    model.save(\"ppo_padna_mppi_0810\")\n \n     # del model # remove to demonstrate saving and loading\n \n     # model = PPO.load(\"./logs/best_model.zip\")\n"
                },
                {
                    "date": 1691643328051,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -25,9 +25,9 @@\n \n     n_step = 10000 #1000\n     total_timestep = 10000 * 5000\n     # print(\"1\")\n-    model = PPO(\"MlpPolicy\", env, n_steps=n_step, policy_kwargs=policy_kwargs, verbose=1)\n+    model = PPO(\"MlpPolicy\", env, n_steps=n_step, n_envs = 10, batch_size = 200000, policy_kwargs=policy_kwargs, verbose=1)\n     # print(\"2\")\n     # model.to(device)\n     # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n     # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n"
                },
                {
                    "date": 1691643339934,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -25,9 +25,9 @@\n \n     n_step = 10000 #1000\n     total_timestep = 10000 * 5000\n     # print(\"1\")\n-    model = PPO(\"MlpPolicy\", env, n_steps=n_step, n_envs = 10, batch_size = 200000, policy_kwargs=policy_kwargs, verbose=1)\n+    model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = 100000, policy_kwargs=policy_kwargs, verbose=1)\n     # print(\"2\")\n     # model.to(device)\n     # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n     # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n"
                },
                {
                    "date": 1691643494415,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -25,9 +25,10 @@\n \n     n_step = 10000 #1000\n     total_timestep = 10000 * 5000\n     # print(\"1\")\n-    model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = 100000, policy_kwargs=policy_kwargs, verbose=1)\n+    # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = 100000, policy_kwargs=policy_kwargs, verbose=1)\n+    model = PPO(\"MlpPolicy\", env, verbose=1)\n     # print(\"2\")\n     # model.to(device)\n     # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n     # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n"
                },
                {
                    "date": 1691643709785,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -21,8 +21,9 @@\n     \n def main():\n     # Parallel environments\n     env = frankaEnv.FrankaEnv()\n+    env = Monitor(env, log_dir)\n \n     n_step = 10000 #1000\n     total_timestep = 10000 * 5000\n     # print(\"1\")\n"
                },
                {
                    "date": 1691643800514,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -2,8 +2,9 @@\n import gym\n import os\n import torch as th\n import numpy as np\n+from gym import monitor \n \n from stable_baselines3 import PPO\n from stable_baselines3.common.env_util import make_vec_env\n # from stable_baselines3.common.callbacks import EvalCallback\n"
                },
                {
                    "date": 1691643805664,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -2,9 +2,9 @@\n import gym\n import os\n import torch as th\n import numpy as np\n-from gym import monitor \n+from gym import Monitor \n \n from stable_baselines3 import PPO\n from stable_baselines3.common.env_util import make_vec_env\n # from stable_baselines3.common.callbacks import EvalCallback\n"
                },
                {
                    "date": 1691643814351,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -2,9 +2,9 @@\n import gym\n import os\n import torch as th\n import numpy as np\n-from gym import Monitor \n+# from gym import Monitor \n \n from stable_baselines3 import PPO\n from stable_baselines3.common.env_util import make_vec_env\n # from stable_baselines3.common.callbacks import EvalCallback\n"
                },
                {
                    "date": 1691643861722,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -2,9 +2,9 @@\n import gym\n import os\n import torch as th\n import numpy as np\n-# from gym import Monitor \n+from stable_baselines3.common.vec_env import VecMonitor\n \n from stable_baselines3 import PPO\n from stable_baselines3.common.env_util import make_vec_env\n # from stable_baselines3.common.callbacks import EvalCallback\n"
                },
                {
                    "date": 1691643868860,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -22,9 +22,9 @@\n     \n def main():\n     # Parallel environments\n     env = frankaEnv.FrankaEnv()\n-    env = Monitor(env, log_dir)\n+    env = VecMonitor(env)\n \n     n_step = 10000 #1000\n     total_timestep = 10000 * 5000\n     # print(\"1\")\n"
                },
                {
                    "date": 1691643874607,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -22,9 +22,9 @@\n     \n def main():\n     # Parallel environments\n     env = frankaEnv.FrankaEnv()\n-    env = VecMonitor(env)\n+    env = VecMonitor(env, log_dir)\n \n     n_step = 10000 #1000\n     total_timestep = 10000 * 5000\n     # print(\"1\")\n"
                },
                {
                    "date": 1691643938030,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -21,8 +21,9 @@\n # policy_kwargs = dict(activation_fn=th.nn.Tanh,net_arch=[128, 128])\n     \n def main():\n     # Parallel environments\n+    log_dir = \"./log\"\n     env = frankaEnv.FrankaEnv()\n     env = VecMonitor(env, log_dir)\n \n     n_step = 10000 #1000\n"
                },
                {
                    "date": 1691643959301,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -2,9 +2,9 @@\n import gym\n import os\n import torch as th\n import numpy as np\n-from stable_baselines3.common.vec_env import VecMonitor\n+from stable_baselines.bench import Monitor\n \n from stable_baselines3 import PPO\n from stable_baselines3.common.env_util import make_vec_env\n # from stable_baselines3.common.callbacks import EvalCallback\n"
                },
                {
                    "date": 1691643969778,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -2,9 +2,9 @@\n import gym\n import os\n import torch as th\n import numpy as np\n-from stable_baselines.bench import Monitor\n+from stable_baselines3.bench import Monitor\n \n from stable_baselines3 import PPO\n from stable_baselines3.common.env_util import make_vec_env\n # from stable_baselines3.common.callbacks import EvalCallback\n@@ -23,9 +23,9 @@\n def main():\n     # Parallel environments\n     log_dir = \"./log\"\n     env = frankaEnv.FrankaEnv()\n-    env = VecMonitor(env, log_dir)\n+    env = Monitor(env, log_dir)\n \n     n_step = 10000 #1000\n     total_timestep = 10000 * 5000\n     # print(\"1\")\n"
                },
                {
                    "date": 1691643975199,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -2,9 +2,9 @@\n import gym\n import os\n import torch as th\n import numpy as np\n-from stable_baselines3.bench import Monitor\n+from stable_baselines3 import Monitor\n \n from stable_baselines3 import PPO\n from stable_baselines3.common.env_util import make_vec_env\n # from stable_baselines3.common.callbacks import EvalCallback\n"
                },
                {
                    "date": 1691643992214,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -2,9 +2,9 @@\n import gym\n import os\n import torch as th\n import numpy as np\n-from stable_baselines3 import Monitor\n+from stable_baselines3.common.monitor import Monitor\n \n from stable_baselines3 import PPO\n from stable_baselines3.common.env_util import make_vec_env\n # from stable_baselines3.common.callbacks import EvalCallback\n"
                },
                {
                    "date": 1691644277160,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -35,9 +35,9 @@\n     # model.to(device)\n     # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n     # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n     eval_callback = EvalCallback(env, best_model_save_path=\"./logs/\",\n-                             log_path=\"./logs/\", eval_freq=1,\n+                             log_path=log_dir, eval_freq=1,\n                              deterministic=False, verbose=0)\n \n     model.learn(total_timesteps=total_timestep, callback=eval_callback)\n     model.save(\"ppo_padna_mppi_0810\")\n"
                },
                {
                    "date": 1691644283711,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -34,9 +34,9 @@\n     # print(\"2\")\n     # model.to(device)\n     # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n     # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n-    eval_callback = EvalCallback(env, best_model_save_path=\"./logs/\",\n+    eval_callback = EvalCallback(env, best_model_save_path=log_dir,\n                              log_path=log_dir, eval_freq=1,\n                              deterministic=False, verbose=0)\n \n     model.learn(total_timesteps=total_timestep, callback=eval_callback)\n"
                },
                {
                    "date": 1691644296613,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -35,10 +35,9 @@\n     # model.to(device)\n     # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n     # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n     eval_callback = EvalCallback(env, best_model_save_path=log_dir,\n-                             log_path=log_dir, eval_freq=1,\n-                             deterministic=False, verbose=0)\n+                             log_path=log_dir,deterministic=False, verbose=1)\n \n     model.learn(total_timesteps=total_timestep, callback=eval_callback)\n     model.save(\"ppo_padna_mppi_0810\")\n \n"
                },
                {
                    "date": 1691644310964,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -35,9 +35,10 @@\n     # model.to(device)\n     # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n     # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n     eval_callback = EvalCallback(env, best_model_save_path=log_dir,\n-                             log_path=log_dir,deterministic=False, verbose=1)\n+                             log_path=log_dir, eval_freq=1,\n+                             deterministic=False, verbose=0)\n \n     model.learn(total_timesteps=total_timestep, callback=eval_callback)\n     model.save(\"ppo_padna_mppi_0810\")\n \n"
                },
                {
                    "date": 1691644359023,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -29,9 +29,9 @@\n     n_step = 10000 #1000\n     total_timestep = 10000 * 5000\n     # print(\"1\")\n     # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = 100000, policy_kwargs=policy_kwargs, verbose=1)\n-    model = PPO(\"MlpPolicy\", env, verbose=1)\n+    model = DQN(\"MlpPolicy\", env, verbose=1)\n     # print(\"2\")\n     # model.to(device)\n     # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n     # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n"
                },
                {
                    "date": 1691644366380,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -4,9 +4,9 @@\n import torch as th\n import numpy as np\n from stable_baselines3.common.monitor import Monitor\n \n-from stable_baselines3 import PPO\n+from stable_baselines3 import DQN\n from stable_baselines3.common.env_util import make_vec_env\n # from stable_baselines3.common.callbacks import EvalCallback\n from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n import frankaEnv\n"
                },
                {
                    "date": 1691644375542,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -4,9 +4,9 @@\n import torch as th\n import numpy as np\n from stable_baselines3.common.monitor import Monitor\n \n-from stable_baselines3 import DQN\n+from stable_baselines3 import DDPG\n from stable_baselines3.common.env_util import make_vec_env\n # from stable_baselines3.common.callbacks import EvalCallback\n from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n import frankaEnv\n@@ -29,9 +29,9 @@\n     n_step = 10000 #1000\n     total_timestep = 10000 * 5000\n     # print(\"1\")\n     # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = 100000, policy_kwargs=policy_kwargs, verbose=1)\n-    model = DQN(\"MlpPolicy\", env, verbose=1)\n+    model = DDPG(\"MlpPolicy\", env, verbose=1)\n     # print(\"2\")\n     # model.to(device)\n     # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n     # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n"
                },
                {
                    "date": 1691644393488,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -4,9 +4,9 @@\n import torch as th\n import numpy as np\n from stable_baselines3.common.monitor import Monitor\n \n-from stable_baselines3 import DDPG\n+from stable_baselines3 import PPO\n from stable_baselines3.common.env_util import make_vec_env\n # from stable_baselines3.common.callbacks import EvalCallback\n from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n import frankaEnv\n@@ -29,14 +29,14 @@\n     n_step = 10000 #1000\n     total_timestep = 10000 * 5000\n     # print(\"1\")\n     # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = 100000, policy_kwargs=policy_kwargs, verbose=1)\n-    model = DDPG(\"MlpPolicy\", env, verbose=1)\n+    model = PPO(\"MlpPolicy\", env, verbose=1)\n     # print(\"2\")\n     # model.to(device)\n     # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n     # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n-    eval_callback = EvalCallback(env, best_model_save_path=log_dir,\n+    eval_callback = EvalCallback(env, best_model_save_path=\"./logs/\",\n                              log_path=log_dir, eval_freq=1,\n                              deterministic=False, verbose=0)\n \n     model.learn(total_timesteps=total_timestep, callback=eval_callback)\n"
                },
                {
                    "date": 1691644584725,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -3,8 +3,9 @@\n import os\n import torch as th\n import numpy as np\n from stable_baselines3.common.monitor import Monitor\n+from gym import spaces\n \n from stable_baselines3 import PPO\n from stable_baselines3.common.env_util import make_vec_env\n # from stable_baselines3.common.callbacks import EvalCallback\n"
                },
                {
                    "date": 1691644590957,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -3,9 +3,8 @@\n import os\n import torch as th\n import numpy as np\n from stable_baselines3.common.monitor import Monitor\n-from gym import spaces\n \n from stable_baselines3 import PPO\n from stable_baselines3.common.env_util import make_vec_env\n # from stable_baselines3.common.callbacks import EvalCallback\n"
                },
                {
                    "date": 1691644712090,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,6 +1,6 @@\n #!/usr/bin/env python3\n-import gym\n+import gymnasium as gym\n import os\n import torch as th\n import numpy as np\n from stable_baselines3.common.monitor import Monitor\n"
                },
                {
                    "date": 1691644753968,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -29,9 +29,9 @@\n     n_step = 10000 #1000\n     total_timestep = 10000 * 5000\n     # print(\"1\")\n     # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = 100000, policy_kwargs=policy_kwargs, verbose=1)\n-    model = PPO(\"MlpPolicy\", env, verbose=1)\n+    model = PPO(\"MlpPolicy\", env, policy_kwargs=policy_kwargs, verbose=1)\n     # print(\"2\")\n     # model.to(device)\n     # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n     # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n"
                },
                {
                    "date": 1691644773460,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -29,9 +29,9 @@\n     n_step = 10000 #1000\n     total_timestep = 10000 * 5000\n     # print(\"1\")\n     # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = 100000, policy_kwargs=policy_kwargs, verbose=1)\n-    model = PPO(\"MlpPolicy\", env, policy_kwargs=policy_kwargs, verbose=1)\n+    model = PPO(\"MlpPolicy\", env, verbose=1)\n     # print(\"2\")\n     # model.to(device)\n     # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n     # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n"
                },
                {
                    "date": 1691644857233,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -28,10 +28,10 @@\n \n     n_step = 10000 #1000\n     total_timestep = 10000 * 5000\n     # print(\"1\")\n-    # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = 100000, policy_kwargs=policy_kwargs, verbose=1)\n-    model = PPO(\"MlpPolicy\", env, verbose=1)\n+    model = PPO(\"MlpPolicy\", env, n_steps=n_step, policy_kwargs=policy_kwargs, verbose=1)\n+    # model = PPO(\"MlpPolicy\", env, verbose=1)\n     # print(\"2\")\n     # model.to(device)\n     # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n     # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n"
                },
                {
                    "date": 1691644904489,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -16,10 +16,10 @@\n device = th.device('cuda:0')\n # th.cuda.is_available()\n # Parallel environments\n     \n-policy_kwargs = dict(activation_fn=th.nn.Tanh,net_arch=[128, dict(pi=[64,64], vf=[64,64])])\n-# policy_kwargs = dict(activation_fn=th.nn.Tanh,net_arch=[128, 128])\n+# policy_kwargs = dict(activation_fn=th.nn.Tanh,net_arch=[128, dict(pi=[64,64], vf=[64,64])])\n+policy_kwargs = dict(activation_fn=th.nn.Tanh,net_arch=[128, 128])\n     \n def main():\n     # Parallel environments\n     log_dir = \"./log\"\n"
                },
                {
                    "date": 1691644981257,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -24,8 +24,9 @@\n     # Parallel environments\n     log_dir = \"./log\"\n     env = frankaEnv.FrankaEnv()\n     env = Monitor(env, log_dir)\n+    env.seed(1234)\n \n     n_step = 10000 #1000\n     total_timestep = 10000 * 5000\n     # print(\"1\")\n"
                },
                {
                    "date": 1691644989554,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -24,9 +24,8 @@\n     # Parallel environments\n     log_dir = \"./log\"\n     env = frankaEnv.FrankaEnv()\n     env = Monitor(env, log_dir)\n-    env.seed(1234)\n \n     n_step = 10000 #1000\n     total_timestep = 10000 * 5000\n     # print(\"1\")\n"
                },
                {
                    "date": 1691645257500,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -38,9 +38,9 @@\n     eval_callback = EvalCallback(env, best_model_save_path=\"./logs/\",\n                              log_path=log_dir, eval_freq=1,\n                              deterministic=False, verbose=0)\n \n-    model.learn(total_timesteps=total_timestep, callback=eval_callback)\n+    model.learn(seed=none, total_timesteps=total_timestep, callback=eval_callback)\n     model.save(\"ppo_padna_mppi_0810\")\n \n     # del model # remove to demonstrate saving and loading\n \n"
                },
                {
                    "date": 1691645264646,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -38,9 +38,9 @@\n     eval_callback = EvalCallback(env, best_model_save_path=\"./logs/\",\n                              log_path=log_dir, eval_freq=1,\n                              deterministic=False, verbose=0)\n \n-    model.learn(seed=none, total_timesteps=total_timestep, callback=eval_callback)\n+    model.learn(seed=1234, total_timesteps=total_timestep, callback=eval_callback)\n     model.save(\"ppo_padna_mppi_0810\")\n \n     # del model # remove to demonstrate saving and loading\n \n"
                },
                {
                    "date": 1691645275408,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -38,9 +38,9 @@\n     eval_callback = EvalCallback(env, best_model_save_path=\"./logs/\",\n                              log_path=log_dir, eval_freq=1,\n                              deterministic=False, verbose=0)\n \n-    model.learn(seed=1234, total_timesteps=total_timestep, callback=eval_callback)\n+    model.learn(total_timesteps=total_timestep, callback=eval_callback)\n     model.save(\"ppo_padna_mppi_0810\")\n \n     # del model # remove to demonstrate saving and loading\n \n"
                },
                {
                    "date": 1691645395267,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -22,9 +22,9 @@\n     \n def main():\n     # Parallel environments\n     log_dir = \"./log\"\n-    env = frankaEnv.FrankaEnv()\n+    env = frankaEnv.FrankaEnv(seed=0)\n     env = Monitor(env, log_dir)\n \n     n_step = 10000 #1000\n     total_timestep = 10000 * 5000\n"
                },
                {
                    "date": 1691645405187,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -22,9 +22,9 @@\n     \n def main():\n     # Parallel environments\n     log_dir = \"./log\"\n-    env = frankaEnv.FrankaEnv(seed=0)\n+    env = frankaEnv.FrankaEnv()\n     env = Monitor(env, log_dir)\n \n     n_step = 10000 #1000\n     total_timestep = 10000 * 5000\n"
                },
                {
                    "date": 1691646298572,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -23,9 +23,9 @@\n def main():\n     # Parallel environments\n     log_dir = \"./log\"\n     env = frankaEnv.FrankaEnv()\n-    env = Monitor(env, log_dir)\n+    # env = Monitor(env, log_dir)\n \n     n_step = 10000 #1000\n     total_timestep = 10000 * 5000\n     # print(\"1\")\n"
                },
                {
                    "date": 1691646309939,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -2,12 +2,12 @@\n import gymnasium as gym\n import os\n import torch as th\n import numpy as np\n-from stable_baselines3.common.monitor import Monitor\n+# from stable_baselines3.common.monitor import Monitor\n \n from stable_baselines3 import PPO\n-from stable_baselines3.common.env_util import make_vec_env\n+# from stable_baselines3.common.env_util import make_vec_env\n # from stable_baselines3.common.callbacks import EvalCallback\n from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n import frankaEnv\n \n"
                },
                {
                    "date": 1691646354923,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -28,10 +28,10 @@\n \n     n_step = 10000 #1000\n     total_timestep = 10000 * 5000\n     # print(\"1\")\n-    model = PPO(\"MlpPolicy\", env, n_steps=n_step, policy_kwargs=policy_kwargs, verbose=1)\n-    # model = PPO(\"MlpPolicy\", env, verbose=1)\n+    # model = PPO(\"MlpPolicy\", env, n_steps=n_step, policy_kwargs=policy_kwargs, verbose=1)\n+    model = PPO(\"MlpPolicy\", env, verbose=1)\n     # print(\"2\")\n     # model.to(device)\n     # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n     # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n"
                },
                {
                    "date": 1691647982030,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -23,8 +23,9 @@\n def main():\n     # Parallel environments\n     log_dir = \"./log\"\n     env = frankaEnv.FrankaEnv()\n+    env.reset()\n     # env = Monitor(env, log_dir)\n \n     n_step = 10000 #1000\n     total_timestep = 10000 * 5000\n"
                },
                {
                    "date": 1691647995650,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -23,9 +23,9 @@\n def main():\n     # Parallel environments\n     log_dir = \"./log\"\n     env = frankaEnv.FrankaEnv()\n-    env.reset()\n+    # env.reset()\n     # env = Monitor(env, log_dir)\n \n     n_step = 10000 #1000\n     total_timestep = 10000 * 5000\n"
                },
                {
                    "date": 1691648032581,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -39,9 +39,9 @@\n     eval_callback = EvalCallback(env, best_model_save_path=\"./logs/\",\n                              log_path=log_dir, eval_freq=1,\n                              deterministic=False, verbose=0)\n \n-    model.learn(total_timesteps=total_timestep, callback=eval_callback)\n+    model.learn(total_timesteps=total_timestep)\n     model.save(\"ppo_padna_mppi_0810\")\n \n     # del model # remove to demonstrate saving and loading\n \n"
                },
                {
                    "date": 1691648054618,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -8,8 +8,9 @@\n from stable_baselines3 import PPO\n # from stable_baselines3.common.env_util import make_vec_env\n # from stable_baselines3.common.callbacks import EvalCallback\n from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n+from stable_baselines3.common.env_util import make_vec_env\n import frankaEnv\n \n \n # device = th.device('cuda:0')\n"
                },
                {
                    "date": 1691648287852,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -24,16 +24,19 @@\n def main():\n     # Parallel environments\n     log_dir = \"./log\"\n     env = frankaEnv.FrankaEnv()\n+    # vec_env = make_vec_env(\"CartPole-v1\", n_envs=4)\n     # env.reset()\n     # env = Monitor(env, log_dir)\n \n     n_step = 10000 #1000\n     total_timestep = 10000 * 5000\n     # print(\"1\")\n     # model = PPO(\"MlpPolicy\", env, n_steps=n_step, policy_kwargs=policy_kwargs, verbose=1)\n     model = PPO(\"MlpPolicy\", env, verbose=1)\n+    vec_env = model.get_env()\n+\n     # print(\"2\")\n     # model.to(device)\n     # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n     # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n"
                },
                {
                    "date": 1691648295519,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -33,9 +33,9 @@\n     total_timestep = 10000 * 5000\n     # print(\"1\")\n     # model = PPO(\"MlpPolicy\", env, n_steps=n_step, policy_kwargs=policy_kwargs, verbose=1)\n     model = PPO(\"MlpPolicy\", env, verbose=1)\n-    vec_env = model.get_env()\n+    vec_env = model.env()\n \n     # print(\"2\")\n     # model.to(device)\n     # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
                },
                {
                    "date": 1691648307867,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -33,9 +33,8 @@\n     total_timestep = 10000 * 5000\n     # print(\"1\")\n     # model = PPO(\"MlpPolicy\", env, n_steps=n_step, policy_kwargs=policy_kwargs, verbose=1)\n     model = PPO(\"MlpPolicy\", env, verbose=1)\n-    vec_env = model.env()\n \n     # print(\"2\")\n     # model.to(device)\n     # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
                },
                {
                    "date": 1691648332803,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -24,9 +24,9 @@\n def main():\n     # Parallel environments\n     log_dir = \"./log\"\n     env = frankaEnv.FrankaEnv()\n-    # vec_env = make_vec_env(\"CartPole-v1\", n_envs=4)\n+    vec_env = make_vec_env(env, n_envs=4)\n     # env.reset()\n     # env = Monitor(env, log_dir)\n \n     n_step = 10000 #1000\n"
                },
                {
                    "date": 1691648344256,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -32,17 +32,17 @@\n     n_step = 10000 #1000\n     total_timestep = 10000 * 5000\n     # print(\"1\")\n     # model = PPO(\"MlpPolicy\", env, n_steps=n_step, policy_kwargs=policy_kwargs, verbose=1)\n-    model = PPO(\"MlpPolicy\", env, verbose=1)\n+    model = PPO(\"MlpPolicy\", vec_env, verbose=1)\n \n     # print(\"2\")\n     # model.to(device)\n     # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n     # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n-    eval_callback = EvalCallback(env, best_model_save_path=\"./logs/\",\n-                             log_path=log_dir, eval_freq=1,\n-                             deterministic=False, verbose=0)\n+    # eval_callback = EvalCallback(vec_env, best_model_save_path=\"./logs/\",\n+    #                          log_path=log_dir, eval_freq=1,\n+    #                          deterministic=False, verbose=0)\n \n     model.learn(total_timesteps=total_timestep)\n     model.save(\"ppo_padna_mppi_0810\")\n \n"
                },
                {
                    "date": 1691648373715,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,50 +1,45 @@\n #!/usr/bin/env python3\n-import gymnasium as gym\n+import gym\n import os\n import torch as th\n import numpy as np\n-# from stable_baselines3.common.monitor import Monitor\n+# from gtimport monitor \n \n from stable_baselines3 import PPO\n-# from stable_baselines3.common.env_util import make_vec_env\n+from stable_baselines3.common.env_util import make_vec_env\n # from stable_baselines3.common.callbacks import EvalCallback\n from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n-from stable_baselines3.common.env_util import make_vec_env\n import frankaEnv\n \n \n # device = th.device('cuda:0')\n device = th.device('cuda:0')\n # th.cuda.is_available()\n # Parallel environments\n     \n-# policy_kwargs = dict(activation_fn=th.nn.Tanh,net_arch=[128, dict(pi=[64,64], vf=[64,64])])\n-policy_kwargs = dict(activation_fn=th.nn.Tanh,net_arch=[128, 128])\n+policy_kwargs = dict(activation_fn=th.nn.Tanh,net_arch=[128, dict(pi=[64,64], vf=[64,64])])\n+# policy_kwargs = dict(activation_fn=th.nn.Tanh,net_arch=[128, 128])\n     \n def main():\n     # Parallel environments\n-    log_dir = \"./log\"\n     env = frankaEnv.FrankaEnv()\n-    vec_env = make_vec_env(env, n_envs=4)\n-    # env.reset()\n-    # env = Monitor(env, log_dir)\n+    env = Monitor(env, log_dir)\n \n     n_step = 10000 #1000\n     total_timestep = 10000 * 5000\n     # print(\"1\")\n-    # model = PPO(\"MlpPolicy\", env, n_steps=n_step, policy_kwargs=policy_kwargs, verbose=1)\n-    model = PPO(\"MlpPolicy\", vec_env, verbose=1)\n-\n+    # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = 100000, policy_kwargs=policy_kwargs, verbose=1)\n+    model = PPO(\"MlpPolicy\", env, verbose=1)\n     # print(\"2\")\n     # model.to(device)\n     # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n     # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n-    # eval_callback = EvalCallback(vec_env, best_model_save_path=\"./logs/\",\n-    #                          log_path=log_dir, eval_freq=1,\n-    #                          deterministic=False, verbose=0)\n+    eval_callback = EvalCallback(env, best_model_save_path=\"./logs/\",\n+                             log_path=\"./logs/\", eval_freq=1,\n+                             deterministic=False, verbose=0)\n \n-    model.learn(total_timesteps=total_timestep)\n+    model.learn(total_timesteps=total_timestep, callback=eval_callback)\n     model.save(\"ppo_padna_mppi_0810\")\n \n     # del model # remove to demonstrate saving and loading\n \n"
                },
                {
                    "date": 1691648506358,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -5,9 +5,9 @@\n import numpy as np\n # from gtimport monitor \n \n from stable_baselines3 import PPO\n-from stable_baselines3.common.env_util import make_vec_env\n+# from stable_baselines3.common.env_util import make_vec_env\n # from stable_baselines3.common.callbacks import EvalCallback\n from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n import frankaEnv\n \n@@ -22,9 +22,9 @@\n     \n def main():\n     # Parallel environments\n     env = frankaEnv.FrankaEnv()\n-    env = Monitor(env, log_dir)\n+    # env = Monitor(env, log_dir)\n \n     n_step = 10000 #1000\n     total_timestep = 10000 * 5000\n     # print(\"1\")\n"
                },
                {
                    "date": 1691648521228,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,6 +1,6 @@\n #!/usr/bin/env python3\n-import gym\n+import gymnasium as gym\n import os\n import torch as th\n import numpy as np\n # from gtimport monitor \n"
                },
                {
                    "date": 1691649559551,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -28,9 +28,9 @@\n     n_step = 10000 #1000\n     total_timestep = 10000 * 5000\n     # print(\"1\")\n     # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = 100000, policy_kwargs=policy_kwargs, verbose=1)\n-    model = PPO(\"MlpPolicy\", env, verbose=1)\n+    model = PPO(\"MlpPolicy\", env, num_envs=1, verbose=1)\n     # print(\"2\")\n     # model.to(device)\n     # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n     # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n"
                },
                {
                    "date": 1691649639956,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -28,9 +28,9 @@\n     n_step = 10000 #1000\n     total_timestep = 10000 * 5000\n     # print(\"1\")\n     # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = 100000, policy_kwargs=policy_kwargs, verbose=1)\n-    model = PPO(\"MlpPolicy\", env, num_envs=1, verbose=1)\n+    model = PPO(\"MlpPolicy\", env, verbose=1)\n     # print(\"2\")\n     # model.to(device)\n     # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n     # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n"
                },
                {
                    "date": 1691649747409,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -22,9 +22,9 @@\n     \n def main():\n     # Parallel environments\n     env = frankaEnv.FrankaEnv()\n-    # env = Monitor(env, log_dir)\n+    env = Monitor(env, log_dir)\n \n     n_step = 10000 #1000\n     total_timestep = 10000 * 5000\n     # print(\"1\")\n"
                },
                {
                    "date": 1691649759274,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -2,9 +2,9 @@\n import gymnasium as gym\n import os\n import torch as th\n import numpy as np\n-# from gtimport monitor \n+from gtimport monitor \n \n from stable_baselines3 import PPO\n # from stable_baselines3.common.env_util import make_vec_env\n # from stable_baselines3.common.callbacks import EvalCallback\n"
                },
                {
                    "date": 1691649786463,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -2,9 +2,9 @@\n import gymnasium as gym\n import os\n import torch as th\n import numpy as np\n-from gtimport monitor \n+from gymnasium import monitor \n \n from stable_baselines3 import PPO\n # from stable_baselines3.common.env_util import make_vec_env\n # from stable_baselines3.common.callbacks import EvalCallback\n"
                },
                {
                    "date": 1691649795793,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -2,9 +2,9 @@\n import gymnasium as gym\n import os\n import torch as th\n import numpy as np\n-from gymnasium import monitor \n+from gym import Monitor \n \n from stable_baselines3 import PPO\n # from stable_baselines3.common.env_util import make_vec_env\n # from stable_baselines3.common.callbacks import EvalCallback\n"
                },
                {
                    "date": 1691649803983,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -2,9 +2,9 @@\n import gymnasium as gym\n import os\n import torch as th\n import numpy as np\n-from gym import Monitor \n+from stable_baselines3 import Monitor \n \n from stable_baselines3 import PPO\n # from stable_baselines3.common.env_util import make_vec_env\n # from stable_baselines3.common.callbacks import EvalCallback\n"
                },
                {
                    "date": 1691649820345,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -2,9 +2,9 @@\n import gymnasium as gym\n import os\n import torch as th\n import numpy as np\n-from stable_baselines3 import Monitor \n+from stable_baselines3.common.monitor import Monitor \n \n from stable_baselines3 import PPO\n # from stable_baselines3.common.env_util import make_vec_env\n # from stable_baselines3.common.callbacks import EvalCallback\n"
                },
                {
                    "date": 1691649833581,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -21,8 +21,9 @@\n # policy_kwargs = dict(activation_fn=th.nn.Tanh,net_arch=[128, 128])\n     \n def main():\n     # Parallel environments\n+    log_dir=\"./log\"\n     env = frankaEnv.FrankaEnv()\n     env = Monitor(env, log_dir)\n \n     n_step = 10000 #1000\n"
                },
                {
                    "date": 1691649938184,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -24,8 +24,10 @@\n     # Parallel environments\n     log_dir=\"./log\"\n     env = frankaEnv.FrankaEnv()\n     env = Monitor(env, log_dir)\n+    pendulum_env = RGBArrayAsObservationWrapper(pendulum_env)\n+    check_env(pendulum_env, warn=True)\n \n     n_step = 10000 #1000\n     total_timestep = 10000 * 5000\n     # print(\"1\")\n"
                },
                {
                    "date": 1691649964815,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -18,16 +18,47 @@\n # Parallel environments\n     \n policy_kwargs = dict(activation_fn=th.nn.Tanh,net_arch=[128, dict(pi=[64,64], vf=[64,64])])\n # policy_kwargs = dict(activation_fn=th.nn.Tanh,net_arch=[128, 128])\n+class RGBArrayAsObservationWrapper(Wrapper):\n+    \"\"\"\n+    Use env.render(rgb_array) as observation\n+    rather than the observation environment provides\n+    \"\"\"\n+\n+    def __init__(self, env):\n+        # TODO this might not work before environment has been reset\n+        super(RGBArrayAsObservationWrapper, self).__init__(env)\n+        self.reset()\n+        dummy_obs = env.render('rgb_array')\n+        dummy_obs_resized = resize(dummy_obs, (dummy_obs.shape[0] // 10, dummy_obs.shape[1] // 10),\n+                                   anti_aliasing=True)\n+        # Update observation space\n+        # TODO assign correct low and high\n+        self.observation_space = spaces.Box(low=0, high=255, shape=dummy_obs_resized.shape,\n+                                            dtype=dummy_obs_resized.dtype)\n+\n+    def reset(self, **kwargs):\n+        obs = self.env.reset(**kwargs)\n+        obs = self.env.render(\"rgb_array\")\n+        obs = resize(obs, (obs.shape[0] // 10, obs.shape[1] // 10),\n+                     anti_aliasing=True)\n+        return obs\n+\n+    def step(self, action):\n+        obs, reward, done, info = self.env.step(action)\n+        obs = self.env.render(\"rgb_array\")\n+        obs = resize(obs, (obs.shape[0] // 10, obs.shape[1] // 10),\n+                     anti_aliasing=True)\n+        return obs, reward, done, info\n     \n def main():\n     # Parallel environments\n     log_dir=\"./log\"\n     env = frankaEnv.FrankaEnv()\n     env = Monitor(env, log_dir)\n-    pendulum_env = RGBArrayAsObservationWrapper(pendulum_env)\n-    check_env(pendulum_env, warn=True)\n+    env = RGBArrayAsObservationWrapper(env)\n+    check_env(env, warn=True)\n \n     n_step = 10000 #1000\n     total_timestep = 10000 * 5000\n     # print(\"1\")\n"
                },
                {
                    "date": 1691649982055,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -9,10 +9,10 @@\n # from stable_baselines3.common.env_util import make_vec_env\n # from stable_baselines3.common.callbacks import EvalCallback\n from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n import frankaEnv\n+from gym import Wrapper, spaces\n \n-\n # device = th.device('cuda:0')\n device = th.device('cuda:0')\n # th.cuda.is_available()\n # Parallel environments\n"
                },
                {
                    "date": 1691650001172,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -28,9 +28,9 @@\n     def __init__(self, env):\n         # TODO this might not work before environment has been reset\n         super(RGBArrayAsObservationWrapper, self).__init__(env)\n         self.reset()\n-        dummy_obs = env.render('rgb_array')\n+        dummy_obs = env.render()\n         dummy_obs_resized = resize(dummy_obs, (dummy_obs.shape[0] // 10, dummy_obs.shape[1] // 10),\n                                    anti_aliasing=True)\n         # Update observation space\n         # TODO assign correct low and high\n@@ -38,16 +38,16 @@\n                                             dtype=dummy_obs_resized.dtype)\n \n     def reset(self, **kwargs):\n         obs = self.env.reset(**kwargs)\n-        obs = self.env.render(\"rgb_array\")\n+        obs = self.env.render()\n         obs = resize(obs, (obs.shape[0] // 10, obs.shape[1] // 10),\n                      anti_aliasing=True)\n         return obs\n \n     def step(self, action):\n         obs, reward, done, info = self.env.step(action)\n-        obs = self.env.render(\"rgb_array\")\n+        obs = self.env.render()\n         obs = resize(obs, (obs.shape[0] // 10, obs.shape[1] // 10),\n                      anti_aliasing=True)\n         return obs, reward, done, info\n     \n"
                },
                {
                    "date": 1691650027728,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -18,40 +18,8 @@\n # Parallel environments\n     \n policy_kwargs = dict(activation_fn=th.nn.Tanh,net_arch=[128, dict(pi=[64,64], vf=[64,64])])\n # policy_kwargs = dict(activation_fn=th.nn.Tanh,net_arch=[128, 128])\n-class RGBArrayAsObservationWrapper(Wrapper):\n-    \"\"\"\n-    Use env.render(rgb_array) as observation\n-    rather than the observation environment provides\n-    \"\"\"\n-\n-    def __init__(self, env):\n-        # TODO this might not work before environment has been reset\n-        super(RGBArrayAsObservationWrapper, self).__init__(env)\n-        self.reset()\n-        dummy_obs = env.render()\n-        dummy_obs_resized = resize(dummy_obs, (dummy_obs.shape[0] // 10, dummy_obs.shape[1] // 10),\n-                                   anti_aliasing=True)\n-        # Update observation space\n-        # TODO assign correct low and high\n-        self.observation_space = spaces.Box(low=0, high=255, shape=dummy_obs_resized.shape,\n-                                            dtype=dummy_obs_resized.dtype)\n-\n-    def reset(self, **kwargs):\n-        obs = self.env.reset(**kwargs)\n-        obs = self.env.render()\n-        obs = resize(obs, (obs.shape[0] // 10, obs.shape[1] // 10),\n-                     anti_aliasing=True)\n-        return obs\n-\n-    def step(self, action):\n-        obs, reward, done, info = self.env.step(action)\n-        obs = self.env.render()\n-        obs = resize(obs, (obs.shape[0] // 10, obs.shape[1] // 10),\n-                     anti_aliasing=True)\n-        return obs, reward, done, info\n-    \n def main():\n     # Parallel environments\n     log_dir=\"./log\"\n     env = frankaEnv.FrankaEnv()\n"
                },
                {
                    "date": 1691650057588,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -23,10 +23,9 @@\n     # Parallel environments\n     log_dir=\"./log\"\n     env = frankaEnv.FrankaEnv()\n     env = Monitor(env, log_dir)\n-    env = RGBArrayAsObservationWrapper(env)\n-    check_env(env, warn=True)\n+    # check_env(env, warn=True)\n \n     n_step = 10000 #1000\n     total_timestep = 10000 * 5000\n     # print(\"1\")\n"
                },
                {
                    "date": 1691651014712,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -7,9 +7,9 @@\n \n from stable_baselines3 import PPO\n # from stable_baselines3.common.env_util import make_vec_env\n # from stable_baselines3.common.callbacks import EvalCallback\n-from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n+from stable_baselines3.common.callbacks import EpisodeEndCallback, EvalCallback, StopTrainingOnRewardThreshold\n import frankaEnv\n from gym import Wrapper, spaces\n \n # device = th.device('cuda:0')\n@@ -34,12 +34,14 @@\n     # print(\"2\")\n     # model.to(device)\n     # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n     # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n-    eval_callback = EvalCallback(env, best_model_save_path=\"./logs/\",\n-                             log_path=\"./logs/\", eval_freq=1,\n-                             deterministic=False, verbose=0)\n+    # eval_callback = EvalCallback(env, best_model_save_path=\"./logs/\",\n+    #                          log_path=\"./logs/\", eval_freq=1,\n+    #                          deterministic=False, verbose=0)\n+    episode_end_callback = EpisodeEndCallback()\n \n+\n     model.learn(total_timesteps=total_timestep, callback=eval_callback)\n     model.save(\"ppo_padna_mppi_0810\")\n \n     # del model # remove to demonstrate saving and loading\n"
                },
                {
                    "date": 1691651027690,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -40,9 +40,9 @@\n     #                          deterministic=False, verbose=0)\n     episode_end_callback = EpisodeEndCallback()\n \n \n-    model.learn(total_timesteps=total_timestep, callback=eval_callback)\n+    model.learn(total_timesteps=total_timestep, callback=episode_end_callback)\n     model.save(\"ppo_padna_mppi_0810\")\n \n     # del model # remove to demonstrate saving and loading\n \n"
                },
                {
                    "date": 1691651052849,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -7,9 +7,9 @@\n \n from stable_baselines3 import PPO\n # from stable_baselines3.common.env_util import make_vec_env\n # from stable_baselines3.common.callbacks import EvalCallback\n-from stable_baselines3.common.callbacks import EpisodeEndCallback, EvalCallback, StopTrainingOnRewardThreshold\n+from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n import frankaEnv\n from gym import Wrapper, spaces\n \n # device = th.device('cuda:0')\n@@ -34,15 +34,13 @@\n     # print(\"2\")\n     # model.to(device)\n     # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n     # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n-    # eval_callback = EvalCallback(env, best_model_save_path=\"./logs/\",\n-    #                          log_path=\"./logs/\", eval_freq=1,\n-    #                          deterministic=False, verbose=0)\n-    episode_end_callback = EpisodeEndCallback()\n+    eval_callback = EvalCallback(env, best_model_save_path=\"./logs/\",\n+                             log_path=\"./logs/\", eval_freq=1,\n+                             deterministic=False, verbose=0)\n \n-\n-    model.learn(total_timesteps=total_timestep, callback=episode_end_callback)\n+    model.learn(total_timesteps=total_timestep, callback=eval_callback)\n     model.save(\"ppo_padna_mppi_0810\")\n \n     # del model # remove to demonstrate saving and loading\n \n"
                },
                {
                    "date": 1691651136745,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -15,9 +15,20 @@\n # device = th.device('cuda:0')\n device = th.device('cuda:0')\n # th.cuda.is_available()\n # Parallel environments\n+\n+class CustomEvalCallback(EvalCallback):\n+    def __init__(self, *args, **kwargs):\n+        super(CustomEvalCallback, self).__init__(*args, **kwargs)\n+\n+    def _on_step(self) -> bool:\n+        if self.n_calls % self.eval_freq == 0:\n+            mean_reward, mean_episode_length = self._evaluate_model()\n+            print(f\"Step: {self.num_timesteps}, Mean Reward: {mean_reward:.2f}, Mean Episode Length: {mean_episode_length:.2f}\")\n+        return True\n     \n+    \n policy_kwargs = dict(activation_fn=th.nn.Tanh,net_arch=[128, dict(pi=[64,64], vf=[64,64])])\n # policy_kwargs = dict(activation_fn=th.nn.Tanh,net_arch=[128, 128])\n def main():\n     # Parallel environments\n@@ -34,11 +45,12 @@\n     # print(\"2\")\n     # model.to(device)\n     # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n     # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n-    eval_callback = EvalCallback(env, best_model_save_path=\"./logs/\",\n-                             log_path=\"./logs/\", eval_freq=1,\n-                             deterministic=False, verbose=0)\n+    # eval_callback = EvalCallback(env, best_model_save_path=\"./logs/\",\n+    #                          log_path=\"./logs/\", eval_freq=1,\n+    #                          deterministic=False, verbose=0)\n+    eval_callback = CustomEvalCallback(env, eval_freq=1, deterministic=False)\n \n     model.learn(total_timesteps=total_timestep, callback=eval_callback)\n     model.save(\"ppo_padna_mppi_0810\")\n \n"
                },
                {
                    "date": 1691651192885,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -25,10 +25,10 @@\n         if self.n_calls % self.eval_freq == 0:\n             mean_reward, mean_episode_length = self._evaluate_model()\n             print(f\"Step: {self.num_timesteps}, Mean Reward: {mean_reward:.2f}, Mean Episode Length: {mean_episode_length:.2f}\")\n         return True\n+        \n     \n-    \n policy_kwargs = dict(activation_fn=th.nn.Tanh,net_arch=[128, dict(pi=[64,64], vf=[64,64])])\n # policy_kwargs = dict(activation_fn=th.nn.Tanh,net_arch=[128, 128])\n def main():\n     # Parallel environments\n@@ -45,12 +45,11 @@\n     # print(\"2\")\n     # model.to(device)\n     # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n     # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n-    # eval_callback = EvalCallback(env, best_model_save_path=\"./logs/\",\n-    #                          log_path=\"./logs/\", eval_freq=1,\n-    #                          deterministic=False, verbose=0)\n-    eval_callback = CustomEvalCallback(env, eval_freq=1, deterministic=False)\n+    eval_callback = EvalCallback(env, best_model_save_path=\"./logs/\",\n+                             log_path=\"./logs/\", eval_freq=1,\n+                             deterministic=False, verbose=0)\n \n     model.learn(total_timesteps=total_timestep, callback=eval_callback)\n     model.save(\"ppo_padna_mppi_0810\")\n \n"
                },
                {
                    "date": 1691651199410,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -16,18 +16,8 @@\n device = th.device('cuda:0')\n # th.cuda.is_available()\n # Parallel environments\n \n-class CustomEvalCallback(EvalCallback):\n-    def __init__(self, *args, **kwargs):\n-        super(CustomEvalCallback, self).__init__(*args, **kwargs)\n-\n-    def _on_step(self) -> bool:\n-        if self.n_calls % self.eval_freq == 0:\n-            mean_reward, mean_episode_length = self._evaluate_model()\n-            print(f\"Step: {self.num_timesteps}, Mean Reward: {mean_reward:.2f}, Mean Episode Length: {mean_episode_length:.2f}\")\n-        return True\n-        \n     \n policy_kwargs = dict(activation_fn=th.nn.Tanh,net_arch=[128, dict(pi=[64,64], vf=[64,64])])\n # policy_kwargs = dict(activation_fn=th.nn.Tanh,net_arch=[128, 128])\n def main():\n@@ -47,9 +37,9 @@\n     # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n     # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n     eval_callback = EvalCallback(env, best_model_save_path=\"./logs/\",\n                              log_path=\"./logs/\", eval_freq=1,\n-                             deterministic=False, verbose=0)\n+                             deterministic=False, verbose=1)\n \n     model.learn(total_timesteps=total_timestep, callback=eval_callback)\n     model.save(\"ppo_padna_mppi_0810\")\n \n"
                },
                {
                    "date": 1691651250762,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -36,9 +36,9 @@\n     # model.to(device)\n     # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n     # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n     eval_callback = EvalCallback(env, best_model_save_path=\"./logs/\",\n-                             log_path=\"./logs/\", eval_freq=1,\n+                             log_path=\"./logs/\", eval_freq=1000,\n                              deterministic=False, verbose=1)\n \n     model.learn(total_timesteps=total_timestep, callback=eval_callback)\n     model.save(\"ppo_padna_mppi_0810\")\n"
                },
                {
                    "date": 1691651298437,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -36,9 +36,9 @@\n     # model.to(device)\n     # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n     # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n     eval_callback = EvalCallback(env, best_model_save_path=\"./logs/\",\n-                             log_path=\"./logs/\", eval_freq=1000,\n+                             log_path=\"./logs/\", eval_freq=1000, n_eval_episodes = 10\n                              deterministic=False, verbose=1)\n \n     model.learn(total_timesteps=total_timestep, callback=eval_callback)\n     model.save(\"ppo_padna_mppi_0810\")\n"
                },
                {
                    "date": 1691651457029,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -36,9 +36,9 @@\n     # model.to(device)\n     # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n     # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n     eval_callback = EvalCallback(env, best_model_save_path=\"./logs/\",\n-                             log_path=\"./logs/\", eval_freq=1000, n_eval_episodes = 10\n+                             log_path=\"./logs/\", eval_freq=1, n_eval_episodes = 1,\n                              deterministic=False, verbose=1)\n \n     model.learn(total_timesteps=total_timestep, callback=eval_callback)\n     model.save(\"ppo_padna_mppi_0810\")\n"
                },
                {
                    "date": 1691651739771,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -29,10 +29,10 @@\n \n     n_step = 10000 #1000\n     total_timestep = 10000 * 5000\n     # print(\"1\")\n-    # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = 100000, policy_kwargs=policy_kwargs, verbose=1)\n-    model = PPO(\"MlpPolicy\", env, verbose=1)\n+    model = PPO(\"MlpPolicy\", env, n_steps=n_step, policy_kwargs=policy_kwargs, verbose=1)\n+    # model = PPO(\"MlpPolicy\", env, verbose=1)\n     # print(\"2\")\n     # model.to(device)\n     # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n     # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n"
                },
                {
                    "date": 1691651770789,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -29,10 +29,10 @@\n \n     n_step = 10000 #1000\n     total_timestep = 10000 * 5000\n     # print(\"1\")\n-    model = PPO(\"MlpPolicy\", env, n_steps=n_step, policy_kwargs=policy_kwargs, verbose=1)\n-    # model = PPO(\"MlpPolicy\", env, verbose=1)\n+    # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = 100000, policy_kwargs=policy_kwargs, verbose=1)\n+    model = PPO(\"MlpPolicy\", env, verbose=1)\n     # print(\"2\")\n     # model.to(device)\n     # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n     # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n"
                },
                {
                    "date": 1691651887672,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -15,9 +15,18 @@\n # device = th.device('cuda:0')\n device = th.device('cuda:0')\n # th.cuda.is_available()\n # Parallel environments\n+class PrintOnEpisodeEnd(Callback):\n+    def __init__(self):\n+        super(PrintOnEpisodeEnd, self).__init__()\n \n+    def _on_step(self) -> bool:\n+        if self.locals.get(\"done\"):\n+            episode_reward = self.locals.get(\"episode_reward\")\n+            episode_length = self.locals.get(\"t\")\n+            print(f\"Episode {self.num_episodes}: Reward: {episode_reward:.2f}, Length: {episode_length}\")\n+        return True\n     \n policy_kwargs = dict(activation_fn=th.nn.Tanh,net_arch=[128, dict(pi=[64,64], vf=[64,64])])\n # policy_kwargs = dict(activation_fn=th.nn.Tanh,net_arch=[128, 128])\n def main():\n"
                },
                {
                    "date": 1691651898088,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -47,8 +47,10 @@\n     # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n     eval_callback = EvalCallback(env, best_model_save_path=\"./logs/\",\n                              log_path=\"./logs/\", eval_freq=1, n_eval_episodes = 1,\n                              deterministic=False, verbose=1)\n+    \n+    print_callback = PrintOnEpisodeEnd()\n \n     model.learn(total_timesteps=total_timestep, callback=eval_callback)\n     model.save(\"ppo_padna_mppi_0810\")\n \n"
                },
                {
                    "date": 1691651917976,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -7,9 +7,9 @@\n \n from stable_baselines3 import PPO\n # from stable_baselines3.common.env_util import make_vec_env\n # from stable_baselines3.common.callbacks import EvalCallback\n-from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n+from stable_baselines3.common.callbacks import Callback, EvalCallback, StopTrainingOnRewardThreshold\n import frankaEnv\n from gym import Wrapper, spaces\n \n # device = th.device('cuda:0')\n@@ -50,9 +50,9 @@\n                              deterministic=False, verbose=1)\n     \n     print_callback = PrintOnEpisodeEnd()\n \n-    model.learn(total_timesteps=total_timestep, callback=eval_callback)\n+    model.learn(total_timesteps=total_timestep, callback=print_callback)\n     model.save(\"ppo_padna_mppi_0810\")\n \n     # del model # remove to demonstrate saving and loading\n \n"
                },
                {
                    "date": 1691651955836,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -15,18 +15,41 @@\n # device = th.device('cuda:0')\n device = th.device('cuda:0')\n # th.cuda.is_available()\n # Parallel environments\n-class PrintOnEpisodeEnd(Callback):\n-    def __init__(self):\n-        super(PrintOnEpisodeEnd, self).__init__()\n+class PrintOnEpisodeEnd(EvalCallback):\n+    def __init__(self, *args, **kwargs):\n+        super(PrintOnEpisodeEnd, self).__init__(*args, **kwargs)\n \n     def _on_step(self) -> bool:\n-        if self.locals.get(\"done\"):\n-            episode_reward = self.locals.get(\"episode_reward\")\n-            episode_length = self.locals.get(\"t\")\n-            print(f\"Episode {self.num_episodes}: Reward: {episode_reward:.2f}, Length: {episode_length}\")\n+        if self.n_calls % self.eval_freq == 0:\n+            mean_reward, mean_episode_length = self._custom_evaluate_model()\n+            print(f\"Step: {self.num_timesteps}, Mean Reward: {mean_reward:.2f}, Mean Episode Length: {mean_episode_length:.2f}\")\n         return True\n+\n+    def _custom_evaluate_model(self):\n+        episode_rewards = []\n+        episode_lengths = []\n+\n+        for _ in range(self.n_eval_episodes):\n+            episode_reward = 0\n+            episode_length = 0\n+            done = False\n+            obs = self.env.reset()\n+\n+            while not done:\n+                action, _ = self.model.predict(obs, deterministic=self.deterministic_eval)\n+                obs, reward, done, _ = self.env.step(action)\n+                episode_reward += reward\n+                episode_length += 1\n+\n+            episode_rewards.append(episode_reward)\n+            episode_lengths.append(episode_length)\n+\n+        mean_reward = sum(episode_rewards) / len(episode_rewards)\n+        mean_episode_length = sum(episode_lengths) / len(episode_lengths)\n+\n+        return mean_reward, mean_episode_length\n     \n policy_kwargs = dict(activation_fn=th.nn.Tanh,net_arch=[128, dict(pi=[64,64], vf=[64,64])])\n # policy_kwargs = dict(activation_fn=th.nn.Tanh,net_arch=[128, 128])\n def main():\n"
                },
                {
                    "date": 1691652009438,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -7,9 +7,9 @@\n \n from stable_baselines3 import PPO\n # from stable_baselines3.common.env_util import make_vec_env\n # from stable_baselines3.common.callbacks import EvalCallback\n-from stable_baselines3.common.callbacks import Callback, EvalCallback, StopTrainingOnRewardThreshold\n+from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n import frankaEnv\n from gym import Wrapper, spaces\n \n # device = th.device('cuda:0')\n@@ -71,9 +71,9 @@\n     eval_callback = EvalCallback(env, best_model_save_path=\"./logs/\",\n                              log_path=\"./logs/\", eval_freq=1, n_eval_episodes = 1,\n                              deterministic=False, verbose=1)\n     \n-    print_callback = PrintOnEpisodeEnd()\n+    print_callback = PrintOnEpisodeEnd(eval_env=env, eval_freq=1, deterministic=False, n_eval_episodes=1)\n \n     model.learn(total_timesteps=total_timestep, callback=print_callback)\n     model.save(\"ppo_padna_mppi_0810\")\n \n"
                },
                {
                    "date": 1691652066407,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -15,41 +15,18 @@\n # device = th.device('cuda:0')\n device = th.device('cuda:0')\n # th.cuda.is_available()\n # Parallel environments\n-class PrintOnEpisodeEnd(EvalCallback):\n-    def __init__(self, *args, **kwargs):\n-        super(PrintOnEpisodeEnd, self).__init__(*args, **kwargs)\n+class PrintOnEpisodeEnd(Callback):\n+    def __init__(self):\n+        super(PrintOnEpisodeEnd, self).__init__()\n \n     def _on_step(self) -> bool:\n-        if self.n_calls % self.eval_freq == 0:\n-            mean_reward, mean_episode_length = self._custom_evaluate_model()\n-            print(f\"Step: {self.num_timesteps}, Mean Reward: {mean_reward:.2f}, Mean Episode Length: {mean_episode_length:.2f}\")\n+        if self.locals.get(\"done\"):\n+            episode_reward = self.locals.get(\"episode_reward\")\n+            episode_length = self.locals.get(\"t\")\n+            print(f\"Episode {self.num_episodes}: Reward: {episode_reward:.2f}, Length: {episode_length}\")\n         return True\n-\n-    def _custom_evaluate_model(self):\n-        episode_rewards = []\n-        episode_lengths = []\n-\n-        for _ in range(self.n_eval_episodes):\n-            episode_reward = 0\n-            episode_length = 0\n-            done = False\n-            obs = self.env.reset()\n-\n-            while not done:\n-                action, _ = self.model.predict(obs, deterministic=self.deterministic_eval)\n-                obs, reward, done, _ = self.env.step(action)\n-                episode_reward += reward\n-                episode_length += 1\n-\n-            episode_rewards.append(episode_reward)\n-            episode_lengths.append(episode_length)\n-\n-        mean_reward = sum(episode_rewards) / len(episode_rewards)\n-        mean_episode_length = sum(episode_lengths) / len(episode_lengths)\n-\n-        return mean_reward, mean_episode_length\n     \n policy_kwargs = dict(activation_fn=th.nn.Tanh,net_arch=[128, dict(pi=[64,64], vf=[64,64])])\n # policy_kwargs = dict(activation_fn=th.nn.Tanh,net_arch=[128, 128])\n def main():\n@@ -70,12 +47,10 @@\n     # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n     eval_callback = EvalCallback(env, best_model_save_path=\"./logs/\",\n                              log_path=\"./logs/\", eval_freq=1, n_eval_episodes = 1,\n                              deterministic=False, verbose=1)\n-    \n-    print_callback = PrintOnEpisodeEnd(eval_env=env, eval_freq=1, deterministic=False, n_eval_episodes=1)\n \n-    model.learn(total_timesteps=total_timestep, callback=print_callback)\n+    model.learn(total_timesteps=total_timestep, callback=eval_callback)\n     model.save(\"ppo_padna_mppi_0810\")\n \n     # del model # remove to demonstrate saving and loading\n \n"
                },
                {
                    "date": 1691652072699,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -15,18 +15,8 @@\n # device = th.device('cuda:0')\n device = th.device('cuda:0')\n # th.cuda.is_available()\n # Parallel environments\n-class PrintOnEpisodeEnd(Callback):\n-    def __init__(self):\n-        super(PrintOnEpisodeEnd, self).__init__()\n-\n-    def _on_step(self) -> bool:\n-        if self.locals.get(\"done\"):\n-            episode_reward = self.locals.get(\"episode_reward\")\n-            episode_length = self.locals.get(\"t\")\n-            print(f\"Episode {self.num_episodes}: Reward: {episode_reward:.2f}, Length: {episode_length}\")\n-        return True\n     \n policy_kwargs = dict(activation_fn=th.nn.Tanh,net_arch=[128, dict(pi=[64,64], vf=[64,64])])\n # policy_kwargs = dict(activation_fn=th.nn.Tanh,net_arch=[128, 128])\n def main():\n"
                },
                {
                    "date": 1691652511438,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -36,9 +36,9 @@\n     # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n     # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n     eval_callback = EvalCallback(env, best_model_save_path=\"./logs/\",\n                              log_path=\"./logs/\", eval_freq=1, n_eval_episodes = 1,\n-                             deterministic=False, verbose=1)\n+                             deterministic=False, verbose=10)\n \n     model.learn(total_timesteps=total_timestep, callback=eval_callback)\n     model.save(\"ppo_padna_mppi_0810\")\n \n"
                },
                {
                    "date": 1691652573520,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -35,9 +35,9 @@\n     # model.to(device)\n     # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n     # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n     eval_callback = EvalCallback(env, best_model_save_path=\"./logs/\",\n-                             log_path=\"./logs/\", eval_freq=1, n_eval_episodes = 1,\n+                             log_path=\"./logs/\", eval_freq=10000, n_eval_episodes = 1,\n                              deterministic=False, verbose=10)\n \n     model.learn(total_timesteps=total_timestep, callback=eval_callback)\n     model.save(\"ppo_padna_mppi_0810\")\n"
                },
                {
                    "date": 1691652582839,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -35,9 +35,9 @@\n     # model.to(device)\n     # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n     # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n     eval_callback = EvalCallback(env, best_model_save_path=\"./logs/\",\n-                             log_path=\"./logs/\", eval_freq=10000, n_eval_episodes = 1,\n+                             log_path=\"./logs/\", eval_freq=3333, n_eval_episodes = 5,\n                              deterministic=False, verbose=10)\n \n     model.learn(total_timesteps=total_timestep, callback=eval_callback)\n     model.save(\"ppo_padna_mppi_0810\")\n"
                },
                {
                    "date": 1691652713218,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -35,9 +35,9 @@\n     # model.to(device)\n     # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n     # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n     eval_callback = EvalCallback(env, best_model_save_path=\"./logs/\",\n-                             log_path=\"./logs/\", eval_freq=3333, n_eval_episodes = 5,\n+                             log_path=\"./logs/\", eval_freq=3333, n_eval_episodes = 1,\n                              deterministic=False, verbose=10)\n \n     model.learn(total_timesteps=total_timestep, callback=eval_callback)\n     model.save(\"ppo_padna_mppi_0810\")\n"
                },
                {
                    "date": 1691652815157,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -35,9 +35,9 @@\n     # model.to(device)\n     # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n     # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n     eval_callback = EvalCallback(env, best_model_save_path=\"./logs/\",\n-                             log_path=\"./logs/\", eval_freq=3333, n_eval_episodes = 1,\n+                             log_path=\"./logs/\", eval_freq=1000, n_eval_episodes = 10,\n                              deterministic=False, verbose=10)\n \n     model.learn(total_timesteps=total_timestep, callback=eval_callback)\n     model.save(\"ppo_padna_mppi_0810\")\n"
                },
                {
                    "date": 1691652925196,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -10,8 +10,12 @@\n # from stable_baselines3.common.callbacks import EvalCallback\n from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n import frankaEnv\n from gym import Wrapper, spaces\n+from stable_baselines3.common.logger import configure\n+tmp_path = \"/tmp/sb3_log/\"\n+# set up logger\n+new_logger = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n \n # device = th.device('cuda:0')\n device = th.device('cuda:0')\n # th.cuda.is_available()\n"
                },
                {
                    "date": 1691652935911,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -41,9 +41,9 @@\n     # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n     eval_callback = EvalCallback(env, best_model_save_path=\"./logs/\",\n                              log_path=\"./logs/\", eval_freq=1000, n_eval_episodes = 10,\n                              deterministic=False, verbose=10)\n-\n+    model.set_logger(new_logger)\n     model.learn(total_timesteps=total_timestep, callback=eval_callback)\n     model.save(\"ppo_padna_mppi_0810\")\n \n     # del model # remove to demonstrate saving and loading\n"
                },
                {
                    "date": 1691652973254,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -11,9 +11,9 @@\n from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n import frankaEnv\n from gym import Wrapper, spaces\n from stable_baselines3.common.logger import configure\n-tmp_path = \"/tmp/sb3_log/\"\n+tmp_path = \"./log\"\n # set up logger\n new_logger = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n \n # device = th.device('cuda:0')\n"
                },
                {
                    "date": 1691653136623,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -10,12 +10,8 @@\n # from stable_baselines3.common.callbacks import EvalCallback\n from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n import frankaEnv\n from gym import Wrapper, spaces\n-from stable_baselines3.common.logger import configure\n-tmp_path = \"./log\"\n-# set up logger\n-new_logger = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n \n # device = th.device('cuda:0')\n device = th.device('cuda:0')\n # th.cuda.is_available()\n@@ -39,11 +35,11 @@\n     # model.to(device)\n     # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n     # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n     eval_callback = EvalCallback(env, best_model_save_path=\"./logs/\",\n-                             log_path=\"./logs/\", eval_freq=1000, n_eval_episodes = 10,\n+                             log_path=\"./logs/\", eval_freq=1000, n_eval_episodes = 1,\n                              deterministic=False, verbose=10)\n-    model.set_logger(new_logger)\n+\n     model.learn(total_timesteps=total_timestep, callback=eval_callback)\n     model.save(\"ppo_padna_mppi_0810\")\n \n     # del model # remove to demonstrate saving and loading\n"
                },
                {
                    "date": 1691653508085,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -35,9 +35,9 @@\n     # model.to(device)\n     # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n     # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n     eval_callback = EvalCallback(env, best_model_save_path=\"./logs/\",\n-                             log_path=\"./logs/\", eval_freq=1000, n_eval_episodes = 1,\n+                             log_path=\"./logs/\", eval_freq=0, n_eval_episodes = 1,\n                              deterministic=False, verbose=10)\n \n     model.learn(total_timesteps=total_timestep, callback=eval_callback)\n     model.save(\"ppo_padna_mppi_0810\")\n"
                },
                {
                    "date": 1691653744173,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -34,13 +34,13 @@\n     # print(\"2\")\n     # model.to(device)\n     # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n     # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n-    eval_callback = EvalCallback(env, best_model_save_path=\"./logs/\",\n-                             log_path=\"./logs/\", eval_freq=0, n_eval_episodes = 1,\n-                             deterministic=False, verbose=10)\n+    # eval_callback = EvalCallback(env, best_model_save_path=\"./logs/\",\n+    #                          log_path=\"./logs/\", eval_freq=0, n_eval_episodes = 1,\n+    #                          deterministic=False, verbose=10)\n \n-    model.learn(total_timesteps=total_timestep, callback=eval_callback)\n+    model.learn(total_timesteps=total_timestep)#, callback=eval_callback)\n     model.save(\"ppo_padna_mppi_0810\")\n \n     # del model # remove to demonstrate saving and loading\n \n"
                },
                {
                    "date": 1691653834982,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -25,13 +25,13 @@\n     env = frankaEnv.FrankaEnv()\n     env = Monitor(env, log_dir)\n     # check_env(env, warn=True)\n \n-    n_step = 10000 #1000\n+    n_step = 100 #1000\n     total_timestep = 10000 * 5000\n     # print(\"1\")\n     # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = 100000, policy_kwargs=policy_kwargs, verbose=1)\n-    model = PPO(\"MlpPolicy\", env, verbose=1)\n+    model = PPO(\"MlpPolicy\", env, n_steps=n_step, verbose=1)\n     # print(\"2\")\n     # model.to(device)\n     # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n     # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n"
                },
                {
                    "date": 1691653865796,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -25,9 +25,9 @@\n     env = frankaEnv.FrankaEnv()\n     env = Monitor(env, log_dir)\n     # check_env(env, warn=True)\n \n-    n_step = 100 #1000\n+    n_step = 30000 #1000\n     total_timestep = 10000 * 5000\n     # print(\"1\")\n     # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = 100000, policy_kwargs=policy_kwargs, verbose=1)\n     model = PPO(\"MlpPolicy\", env, n_steps=n_step, verbose=1)\n"
                },
                {
                    "date": 1691653898579,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -25,13 +25,13 @@\n     env = frankaEnv.FrankaEnv()\n     env = Monitor(env, log_dir)\n     # check_env(env, warn=True)\n \n-    n_step = 30000 #1000\n+    n_step = 3000 #1000\n     total_timestep = 10000 * 5000\n     # print(\"1\")\n     # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = 100000, policy_kwargs=policy_kwargs, verbose=1)\n-    model = PPO(\"MlpPolicy\", env, n_steps=n_step, verbose=1)\n+    model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = 3000, verbose=1)\n     # print(\"2\")\n     # model.to(device)\n     # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n     # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n"
                },
                {
                    "date": 1691972052826,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -29,9 +29,9 @@\n     n_step = 3000 #1000\n     total_timestep = 10000 * 5000\n     # print(\"1\")\n     # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = 100000, policy_kwargs=policy_kwargs, verbose=1)\n-    model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = 3000, verbose=1)\n+    # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = n_step, verbose=1)\n     # print(\"2\")\n     # model.to(device)\n     # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n     # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n@@ -43,15 +43,15 @@\n     model.save(\"ppo_padna_mppi_0810\")\n \n     # del model # remove to demonstrate saving and loading\n \n-    # model = PPO.load(\"./logs/best_model.zip\")\n+    model = PPO.load(\"./logs/best_model.zip\")\n \n     # obs = env.reset()\n-    # while True:\n-    #     action, _states = model.predict(obs)\n-    #     obs, rewards, dones, info = env.step(action)\n-    #     env.render()\n+    while True:\n+        action, _states = model.predict(obs)\n+        obs, rewards, dones, info = env.step(action)\n+        env.render()\n     \n     \n     \n if __name__ == \"__main__\":\n"
                },
                {
                    "date": 1691972060702,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -25,10 +25,10 @@\n     env = frankaEnv.FrankaEnv()\n     env = Monitor(env, log_dir)\n     # check_env(env, warn=True)\n \n-    n_step = 3000 #1000\n-    total_timestep = 10000 * 5000\n+    # n_step = 3000 #1000\n+    # total_timestep = 10000 * 5000\n     # print(\"1\")\n     # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = 100000, policy_kwargs=policy_kwargs, verbose=1)\n     # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = n_step, verbose=1)\n     # print(\"2\")\n@@ -38,10 +38,10 @@\n     # eval_callback = EvalCallback(env, best_model_save_path=\"./logs/\",\n     #                          log_path=\"./logs/\", eval_freq=0, n_eval_episodes = 1,\n     #                          deterministic=False, verbose=10)\n \n-    model.learn(total_timesteps=total_timestep)#, callback=eval_callback)\n-    model.save(\"ppo_padna_mppi_0810\")\n+    # model.learn(total_timesteps=total_timestep)#, callback=eval_callback)\n+    # model.save(\"ppo_padna_mppi_0810\")\n \n     # del model # remove to demonstrate saving and loading\n \n     model = PPO.load(\"./logs/best_model.zip\")\n"
                },
                {
                    "date": 1691972117417,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -43,9 +43,9 @@\n     # model.save(\"ppo_padna_mppi_0810\")\n \n     # del model # remove to demonstrate saving and loading\n \n-    model = PPO.load(\"./logs/best_model.zip\")\n+    model = PPO.load(\"./logs/ppo_padna_mppi_0810.zip\")\n \n     # obs = env.reset()\n     while True:\n         action, _states = model.predict(obs)\n"
                },
                {
                    "date": 1691972145423,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -43,11 +43,11 @@\n     # model.save(\"ppo_padna_mppi_0810\")\n \n     # del model # remove to demonstrate saving and loading\n \n-    model = PPO.load(\"./logs/ppo_padna_mppi_0810.zip\")\n+    model = PPO.load(\"./ppo_padna_mppi_0810.zip\")\n \n-    # obs = env.reset()\n+    obs = env.reset()\n     while True:\n         action, _states = model.predict(obs)\n         obs, rewards, dones, info = env.step(action)\n         env.render()\n"
                },
                {
                    "date": 1691972172687,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -22,9 +22,9 @@\n def main():\n     # Parallel environments\n     log_dir=\"./log\"\n     env = frankaEnv.FrankaEnv()\n-    env = Monitor(env, log_dir)\n+    # env = Monitor(env, log_dir)\n     # check_env(env, warn=True)\n \n     # n_step = 3000 #1000\n     # total_timestep = 10000 * 5000\n"
                },
                {
                    "date": 1691980590997,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -22,36 +22,36 @@\n def main():\n     # Parallel environments\n     log_dir=\"./log\"\n     env = frankaEnv.FrankaEnv()\n-    # env = Monitor(env, log_dir)\n+    env = Monitor(env, log_dir)\n     # check_env(env, warn=True)\n \n-    # n_step = 3000 #1000\n-    # total_timestep = 10000 * 5000\n+    n_step = 3000 #1000\n+    total_timestep = 10000 * 5000\n     # print(\"1\")\n     # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = 100000, policy_kwargs=policy_kwargs, verbose=1)\n-    # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = n_step, verbose=1)\n+    model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = n_step, verbose=1)\n     # print(\"2\")\n     # model.to(device)\n     # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n     # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n     # eval_callback = EvalCallback(env, best_model_save_path=\"./logs/\",\n     #                          log_path=\"./logs/\", eval_freq=0, n_eval_episodes = 1,\n     #                          deterministic=False, verbose=10)\n \n-    # model.learn(total_timesteps=total_timestep)#, callback=eval_callback)\n-    # model.save(\"ppo_padna_mppi_0810\")\n+    model.learn(total_timesteps=total_timestep)#, callback=eval_callback)\n+    model.save(\"ppo_padna_mppi_0810\")\n \n     # del model # remove to demonstrate saving and loading\n \n-    model = PPO.load(\"./ppo_padna_mppi_0810.zip\")\n+    # model = PPO.load(\"./logs/best_model.zip\")\n \n-    obs = env.reset()\n-    while True:\n-        action, _states = model.predict(obs)\n-        obs, rewards, dones, info = env.step(action)\n-        env.render()\n+    # obs = env.reset()\n+    # while True:\n+    #     action, _states = model.predict(obs)\n+    #     obs, rewards, dones, info = env.step(action)\n+    #     env.render()\n     \n     \n     \n if __name__ == \"__main__\":\n"
                },
                {
                    "date": 1691981607747,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -29,29 +29,29 @@\n     n_step = 3000 #1000\n     total_timestep = 10000 * 5000\n     # print(\"1\")\n     # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = 100000, policy_kwargs=policy_kwargs, verbose=1)\n-    model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = n_step, verbose=1)\n-    # print(\"2\")\n-    # model.to(device)\n-    # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n-    # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n-    # eval_callback = EvalCallback(env, best_model_save_path=\"./logs/\",\n-    #                          log_path=\"./logs/\", eval_freq=0, n_eval_episodes = 1,\n-    #                          deterministic=False, verbose=10)\n+    # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = n_step, verbose=1)\n+    # # print(\"2\")\n+    # # model.to(device)\n+    # # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n+    # # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n+    # # eval_callback = EvalCallback(env, best_model_save_path=\"./logs/\",\n+    # #                          log_path=\"./logs/\", eval_freq=0, n_eval_episodes = 1,\n+    # #                          deterministic=False, verbose=10)\n \n-    model.learn(total_timesteps=total_timestep)#, callback=eval_callback)\n-    model.save(\"ppo_padna_mppi_0810\")\n+    # model.learn(total_timesteps=total_timestep)#, callback=eval_callback)\n+    # model.save(\"ppo_padna_mppi_0810\")\n \n     # del model # remove to demonstrate saving and loading\n \n-    # model = PPO.load(\"./logs/best_model.zip\")\n+    model = PPO.load(\"./logs/best_model.zip\")\n \n-    # obs = env.reset()\n-    # while True:\n-    #     action, _states = model.predict(obs)\n-    #     obs, rewards, dones, info = env.step(action)\n-    #     env.render()\n+    obs = env.reset()\n+    while True:\n+        action, _states = model.predict(obs)\n+        obs, rewards, dones, info = env.step(action)\n+        env.render()\n     \n     \n     \n if __name__ == \"__main__\":\n"
                },
                {
                    "date": 1691981625554,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -43,9 +43,9 @@\n     # model.save(\"ppo_padna_mppi_0810\")\n \n     # del model # remove to demonstrate saving and loading\n \n-    model = PPO.load(\"./logs/best_model.zip\")\n+    model = PPO.load(\"./best_model.zip\")\n \n     obs = env.reset()\n     while True:\n         action, _states = model.predict(obs)\n"
                },
                {
                    "date": 1691981640754,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -43,9 +43,9 @@\n     # model.save(\"ppo_padna_mppi_0810\")\n \n     # del model # remove to demonstrate saving and loading\n \n-    model = PPO.load(\"./best_model.zip\")\n+    model = PPO.load(\"./ppo_padna_mppi_0810.zip\")\n \n     obs = env.reset()\n     while True:\n         action, _states = model.predict(obs)\n"
                },
                {
                    "date": 1691981813217,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -46,8 +46,9 @@\n \n     model = PPO.load(\"./ppo_padna_mppi_0810.zip\")\n \n     obs = env.reset()\n+    obs = obs[0]\n     while True:\n         action, _states = model.predict(obs)\n         obs, rewards, dones, info = env.step(action)\n         env.render()\n"
                },
                {
                    "date": 1691981868532,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -49,9 +49,9 @@\n     obs = env.reset()\n     obs = obs[0]\n     while True:\n         action, _states = model.predict(obs)\n-        obs, rewards, dones, info = env.step(action)\n+        obs, rewards, dones, info, temp = env.step(action)\n         env.render()\n     \n     \n     \n"
                },
                {
                    "date": 1691981934783,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -51,8 +51,10 @@\n     while True:\n         action, _states = model.predict(obs)\n         obs, rewards, dones, info, temp = env.step(action)\n         env.render()\n+\n+\n     \n     \n     \n if __name__ == \"__main__\":\n"
                },
                {
                    "date": 1691981979145,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -50,9 +50,9 @@\n     obs = obs[0]\n     while True:\n         action, _states = model.predict(obs)\n         obs, rewards, dones, info, temp = env.step(action)\n-        env.render()\n+        # env.render()\n \n \n     \n     \n"
                },
                {
                    "date": 1691982555313,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -29,32 +29,29 @@\n     n_step = 3000 #1000\n     total_timestep = 10000 * 5000\n     # print(\"1\")\n     # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = 100000, policy_kwargs=policy_kwargs, verbose=1)\n-    # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = n_step, verbose=1)\n-    # # print(\"2\")\n-    # # model.to(device)\n-    # # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n-    # # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n-    # # eval_callback = EvalCallback(env, best_model_save_path=\"./logs/\",\n-    # #                          log_path=\"./logs/\", eval_freq=0, n_eval_episodes = 1,\n-    # #                          deterministic=False, verbose=10)\n+    model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = n_step, verbose=1)\n+    # print(\"2\")\n+    # model.to(device)\n+    # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n+    # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n+    # eval_callback = EvalCallback(env, best_model_save_path=\"./logs/\",\n+    #                          log_path=\"./logs/\", eval_freq=0, n_eval_episodes = 1,\n+    #                          deterministic=False, verbose=10)\n \n-    # model.learn(total_timesteps=total_timestep)#, callback=eval_callback)\n-    # model.save(\"ppo_padna_mppi_0810\")\n+    model.learn(total_timesteps=total_timestep)#, callback=eval_callback)\n+    model.save(\"ppo_padna_mppi_0810\")\n \n     # del model # remove to demonstrate saving and loading\n \n-    model = PPO.load(\"./ppo_padna_mppi_0810.zip\")\n+    # model = PPO.load(\"./logs/best_model.zip\")\n \n-    obs = env.reset()\n-    obs = obs[0]\n-    while True:\n-        action, _states = model.predict(obs)\n-        obs, rewards, dones, info, temp = env.step(action)\n-        # env.render()\n-\n-\n+    # obs = env.reset()\n+    # while True:\n+    #     action, _states = model.predict(obs)\n+    #     obs, rewards, dones, info = env.step(action)\n+    #     env.render()\n     \n     \n     \n if __name__ == \"__main__\":\n"
                },
                {
                    "date": 1691982690372,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -43,9 +43,9 @@\n     model.save(\"ppo_padna_mppi_0810\")\n \n     # del model # remove to demonstrate saving and loading\n \n-    # model = PPO.load(\"./logs/best_model.zip\")\n+    # model = PPO.load(\"./ppo_padna_mppi_0810.zip\")\n \n     # obs = env.reset()\n     # while True:\n     #     action, _states = model.predict(obs)\n"
                },
                {
                    "date": 1692154693324,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -39,9 +39,9 @@\n     #                          log_path=\"./logs/\", eval_freq=0, n_eval_episodes = 1,\n     #                          deterministic=False, verbose=10)\n \n     model.learn(total_timesteps=total_timestep)#, callback=eval_callback)\n-    model.save(\"ppo_padna_mppi_0810\")\n+    model.save(\"ppo_padna_mppi_0816\")\n \n     # del model # remove to demonstrate saving and loading\n \n     # model = PPO.load(\"./ppo_padna_mppi_0810.zip\")\n"
                },
                {
                    "date": 1692256037241,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -39,9 +39,9 @@\n     #                          log_path=\"./logs/\", eval_freq=0, n_eval_episodes = 1,\n     #                          deterministic=False, verbose=10)\n \n     model.learn(total_timesteps=total_timestep)#, callback=eval_callback)\n-    model.save(\"ppo_padna_mppi_0816\")\n+    model.save(\"ppo_padna_mppi_0817\")\n \n     # del model # remove to demonstrate saving and loading\n \n     # model = PPO.load(\"./ppo_padna_mppi_0810.zip\")\n"
                },
                {
                    "date": 1692257095401,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -12,9 +12,9 @@\n import frankaEnv\n from gym import Wrapper, spaces\n \n # device = th.device('cuda:0')\n-device = th.device('cuda:0')\n+device = th.device('cuda:1')\n # th.cuda.is_available()\n # Parallel environments\n     \n policy_kwargs = dict(activation_fn=th.nn.Tanh,net_arch=[128, dict(pi=[64,64], vf=[64,64])])\n"
                },
                {
                    "date": 1692257102782,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -12,9 +12,9 @@\n import frankaEnv\n from gym import Wrapper, spaces\n \n # device = th.device('cuda:0')\n-device = th.device('cuda:1')\n+device = th.device('cuda:0')\n # th.cuda.is_available()\n # Parallel environments\n     \n policy_kwargs = dict(activation_fn=th.nn.Tanh,net_arch=[128, dict(pi=[64,64], vf=[64,64])])\n"
                },
                {
                    "date": 1692257412546,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -29,9 +29,9 @@\n     n_step = 3000 #1000\n     total_timestep = 10000 * 5000\n     # print(\"1\")\n     # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = 100000, policy_kwargs=policy_kwargs, verbose=1)\n-    model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = n_step, verbose=1)\n+    model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = n_step, policy_kwargs=policy_kwargs,verbose=1)\n     # print(\"2\")\n     # model.to(device)\n     # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n     # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n"
                },
                {
                    "date": 1692257422219,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -28,10 +28,10 @@\n \n     n_step = 3000 #1000\n     total_timestep = 10000 * 5000\n     # print(\"1\")\n-    # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = 100000, policy_kwargs=policy_kwargs, verbose=1)\n-    model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = n_step, policy_kwargs=policy_kwargs,verbose=1)\n+    model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = 100000, policy_kwargs=policy_kwargs, verbose=1)\n+    # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = n_step, policy_kwargs=policy_kwargs, verbose=1)\n     # print(\"2\")\n     # model.to(device)\n     # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n     # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n"
                },
                {
                    "date": 1692257429339,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -28,10 +28,10 @@\n \n     n_step = 3000 #1000\n     total_timestep = 10000 * 5000\n     # print(\"1\")\n-    model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = 100000, policy_kwargs=policy_kwargs, verbose=1)\n-    # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = n_step, policy_kwargs=policy_kwargs, verbose=1)\n+    # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = 100000, policy_kwargs=policy_kwargs, verbose=1)\n+    model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = n_step, verbose=1)\n     # print(\"2\")\n     # model.to(device)\n     # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n     # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n"
                },
                {
                    "date": 1692257598030,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -12,11 +12,17 @@\n import frankaEnv\n from gym import Wrapper, spaces\n \n # device = th.device('cuda:0')\n-device = th.device('cuda:0')\n+# device = th.device('cuda:0')\n # th.cuda.is_available()\n # Parallel environments\n+# import os\n+os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n+\n+device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n+print(device)\n+\n     \n policy_kwargs = dict(activation_fn=th.nn.Tanh,net_arch=[128, dict(pi=[64,64], vf=[64,64])])\n # policy_kwargs = dict(activation_fn=th.nn.Tanh,net_arch=[128, 128])\n def main():\n@@ -26,9 +32,9 @@\n     env = Monitor(env, log_dir)\n     # check_env(env, warn=True)\n \n     n_step = 3000 #1000\n-    total_timestep = 10000 * 5000\n+    total_timestep = 10000 * 3000\n     # print(\"1\")\n     # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = 100000, policy_kwargs=policy_kwargs, verbose=1)\n     model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = n_step, verbose=1)\n     # print(\"2\")\n"
                },
                {
                    "date": 1692257605177,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -18,9 +18,9 @@\n # Parallel environments\n # import os\n os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n \n-device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n+device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n print(device)\n \n     \n policy_kwargs = dict(activation_fn=th.nn.Tanh,net_arch=[128, dict(pi=[64,64], vf=[64,64])])\n"
                },
                {
                    "date": 1692257704580,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -20,9 +20,11 @@\n os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n \n device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n print(device)\n-\n+th.version\n+th.cuda.get_arch_list()\n+th.cuda.get_device_name()\n     \n policy_kwargs = dict(activation_fn=th.nn.Tanh,net_arch=[128, dict(pi=[64,64], vf=[64,64])])\n # policy_kwargs = dict(activation_fn=th.nn.Tanh,net_arch=[128, 128])\n def main():\n"
                },
                {
                    "date": 1692257725830,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -20,11 +20,11 @@\n os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n \n device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n print(device)\n-th.version\n-th.cuda.get_arch_list()\n-th.cuda.get_device_name()\n+print(th.version)\n+print(th.cuda.get_arch_list()\n+print(th.cuda.get_device_name()))\n     \n policy_kwargs = dict(activation_fn=th.nn.Tanh,net_arch=[128, dict(pi=[64,64], vf=[64,64])])\n # policy_kwargs = dict(activation_fn=th.nn.Tanh,net_arch=[128, 128])\n def main():\n"
                },
                {
                    "date": 1692257753088,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -21,10 +21,9 @@\n \n device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n print(device)\n print(th.version)\n-print(th.cuda.get_arch_list()\n-print(th.cuda.get_device_name()))\n+print(th.cuda.get_device_name())\n     \n policy_kwargs = dict(activation_fn=th.nn.Tanh,net_arch=[128, dict(pi=[64,64], vf=[64,64])])\n # policy_kwargs = dict(activation_fn=th.nn.Tanh,net_arch=[128, 128])\n def main():\n"
                },
                {
                    "date": 1692257767974,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -21,9 +21,8 @@\n \n device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n print(device)\n print(th.version)\n-print(th.cuda.get_device_name())\n     \n policy_kwargs = dict(activation_fn=th.nn.Tanh,net_arch=[128, dict(pi=[64,64], vf=[64,64])])\n # policy_kwargs = dict(activation_fn=th.nn.Tanh,net_arch=[128, 128])\n def main():\n"
                },
                {
                    "date": 1692257776540,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -20,9 +20,8 @@\n os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n \n device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n print(device)\n-print(th.version)\n     \n policy_kwargs = dict(activation_fn=th.nn.Tanh,net_arch=[128, dict(pi=[64,64], vf=[64,64])])\n # policy_kwargs = dict(activation_fn=th.nn.Tanh,net_arch=[128, 128])\n def main():\n"
                },
                {
                    "date": 1692261955720,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -30,9 +30,9 @@\n     env = frankaEnv.FrankaEnv()\n     env = Monitor(env, log_dir)\n     # check_env(env, warn=True)\n \n-    n_step = 3000 #1000\n+    n_step = 1000 #1000\n     total_timestep = 10000 * 3000\n     # print(\"1\")\n     # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = 100000, policy_kwargs=policy_kwargs, verbose=1)\n     model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = n_step, verbose=1)\n"
                },
                {
                    "date": 1692261961895,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -34,9 +34,9 @@\n     n_step = 1000 #1000\n     total_timestep = 10000 * 3000\n     # print(\"1\")\n     # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = 100000, policy_kwargs=policy_kwargs, verbose=1)\n-    model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = n_step, verbose=1)\n+    model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = 1, verbose=1)\n     # print(\"2\")\n     # model.to(device)\n     # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n     # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n"
                },
                {
                    "date": 1692261968100,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -34,9 +34,9 @@\n     n_step = 1000 #1000\n     total_timestep = 10000 * 3000\n     # print(\"1\")\n     # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = 100000, policy_kwargs=policy_kwargs, verbose=1)\n-    model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = 1, verbose=1)\n+    model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = 100, verbose=1)\n     # print(\"2\")\n     # model.to(device)\n     # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n     # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n"
                },
                {
                    "date": 1692261981778,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -30,13 +30,13 @@\n     env = frankaEnv.FrankaEnv()\n     env = Monitor(env, log_dir)\n     # check_env(env, warn=True)\n \n-    n_step = 1000 #1000\n-    total_timestep = 10000 * 3000\n+    n_step = 128 #1000\n+    total_timestep = 128 * 3000\n     # print(\"1\")\n     # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = 100000, policy_kwargs=policy_kwargs, verbose=1)\n-    model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = 100, verbose=1)\n+    model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = n_step, verbose=1)\n     # print(\"2\")\n     # model.to(device)\n     # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n     # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n"
                },
                {
                    "date": 1692262109400,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -30,10 +30,10 @@\n     env = frankaEnv.FrankaEnv()\n     env = Monitor(env, log_dir)\n     # check_env(env, warn=True)\n \n-    n_step = 128 #1000\n-    total_timestep = 128 * 3000\n+    n_step = 1024 #1000\n+    total_timestep = 1024 * 3000\n     # print(\"1\")\n     # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = 100000, policy_kwargs=policy_kwargs, verbose=1)\n     model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = n_step, verbose=1)\n     # print(\"2\")\n"
                },
                {
                    "date": 1692320527955,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -30,33 +30,33 @@\n     env = frankaEnv.FrankaEnv()\n     env = Monitor(env, log_dir)\n     # check_env(env, warn=True)\n \n-    n_step = 1024 #1000\n-    total_timestep = 1024 * 3000\n-    # print(\"1\")\n-    # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = 100000, policy_kwargs=policy_kwargs, verbose=1)\n-    model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = n_step, verbose=1)\n-    # print(\"2\")\n-    # model.to(device)\n-    # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n-    # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n-    # eval_callback = EvalCallback(env, best_model_save_path=\"./logs/\",\n-    #                          log_path=\"./logs/\", eval_freq=0, n_eval_episodes = 1,\n-    #                          deterministic=False, verbose=10)\n+    # n_step = 1024 #1000\n+    # total_timestep = 1024 * 3000\n+    # # print(\"1\")\n+    # # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = 100000, policy_kwargs=policy_kwargs, verbose=1)\n+    # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = n_step, verbose=1)\n+    # # print(\"2\")\n+    # # model.to(device)\n+    # # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n+    # # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n+    # # eval_callback = EvalCallback(env, best_model_save_path=\"./logs/\",\n+    # #                          log_path=\"./logs/\", eval_freq=0, n_eval_episodes = 1,\n+    # #                          deterministic=False, verbose=10)\n \n-    model.learn(total_timesteps=total_timestep)#, callback=eval_callback)\n-    model.save(\"ppo_padna_mppi_0817\")\n+    # model.learn(total_timesteps=total_timestep)#, callback=eval_callback)\n+    # model.save(\"ppo_padna_mppi_0817\")\n \n     # del model # remove to demonstrate saving and loading\n \n-    # model = PPO.load(\"./ppo_padna_mppi_0810.zip\")\n+    model = PPO.load(\"./ppo_padna_mppi_0810.zip\")\n \n-    # obs = env.reset()\n-    # while True:\n-    #     action, _states = model.predict(obs)\n-    #     obs, rewards, dones, info = env.step(action)\n-    #     env.render()\n+    obs = env.reset()\n+    while True:\n+        action, _states = model.predict(obs)\n+        obs, rewards, dones, info = env.step(action)\n+        env.render()\n     \n     \n     \n if __name__ == \"__main__\":\n"
                },
                {
                    "date": 1692320551730,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -27,9 +27,9 @@\n def main():\n     # Parallel environments\n     log_dir=\"./log\"\n     env = frankaEnv.FrankaEnv()\n-    env = Monitor(env, log_dir)\n+    # env = Monitor(env, log_dir)\n     # check_env(env, warn=True)\n \n     # n_step = 1024 #1000\n     # total_timestep = 1024 * 3000\n"
                },
                {
                    "date": 1692320558421,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -27,9 +27,9 @@\n def main():\n     # Parallel environments\n     log_dir=\"./log\"\n     env = frankaEnv.FrankaEnv()\n-    # env = Monitor(env, log_dir)\n+    env = Monitor(env, log_dir)\n     # check_env(env, warn=True)\n \n     # n_step = 1024 #1000\n     # total_timestep = 1024 * 3000\n"
                },
                {
                    "date": 1692320570393,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -33,9 +33,9 @@\n \n     # n_step = 1024 #1000\n     # total_timestep = 1024 * 3000\n     # # print(\"1\")\n-    # # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = 100000, policy_kwargs=policy_kwargs, verbose=1)\n+    # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = 100000, policy_kwargs=policy_kwargs, verbose=1)\n     # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = n_step, verbose=1)\n     # # print(\"2\")\n     # # model.to(device)\n     # # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
                },
                {
                    "date": 1692320576950,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -30,13 +30,13 @@\n     env = frankaEnv.FrankaEnv()\n     env = Monitor(env, log_dir)\n     # check_env(env, warn=True)\n \n-    # n_step = 1024 #1000\n-    # total_timestep = 1024 * 3000\n+    n_step = 1024 #1000\n+    total_timestep = 1024 * 3000\n     # # print(\"1\")\n-    # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = 100000, policy_kwargs=policy_kwargs, verbose=1)\n-    # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = n_step, verbose=1)\n+    # # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = 100000, policy_kwargs=policy_kwargs, verbose=1)\n+    model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = n_step, verbose=1)\n     # # print(\"2\")\n     # # model.to(device)\n     # # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n     # # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n"
                },
                {
                    "date": 1692320589389,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -30,13 +30,13 @@\n     env = frankaEnv.FrankaEnv()\n     env = Monitor(env, log_dir)\n     # check_env(env, warn=True)\n \n-    n_step = 1024 #1000\n-    total_timestep = 1024 * 3000\n+    # n_step = 1024 #1000\n+    # total_timestep = 1024 * 3000\n     # # print(\"1\")\n     # # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = 100000, policy_kwargs=policy_kwargs, verbose=1)\n-    model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = n_step, verbose=1)\n+    # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = n_step, verbose=1)\n     # # print(\"2\")\n     # # model.to(device)\n     # # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n     # # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n"
                },
                {
                    "date": 1692320607018,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -48,9 +48,9 @@\n     # model.save(\"ppo_padna_mppi_0817\")\n \n     # del model # remove to demonstrate saving and loading\n \n-    model = PPO.load(\"./ppo_padna_mppi_0810.zip\")\n+    model = PPO.load(\"./ppo_padna_mppi_0817.zip\")\n \n     obs = env.reset()\n     while True:\n         action, _states = model.predict(obs)\n"
                },
                {
                    "date": 1692320722643,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -53,9 +53,9 @@\n \n     obs = env.reset()\n     while True:\n         action, _states = model.predict(obs)\n-        obs, rewards, dones, info = env.step(action)\n+        obs, rewards, dones, info ,xx= env.step(action)\n         env.render()\n     \n     \n     \n"
                },
                {
                    "date": 1692320737149,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -54,9 +54,9 @@\n     obs = env.reset()\n     while True:\n         action, _states = model.predict(obs)\n         obs, rewards, dones, info ,xx= env.step(action)\n-        env.render()\n+        # env.render()\n     \n     \n     \n if __name__ == \"__main__\":\n"
                },
                {
                    "date": 1692321422273,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -48,14 +48,16 @@\n     # model.save(\"ppo_padna_mppi_0817\")\n \n     # del model # remove to demonstrate saving and loading\n \n+    #####################################################################\n     model = PPO.load(\"./ppo_padna_mppi_0817.zip\")\n \n-    obs = env.reset()\n+    obs = env.reset() # reset 함수에서 return값을 return self.get_observation()#, {}로 바꿔주기\n     while True:\n         action, _states = model.predict(obs)\n         obs, rewards, dones, info ,xx= env.step(action)\n+    #####################################################################\n         # env.render()\n     \n     \n     \n"
                },
                {
                    "date": 1692321438874,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -49,14 +49,14 @@\n \n     # del model # remove to demonstrate saving and loading\n \n     #####################################################################\n-    model = PPO.load(\"./ppo_padna_mppi_0817.zip\")\n+    # model = PPO.load(\"./ppo_padna_mppi_0817.zip\")\n \n-    obs = env.reset() # reset 함수에서 return값을 return self.get_observation()#, {}로 바꿔주기\n-    while True:\n-        action, _states = model.predict(obs)\n-        obs, rewards, dones, info ,xx= env.step(action)\n+    # obs = env.reset() # reset 함수에서 return값을 return self.get_observation()#, {}로 바꿔주기\n+    # while True:\n+    #     action, _states = model.predict(obs)\n+    #     obs, rewards, dones, info ,xx= env.step(action)\n     #####################################################################\n         # env.render()\n     \n     \n"
                },
                {
                    "date": 1692321791615,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -30,24 +30,28 @@\n     env = frankaEnv.FrankaEnv()\n     env = Monitor(env, log_dir)\n     # check_env(env, warn=True)\n \n-    # n_step = 1024 #1000\n-    # total_timestep = 1024 * 3000\n-    # # print(\"1\")\n-    # # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = 100000, policy_kwargs=policy_kwargs, verbose=1)\n-    # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = n_step, verbose=1)\n-    # # print(\"2\")\n-    # # model.to(device)\n-    # # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n-    # # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n-    # # eval_callback = EvalCallback(env, best_model_save_path=\"./logs/\",\n-    # #                          log_path=\"./logs/\", eval_freq=0, n_eval_episodes = 1,\n-    # #                          deterministic=False, verbose=10)\n \n-    # model.learn(total_timesteps=total_timestep)#, callback=eval_callback)\n-    # model.save(\"ppo_padna_mppi_0817\")\n+    ######################################### 학습 #############################\n+    n_step = 1024 #1000\n+    total_timestep = 1024 * 3000\n+    # print(\"1\")\n+    # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = 100000, policy_kwargs=policy_kwargs, verbose=1)\n+    model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = n_step, verbose=1)\n+    # print(\"2\")\n+    # model.to(device)\n+    # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n+    # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n+    # eval_callback = EvalCallback(env, best_model_save_path=\"./logs/\",\n+    #                          log_path=\"./logs/\", eval_freq=0, n_eval_episodes = 1,\n+    #                          deterministic=False, verbose=10)\n \n+    model.learn(total_timesteps=total_timestep)#, callback=eval_callback)\n+    model.save(\"ppo_padna_mppi_0817\")\n+\n+    #############################################################################\n+\n     # del model # remove to demonstrate saving and loading\n \n     #####################################################################\n     # model = PPO.load(\"./ppo_padna_mppi_0817.zip\")\n"
                },
                {
                    "date": 1692321797437,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -52,9 +52,9 @@\n     #############################################################################\n \n     # del model # remove to demonstrate saving and loading\n \n-    #####################################################################\n+    ####################################### 확인 #############################\n     # model = PPO.load(\"./ppo_padna_mppi_0817.zip\")\n \n     # obs = env.reset() # reset 함수에서 return값을 return self.get_observation()#, {}로 바꿔주기\n     # while True:\n"
                },
                {
                    "date": 1692322194314,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -46,10 +46,9 @@\n     #                          log_path=\"./logs/\", eval_freq=0, n_eval_episodes = 1,\n     #                          deterministic=False, verbose=10)\n \n     model.learn(total_timesteps=total_timestep)#, callback=eval_callback)\n-    model.save(\"ppo_padna_mppi_0817\")\n-\n+    model.save(\"ppo_padna_mppi_0887\")\n     #############################################################################\n \n     # del model # remove to demonstrate saving and loading\n \n"
                },
                {
                    "date": 1692334363124,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -10,8 +10,9 @@\n # from stable_baselines3.common.callbacks import EvalCallback\n from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n import frankaEnv\n from gym import Wrapper, spaces\n+from sb3_contrib import TQC\n \n # device = th.device('cuda:0')\n # device = th.device('cuda:0')\n # th.cuda.is_available()\n@@ -21,10 +22,11 @@\n \n device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n print(device)\n     \n-policy_kwargs = dict(activation_fn=th.nn.Tanh,net_arch=[128, dict(pi=[64,64], vf=[64,64])])\n+# policy_kwargs = dict(activation_fn=th.nn.Tanh,net_arch=[128, dict(pi=[64,64], vf=[64,64])])\n # policy_kwargs = dict(activation_fn=th.nn.Tanh,net_arch=[128, 128])\n+policy_kwargs = dict(n_critics=2, n_quantiles=25)\n def main():\n     # Parallel environments\n     log_dir=\"./log\"\n     env = frankaEnv.FrankaEnv()\n@@ -36,9 +38,10 @@\n     n_step = 1024 #1000\n     total_timestep = 1024 * 3000\n     # print(\"1\")\n     # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = 100000, policy_kwargs=policy_kwargs, verbose=1)\n-    model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = n_step, verbose=1)\n+    # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = n_step, verbose=1)\n+    model = TQC(\"MlpPolicy\", env, train_freq=(1,\"episode\") ,top_quantiles_to_drop_per_net=2, verbose=1, policy_kwargs=policy_kwargs)\n     # print(\"2\")\n     # model.to(device)\n     # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n     # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n@@ -46,9 +49,9 @@\n     #                          log_path=\"./logs/\", eval_freq=0, n_eval_episodes = 1,\n     #                          deterministic=False, verbose=10)\n \n     model.learn(total_timesteps=total_timestep)#, callback=eval_callback)\n-    model.save(\"ppo_padna_mppi_0887\")\n+    model.save(\"ppo_padna_mppi_0818\")\n     #############################################################################\n \n     # del model # remove to demonstrate saving and loading\n \n"
                },
                {
                    "date": 1692334384949,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -34,10 +34,10 @@\n     # check_env(env, warn=True)\n \n \n     ######################################### 학습 #############################\n-    n_step = 1024 #1000\n-    total_timestep = 1024 * 3000\n+    # n_step = 1024 #1000\n+    # total_timestep = 1024 * 3000\n     # print(\"1\")\n     # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = 100000, policy_kwargs=policy_kwargs, verbose=1)\n     # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = n_step, verbose=1)\n     model = TQC(\"MlpPolicy\", env, train_freq=(1,\"episode\") ,top_quantiles_to_drop_per_net=2, verbose=1, policy_kwargs=policy_kwargs)\n@@ -48,9 +48,11 @@\n     # eval_callback = EvalCallback(env, best_model_save_path=\"./logs/\",\n     #                          log_path=\"./logs/\", eval_freq=0, n_eval_episodes = 1,\n     #                          deterministic=False, verbose=10)\n \n-    model.learn(total_timesteps=total_timestep)#, callback=eval_callback)\n+    # model.learn(total_timesteps=total_timestep)#, callback=eval_callback)\n+\n+    model.learn(total_timesteps=10_000, log_interval=4)\n     model.save(\"ppo_padna_mppi_0818\")\n     #############################################################################\n \n     # del model # remove to demonstrate saving and loading\n"
                },
                {
                    "date": 1692334421127,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -51,9 +51,9 @@\n \n     # model.learn(total_timesteps=total_timestep)#, callback=eval_callback)\n \n     model.learn(total_timesteps=10_000, log_interval=4)\n-    model.save(\"ppo_padna_mppi_0818\")\n+    model.save(\"ppo_padna_mppi_0818_TQC\")\n     #############################################################################\n \n     # del model # remove to demonstrate saving and loading\n \n@@ -61,10 +61,10 @@\n     # model = PPO.load(\"./ppo_padna_mppi_0817.zip\")\n \n     # obs = env.reset() # reset 함수에서 return값을 return self.get_observation()#, {}로 바꿔주기\n     # while True:\n-    #     action, _states = model.predict(obs)\n-    #     obs, rewards, dones, info ,xx= env.step(action)\n+    #     action, _states = model.predict(obs, deterministic=True)\n+    #     obs, rewards, terminated, truncated, info= env.step(action)\n     #####################################################################\n         # env.render()\n     \n     \n"
                },
                {
                    "date": 1692334434946,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -29,9 +29,9 @@\n def main():\n     # Parallel environments\n     log_dir=\"./log\"\n     env = frankaEnv.FrankaEnv()\n-    env = Monitor(env, log_dir)\n+    # env = Monitor(env, log_dir)\n     # check_env(env, warn=True)\n \n \n     ######################################### 학습 #############################\n"
                },
                {
                    "date": 1692335121308,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -44,15 +44,15 @@\n     # print(\"2\")\n     # model.to(device)\n     # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n     # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n-    # eval_callback = EvalCallback(env, best_model_save_path=\"./logs/\",\n-    #                          log_path=\"./logs/\", eval_freq=0, n_eval_episodes = 1,\n-    #                          deterministic=False, verbose=10)\n+    eval_callback = EvalCallback(env, best_model_save_path=\"./logs/\",\n+                             log_path=\"./logs/\", eval_freq=0, n_eval_episodes = 1,\n+                             deterministic=False, verbose=10)\n \n     # model.learn(total_timesteps=total_timestep)#, callback=eval_callback)\n \n-    model.learn(total_timesteps=10_000, log_interval=4)\n+    model.learn(total_timesteps=10_000, log_interval=4, callback=eval_callback)\n     model.save(\"ppo_padna_mppi_0818_TQC\")\n     #############################################################################\n \n     # del model # remove to demonstrate saving and loading\n"
                },
                {
                    "date": 1692335130173,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -45,9 +45,9 @@\n     # model.to(device)\n     # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n     # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n     eval_callback = EvalCallback(env, best_model_save_path=\"./logs/\",\n-                             log_path=\"./logs/\", eval_freq=0, n_eval_episodes = 1,\n+                             log_path=\"./logs/\", eval_freq=1, n_eval_episodes = 1,\n                              deterministic=False, verbose=10)\n \n     # model.learn(total_timesteps=total_timestep)#, callback=eval_callback)\n \n"
                },
                {
                    "date": 1692335141732,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -29,9 +29,9 @@\n def main():\n     # Parallel environments\n     log_dir=\"./log\"\n     env = frankaEnv.FrankaEnv()\n-    # env = Monitor(env, log_dir)\n+    env = Monitor(env, log_dir)\n     # check_env(env, warn=True)\n \n \n     ######################################### 학습 #############################\n"
                },
                {
                    "date": 1692335252582,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -50,9 +50,9 @@\n                              deterministic=False, verbose=10)\n \n     # model.learn(total_timesteps=total_timestep)#, callback=eval_callback)\n \n-    model.learn(total_timesteps=10_000, log_interval=4, callback=eval_callback)\n+    model.learn(total_timesteps=10_000, callback=eval_callback)\n     model.save(\"ppo_padna_mppi_0818_TQC\")\n     #############################################################################\n \n     # del model # remove to demonstrate saving and loading\n"
                },
                {
                    "date": 1692335286382,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -29,9 +29,9 @@\n def main():\n     # Parallel environments\n     log_dir=\"./log\"\n     env = frankaEnv.FrankaEnv()\n-    env = Monitor(env, log_dir)\n+    # env = Monitor(env, log_dir)\n     # check_env(env, warn=True)\n \n \n     ######################################### 학습 #############################\n"
                },
                {
                    "date": 1692335337391,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -44,15 +44,15 @@\n     # print(\"2\")\n     # model.to(device)\n     # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n     # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n-    eval_callback = EvalCallback(env, best_model_save_path=\"./logs/\",\n-                             log_path=\"./logs/\", eval_freq=1, n_eval_episodes = 1,\n-                             deterministic=False, verbose=10)\n+    # eval_callback = EvalCallback(env, best_model_save_path=\"./logs/\",\n+    #                          log_path=\"./logs/\", eval_freq=1, n_eval_episodes = 1,\n+    #                          deterministic=False, verbose=10)\n \n     # model.learn(total_timesteps=total_timestep)#, callback=eval_callback)\n \n-    model.learn(total_timesteps=10_000, callback=eval_callback)\n+    model.learn(total_timesteps=10_000)#, callback=eval_callback)\n     model.save(\"ppo_padna_mppi_0818_TQC\")\n     #############################################################################\n \n     # del model # remove to demonstrate saving and loading\n"
                },
                {
                    "date": 1692335427373,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -39,9 +39,9 @@\n     # total_timestep = 1024 * 3000\n     # print(\"1\")\n     # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = 100000, policy_kwargs=policy_kwargs, verbose=1)\n     # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = n_step, verbose=1)\n-    model = TQC(\"MlpPolicy\", env, train_freq=(1,\"episode\") ,top_quantiles_to_drop_per_net=2, verbose=1, policy_kwargs=policy_kwargs)\n+    model = TQC(\"MlpPolicy\", tensorboard_log=True, env, train_freq=(1,\"episode\") ,top_quantiles_to_drop_per_net=2, verbose=1, policy_kwargs=policy_kwargs)\n     # print(\"2\")\n     # model.to(device)\n     # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n     # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n"
                },
                {
                    "date": 1692335444873,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -39,9 +39,9 @@\n     # total_timestep = 1024 * 3000\n     # print(\"1\")\n     # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = 100000, policy_kwargs=policy_kwargs, verbose=1)\n     # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = n_step, verbose=1)\n-    model = TQC(\"MlpPolicy\", tensorboard_log=True, env, train_freq=(1,\"episode\") ,top_quantiles_to_drop_per_net=2, verbose=1, policy_kwargs=policy_kwargs)\n+    model = TQC(\"MlpPolicy\", env, train_freq=(1,\"episode\") ,top_quantiles_to_drop_per_net=2, verbose=1, policy_kwargs=policy_kwargs, tensorboard_log=True)\n     # print(\"2\")\n     # model.to(device)\n     # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n     # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n"
                },
                {
                    "date": 1692335454229,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -39,9 +39,9 @@\n     # total_timestep = 1024 * 3000\n     # print(\"1\")\n     # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = 100000, policy_kwargs=policy_kwargs, verbose=1)\n     # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = n_step, verbose=1)\n-    model = TQC(\"MlpPolicy\", env, train_freq=(1,\"episode\") ,top_quantiles_to_drop_per_net=2, verbose=1, policy_kwargs=policy_kwargs, tensorboard_log=True)\n+    model = TQC(\"MlpPolicy\", env, train_freq=(1,\"episode\") ,top_quantiles_to_drop_per_net=2, verbose=1, policy_kwargs=policy_kwargs, tensorboard_log=log_dir)\n     # print(\"2\")\n     # model.to(device)\n     # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n     # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n"
                },
                {
                    "date": 1692335490683,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -39,9 +39,9 @@\n     # total_timestep = 1024 * 3000\n     # print(\"1\")\n     # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = 100000, policy_kwargs=policy_kwargs, verbose=1)\n     # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = n_step, verbose=1)\n-    model = TQC(\"MlpPolicy\", env, train_freq=(1,\"episode\") ,top_quantiles_to_drop_per_net=2, verbose=1, policy_kwargs=policy_kwargs, tensorboard_log=log_dir)\n+    model = TQC(\"MlpPolicy\", env, train_freq=(1,\"episode\") ,top_quantiles_to_drop_per_net=2, verbose=1, policy_kwargs=policy_kwargs)\n     # print(\"2\")\n     # model.to(device)\n     # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n     # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n"
                },
                {
                    "date": 1692335620642,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -40,8 +40,10 @@\n     # print(\"1\")\n     # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = 100000, policy_kwargs=policy_kwargs, verbose=1)\n     # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = n_step, verbose=1)\n     model = TQC(\"MlpPolicy\", env, train_freq=(1,\"episode\") ,top_quantiles_to_drop_per_net=2, verbose=1, policy_kwargs=policy_kwargs)\n+\n+    model = TQCsm(\"MultiInputPolicy\", env, top_quantiles_to_drop_per_net=2, verbose=1, policy_kwargs=policy_kwargs, save_path='/home/kist-robot2/catkin_ws/src/franka_emika_panda/py_src/saved_model/step_{}', save_interval=1e5, tensorboard_log=\"log/\", learning_starts=1000, gamma=1, target_update_interval=100)\n     # print(\"2\")\n     # model.to(device)\n     # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n     # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n"
                },
                {
                    "date": 1692335713234,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -39,11 +39,11 @@\n     # total_timestep = 1024 * 3000\n     # print(\"1\")\n     # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = 100000, policy_kwargs=policy_kwargs, verbose=1)\n     # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = n_step, verbose=1)\n-    model = TQC(\"MlpPolicy\", env, train_freq=(1,\"episode\") ,top_quantiles_to_drop_per_net=2, verbose=1, policy_kwargs=policy_kwargs)\n+    model = TQC(\"MlpPolicy\", env, train_freq=(1,\"episode\") ,top_quantiles_to_drop_per_net=2, verbose=1, policy_kwargs=policy_kwargs, tensorboard_log=\"log/\")\n \n-    model = TQCsm(\"MultiInputPolicy\", env, top_quantiles_to_drop_per_net=2, verbose=1, policy_kwargs=policy_kwargs, save_path='/home/kist-robot2/catkin_ws/src/franka_emika_panda/py_src/saved_model/step_{}', save_interval=1e5, tensorboard_log=\"log/\", learning_starts=1000, gamma=1, target_update_interval=100)\n+    # model = TQCsm(\"MultiInputPolicy\", env, top_quantiles_to_drop_per_net=2, verbose=1, policy_kwargs=policy_kwargs, save_path='/home/kist-robot2/catkin_ws/src/franka_emika_panda/py_src/saved_model/step_{}', save_interval=1e5, tensorboard_log=\"log/\", learning_starts=1000, gamma=1, target_update_interval=100)\n     # print(\"2\")\n     # model.to(device)\n     # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n     # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n"
                },
                {
                    "date": 1692338090048,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -46,15 +46,15 @@\n     # print(\"2\")\n     # model.to(device)\n     # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n     # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n-    # eval_callback = EvalCallback(env, best_model_save_path=\"./logs/\",\n-    #                          log_path=\"./logs/\", eval_freq=1, n_eval_episodes = 1,\n-    #                          deterministic=False, verbose=10)\n+    eval_callback = EvalCallback(env, best_model_save_path=\"./logs/\",\n+                             log_path=\"./logs/\", eval_freq=1, n_eval_episodes = 1,\n+                             deterministic=False, verbose=10)\n \n     # model.learn(total_timesteps=total_timestep)#, callback=eval_callback)\n \n-    model.learn(total_timesteps=10_000)#, callback=eval_callback)\n+    model.learn(total_timesteps=10_000, callback=eval_callback)\n     model.save(\"ppo_padna_mppi_0818_TQC\")\n     #############################################################################\n \n     # del model # remove to demonstrate saving and loading\n"
                },
                {
                    "date": 1692338142096,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -46,11 +46,11 @@\n     # print(\"2\")\n     # model.to(device)\n     # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n     # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n-    eval_callback = EvalCallback(env, best_model_save_path=\"./logs/\",\n-                             log_path=\"./logs/\", eval_freq=1, n_eval_episodes = 1,\n-                             deterministic=False, verbose=10)\n+    # eval_callback = EvalCallback(env, best_model_save_path=\"./logs/\",\n+    #                          log_path=\"./logs/\", eval_freq=1, n_eval_episodes = 1,\n+    #                          deterministic=False, verbose=10)\n \n     # model.learn(total_timesteps=total_timestep)#, callback=eval_callback)\n \n     model.learn(total_timesteps=10_000, callback=eval_callback)\n"
                },
                {
                    "date": 1692338222731,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -18,8 +18,9 @@\n # th.cuda.is_available()\n # Parallel environments\n # import os\n os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n+os.environ['TF_ENABLE_ONEDNN_OPTS'] = \"0\"\n \n device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n print(device)\n     \n@@ -52,9 +53,9 @@\n     #                          deterministic=False, verbose=10)\n \n     # model.learn(total_timesteps=total_timestep)#, callback=eval_callback)\n \n-    model.learn(total_timesteps=10_000, callback=eval_callback)\n+    model.learn(total_timesteps=10_000)#, callback=eval_callback)\n     model.save(\"ppo_padna_mppi_0818_TQC\")\n     #############################################################################\n \n     # del model # remove to demonstrate saving and loading\n"
                },
                {
                    "date": 1692338389679,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -40,9 +40,9 @@\n     # total_timestep = 1024 * 3000\n     # print(\"1\")\n     # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = 100000, policy_kwargs=policy_kwargs, verbose=1)\n     # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = n_step, verbose=1)\n-    model = TQC(\"MlpPolicy\", env, train_freq=(1,\"episode\") ,top_quantiles_to_drop_per_net=2, verbose=1, policy_kwargs=policy_kwargs, tensorboard_log=\"log/\")\n+    model = TQC(\"MlpPolicy\", env, log_interval=1, train_freq=(1,\"episode\") ,top_quantiles_to_drop_per_net=2, verbose=1, policy_kwargs=policy_kwargs, tensorboard_log=\"log/\")\n \n     # model = TQCsm(\"MultiInputPolicy\", env, top_quantiles_to_drop_per_net=2, verbose=1, policy_kwargs=policy_kwargs, save_path='/home/kist-robot2/catkin_ws/src/franka_emika_panda/py_src/saved_model/step_{}', save_interval=1e5, tensorboard_log=\"log/\", learning_starts=1000, gamma=1, target_update_interval=100)\n     # print(\"2\")\n     # model.to(device)\n"
                },
                {
                    "date": 1692338417254,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -40,9 +40,9 @@\n     # total_timestep = 1024 * 3000\n     # print(\"1\")\n     # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = 100000, policy_kwargs=policy_kwargs, verbose=1)\n     # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = n_step, verbose=1)\n-    model = TQC(\"MlpPolicy\", env, log_interval=1, train_freq=(1,\"episode\") ,top_quantiles_to_drop_per_net=2, verbose=1, policy_kwargs=policy_kwargs, tensorboard_log=\"log/\")\n+    model = TQC(\"MlpPolicy\", verbose=1, env, train_freq=(1,\"episode\") ,top_quantiles_to_drop_per_net=2, verbose=1, policy_kwargs=policy_kwargs, tensorboard_log=\"log/\")\n \n     # model = TQCsm(\"MultiInputPolicy\", env, top_quantiles_to_drop_per_net=2, verbose=1, policy_kwargs=policy_kwargs, save_path='/home/kist-robot2/catkin_ws/src/franka_emika_panda/py_src/saved_model/step_{}', save_interval=1e5, tensorboard_log=\"log/\", learning_starts=1000, gamma=1, target_update_interval=100)\n     # print(\"2\")\n     # model.to(device)\n"
                },
                {
                    "date": 1692338431251,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -40,9 +40,9 @@\n     # total_timestep = 1024 * 3000\n     # print(\"1\")\n     # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = 100000, policy_kwargs=policy_kwargs, verbose=1)\n     # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = n_step, verbose=1)\n-    model = TQC(\"MlpPolicy\", verbose=1, env, train_freq=(1,\"episode\") ,top_quantiles_to_drop_per_net=2, verbose=1, policy_kwargs=policy_kwargs, tensorboard_log=\"log/\")\n+    model = TQC(\"MlpPolicy\", env, verbose=1 ,train_freq=(1,\"episode\") ,top_quantiles_to_drop_per_net=2, verbose=1, policy_kwargs=policy_kwargs, tensorboard_log=\"log/\")\n \n     # model = TQCsm(\"MultiInputPolicy\", env, top_quantiles_to_drop_per_net=2, verbose=1, policy_kwargs=policy_kwargs, save_path='/home/kist-robot2/catkin_ws/src/franka_emika_panda/py_src/saved_model/step_{}', save_interval=1e5, tensorboard_log=\"log/\", learning_starts=1000, gamma=1, target_update_interval=100)\n     # print(\"2\")\n     # model.to(device)\n"
                },
                {
                    "date": 1692338439563,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -40,9 +40,9 @@\n     # total_timestep = 1024 * 3000\n     # print(\"1\")\n     # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = 100000, policy_kwargs=policy_kwargs, verbose=1)\n     # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = n_step, verbose=1)\n-    model = TQC(\"MlpPolicy\", env, verbose=1 ,train_freq=(1,\"episode\") ,top_quantiles_to_drop_per_net=2, verbose=1, policy_kwargs=policy_kwargs, tensorboard_log=\"log/\")\n+    model = TQC(\"MlpPolicy\", env, train_freq=(1,\"episode\") ,top_quantiles_to_drop_per_net=2, verbose=1, policy_kwargs=policy_kwargs, tensorboard_log=\"log/\")\n \n     # model = TQCsm(\"MultiInputPolicy\", env, top_quantiles_to_drop_per_net=2, verbose=1, policy_kwargs=policy_kwargs, save_path='/home/kist-robot2/catkin_ws/src/franka_emika_panda/py_src/saved_model/step_{}', save_interval=1e5, tensorboard_log=\"log/\", learning_starts=1000, gamma=1, target_update_interval=100)\n     # print(\"2\")\n     # model.to(device)\n"
                },
                {
                    "date": 1692338898164,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -35,10 +35,10 @@\n     # check_env(env, warn=True)\n \n \n     ######################################### 학습 #############################\n-    # n_step = 1024 #1000\n-    # total_timestep = 1024 * 3000\n+    n_step = 3333 #1000\n+    total_timestep = 3333 * 1000\n     # print(\"1\")\n     # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = 100000, policy_kwargs=policy_kwargs, verbose=1)\n     # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = n_step, verbose=1)\n     model = TQC(\"MlpPolicy\", env, train_freq=(1,\"episode\") ,top_quantiles_to_drop_per_net=2, verbose=1, policy_kwargs=policy_kwargs, tensorboard_log=\"log/\")\n"
                },
                {
                    "date": 1692578707578,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -35,39 +35,39 @@\n     # check_env(env, warn=True)\n \n \n     ######################################### 학습 #############################\n-    n_step = 3333 #1000\n-    total_timestep = 3333 * 1000\n-    # print(\"1\")\n-    # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = 100000, policy_kwargs=policy_kwargs, verbose=1)\n-    # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = n_step, verbose=1)\n-    model = TQC(\"MlpPolicy\", env, train_freq=(1,\"episode\") ,top_quantiles_to_drop_per_net=2, verbose=1, policy_kwargs=policy_kwargs, tensorboard_log=\"log/\")\n+    # n_step = 3333 #1000\n+    # total_timestep = 3333 * 1000\n+    # # print(\"1\")\n+    # # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = 100000, policy_kwargs=policy_kwargs, verbose=1)\n+    # # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = n_step, verbose=1)\n+    # model = TQC(\"MlpPolicy\", env, train_freq=(1,\"episode\") ,top_quantiles_to_drop_per_net=2, verbose=1, policy_kwargs=policy_kwargs, tensorboard_log=\"log/\")\n \n-    # model = TQCsm(\"MultiInputPolicy\", env, top_quantiles_to_drop_per_net=2, verbose=1, policy_kwargs=policy_kwargs, save_path='/home/kist-robot2/catkin_ws/src/franka_emika_panda/py_src/saved_model/step_{}', save_interval=1e5, tensorboard_log=\"log/\", learning_starts=1000, gamma=1, target_update_interval=100)\n-    # print(\"2\")\n-    # model.to(device)\n-    # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n-    # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n-    # eval_callback = EvalCallback(env, best_model_save_path=\"./logs/\",\n-    #                          log_path=\"./logs/\", eval_freq=1, n_eval_episodes = 1,\n-    #                          deterministic=False, verbose=10)\n+    # # model = TQCsm(\"MultiInputPolicy\", env, top_quantiles_to_drop_per_net=2, verbose=1, policy_kwargs=policy_kwargs, save_path='/home/kist-robot2/catkin_ws/src/franka_emika_panda/py_src/saved_model/step_{}', save_interval=1e5, tensorboard_log=\"log/\", learning_starts=1000, gamma=1, target_update_interval=100)\n+    # # print(\"2\")\n+    # # model.to(device)\n+    # # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n+    # # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n+    # # eval_callback = EvalCallback(env, best_model_save_path=\"./logs/\",\n+    # #                          log_path=\"./logs/\", eval_freq=1, n_eval_episodes = 1,\n+    # #                          deterministic=False, verbose=10)\n \n+    # # model.learn(total_timesteps=total_timestep)#, callback=eval_callback)\n+\n     # model.learn(total_timesteps=total_timestep)#, callback=eval_callback)\n-\n-    model.learn(total_timesteps=10_000)#, callback=eval_callback)\n-    model.save(\"ppo_padna_mppi_0818_TQC\")\n+    # model.save(\"ppo_padna_mppi_0818_TQC\")\n     #############################################################################\n \n     # del model # remove to demonstrate saving and loading\n \n     ####################################### 확인 #############################\n-    # model = PPO.load(\"./ppo_padna_mppi_0817.zip\")\n+    model = PPO.load(\"./ppo_padna_mppi_0817.zip\")\n \n-    # obs = env.reset() # reset 함수에서 return값을 return self.get_observation()#, {}로 바꿔주기\n-    # while True:\n-    #     action, _states = model.predict(obs, deterministic=True)\n-    #     obs, rewards, terminated, truncated, info= env.step(action)\n+    obs = env.reset() # reset 함수에서 return값을 return self.get_observation()#, {}로 바꿔주기\n+    while True:\n+        action, _states = model.predict(obs, deterministic=True)\n+        obs, rewards, terminated, truncated, info= env.step(action)\n     #####################################################################\n         # env.render()\n     \n     \n"
                },
                {
                    "date": 1692579380332,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -64,9 +64,9 @@\n     model = PPO.load(\"./ppo_padna_mppi_0817.zip\")\n \n     obs = env.reset() # reset 함수에서 return값을 return self.get_observation()#, {}로 바꿔주기\n     while True:\n-        action, _states = model.predict(obs, deterministic=True)\n+        action, _states = model.predict(obs)#, deterministic=True)\n         obs, rewards, terminated, truncated, info= env.step(action)\n     #####################################################################\n         # env.render()\n     \n"
                },
                {
                    "date": 1692579776171,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -64,9 +64,9 @@\n     model = PPO.load(\"./ppo_padna_mppi_0817.zip\")\n \n     obs = env.reset() # reset 함수에서 return값을 return self.get_observation()#, {}로 바꿔주기\n     while True:\n-        action, _states = model.predict(obs)#, deterministic=True)\n+        action, _states = model.predict(obs, deterministic=True)\n         obs, rewards, terminated, truncated, info= env.step(action)\n     #####################################################################\n         # env.render()\n     \n"
                },
                {
                    "date": 1692580378655,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -60,14 +60,14 @@\n \n     # del model # remove to demonstrate saving and loading\n \n     ####################################### 확인 #############################\n-    model = PPO.load(\"./ppo_padna_mppi_0817.zip\")\n+    # model = PPO.load(\"./ppo_padna_mppi_0817.zip\")\n \n-    obs = env.reset() # reset 함수에서 return값을 return self.get_observation()#, {}로 바꿔주기\n-    while True:\n-        action, _states = model.predict(obs, deterministic=True)\n-        obs, rewards, terminated, truncated, info= env.step(action)\n+    # obs = env.reset() # reset 함수에서 return값을 return self.get_observation()#, {}로 바꿔주기\n+    # while True:\n+    #     action, _states = model.predict(obs, deterministic=True)\n+    #     obs, rewards, terminated, truncated, info= env.step(action)\n     #####################################################################\n         # env.render()\n     \n     \n"
                },
                {
                    "date": 1692580384245,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -60,14 +60,14 @@\n \n     # del model # remove to demonstrate saving and loading\n \n     ####################################### 확인 #############################\n-    # model = PPO.load(\"./ppo_padna_mppi_0817.zip\")\n+    model = PPO.load(\"./ppo_padna_mppi_0818_TQC.zip\")\n \n-    # obs = env.reset() # reset 함수에서 return값을 return self.get_observation()#, {}로 바꿔주기\n-    # while True:\n-    #     action, _states = model.predict(obs, deterministic=True)\n-    #     obs, rewards, terminated, truncated, info= env.step(action)\n+    obs = env.reset() # reset 함수에서 return값을 return self.get_observation()#, {}로 바꿔주기\n+    while True:\n+        action, _states = model.predict(obs, deterministic=True)\n+        obs, rewards, terminated, truncated, info= env.step(action)\n     #####################################################################\n         # env.render()\n     \n     \n"
                },
                {
                    "date": 1692580429934,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -35,28 +35,28 @@\n     # check_env(env, warn=True)\n \n \n     ######################################### 학습 #############################\n-    # n_step = 3333 #1000\n-    # total_timestep = 3333 * 1000\n-    # # print(\"1\")\n-    # # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = 100000, policy_kwargs=policy_kwargs, verbose=1)\n-    # # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = n_step, verbose=1)\n-    # model = TQC(\"MlpPolicy\", env, train_freq=(1,\"episode\") ,top_quantiles_to_drop_per_net=2, verbose=1, policy_kwargs=policy_kwargs, tensorboard_log=\"log/\")\n+    n_step = 3333 #1000\n+    total_timestep = 3333 * 1000\n+    # print(\"1\")\n+    # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = 100000, policy_kwargs=policy_kwargs, verbose=1)\n+    # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = n_step, verbose=1)\n+    model = TQC(\"MlpPolicy\", env, train_freq=(1,\"episode\") ,top_quantiles_to_drop_per_net=2, verbose=1, policy_kwargs=policy_kwargs, tensorboard_log=\"log/\")\n \n-    # # model = TQCsm(\"MultiInputPolicy\", env, top_quantiles_to_drop_per_net=2, verbose=1, policy_kwargs=policy_kwargs, save_path='/home/kist-robot2/catkin_ws/src/franka_emika_panda/py_src/saved_model/step_{}', save_interval=1e5, tensorboard_log=\"log/\", learning_starts=1000, gamma=1, target_update_interval=100)\n-    # # print(\"2\")\n-    # # model.to(device)\n-    # # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n-    # # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n-    # # eval_callback = EvalCallback(env, best_model_save_path=\"./logs/\",\n-    # #                          log_path=\"./logs/\", eval_freq=1, n_eval_episodes = 1,\n-    # #                          deterministic=False, verbose=10)\n+    # model = TQCsm(\"MultiInputPolicy\", env, top_quantiles_to_drop_per_net=2, verbose=1, policy_kwargs=policy_kwargs, save_path='/home/kist-robot2/catkin_ws/src/franka_emika_panda/py_src/saved_model/step_{}', save_interval=1e5, tensorboard_log=\"log/\", learning_starts=1000, gamma=1, target_update_interval=100)\n+    # print(\"2\")\n+    # model.to(device)\n+    # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n+    # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n+    # eval_callback = EvalCallback(env, best_model_save_path=\"./logs/\",\n+    #                          log_path=\"./logs/\", eval_freq=1, n_eval_episodes = 1,\n+    #                          deterministic=False, verbose=10)\n \n-    # # model.learn(total_timesteps=total_timestep)#, callback=eval_callback)\n+    # model.learn(total_timesteps=total_timestep)#, callback=eval_callback)\n \n-    # model.learn(total_timesteps=total_timestep)#, callback=eval_callback)\n-    # model.save(\"ppo_padna_mppi_0818_TQC\")\n+    model.learn(total_timesteps=total_timestep)#, callback=eval_callback)\n+    model.save(\"tqc_padna_mppi_0821\")\n     #############################################################################\n \n     # del model # remove to demonstrate saving and loading\n \n"
                },
                {
                    "date": 1692580435116,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -35,28 +35,28 @@\n     # check_env(env, warn=True)\n \n \n     ######################################### 학습 #############################\n-    n_step = 3333 #1000\n-    total_timestep = 3333 * 1000\n-    # print(\"1\")\n-    # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = 100000, policy_kwargs=policy_kwargs, verbose=1)\n-    # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = n_step, verbose=1)\n-    model = TQC(\"MlpPolicy\", env, train_freq=(1,\"episode\") ,top_quantiles_to_drop_per_net=2, verbose=1, policy_kwargs=policy_kwargs, tensorboard_log=\"log/\")\n+    # n_step = 3333 #1000\n+    # total_timestep = 3333 * 1000\n+    # # print(\"1\")\n+    # # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = 100000, policy_kwargs=policy_kwargs, verbose=1)\n+    # # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = n_step, verbose=1)\n+    # model = TQC(\"MlpPolicy\", env, train_freq=(1,\"episode\") ,top_quantiles_to_drop_per_net=2, verbose=1, policy_kwargs=policy_kwargs, tensorboard_log=\"log/\")\n \n-    # model = TQCsm(\"MultiInputPolicy\", env, top_quantiles_to_drop_per_net=2, verbose=1, policy_kwargs=policy_kwargs, save_path='/home/kist-robot2/catkin_ws/src/franka_emika_panda/py_src/saved_model/step_{}', save_interval=1e5, tensorboard_log=\"log/\", learning_starts=1000, gamma=1, target_update_interval=100)\n-    # print(\"2\")\n-    # model.to(device)\n-    # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n-    # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n-    # eval_callback = EvalCallback(env, best_model_save_path=\"./logs/\",\n-    #                          log_path=\"./logs/\", eval_freq=1, n_eval_episodes = 1,\n-    #                          deterministic=False, verbose=10)\n+    # # model = TQCsm(\"MultiInputPolicy\", env, top_quantiles_to_drop_per_net=2, verbose=1, policy_kwargs=policy_kwargs, save_path='/home/kist-robot2/catkin_ws/src/franka_emika_panda/py_src/saved_model/step_{}', save_interval=1e5, tensorboard_log=\"log/\", learning_starts=1000, gamma=1, target_update_interval=100)\n+    # # print(\"2\")\n+    # # model.to(device)\n+    # # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n+    # # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n+    # # eval_callback = EvalCallback(env, best_model_save_path=\"./logs/\",\n+    # #                          log_path=\"./logs/\", eval_freq=1, n_eval_episodes = 1,\n+    # #                          deterministic=False, verbose=10)\n \n+    # # model.learn(total_timesteps=total_timestep)#, callback=eval_callback)\n+\n     # model.learn(total_timesteps=total_timestep)#, callback=eval_callback)\n-\n-    model.learn(total_timesteps=total_timestep)#, callback=eval_callback)\n-    model.save(\"tqc_padna_mppi_0821\")\n+    # model.save(\"tqc_padna_mppi_0821\")\n     #############################################################################\n \n     # del model # remove to demonstrate saving and loading\n \n"
                },
                {
                    "date": 1692580447513,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -60,9 +60,9 @@\n \n     # del model # remove to demonstrate saving and loading\n \n     ####################################### 확인 #############################\n-    model = PPO.load(\"./ppo_padna_mppi_0818_TQC.zip\")\n+    model = TQC.load(\"./ppo_padna_mppi_0818_TQC.zip\")\n \n     obs = env.reset() # reset 함수에서 return값을 return self.get_observation()#, {}로 바꿔주기\n     while True:\n         action, _states = model.predict(obs, deterministic=True)\n"
                },
                {
                    "date": 1692580957017,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -35,39 +35,39 @@\n     # check_env(env, warn=True)\n \n \n     ######################################### 학습 #############################\n-    # n_step = 3333 #1000\n-    # total_timestep = 3333 * 1000\n-    # # print(\"1\")\n-    # # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = 100000, policy_kwargs=policy_kwargs, verbose=1)\n-    # # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = n_step, verbose=1)\n-    # model = TQC(\"MlpPolicy\", env, train_freq=(1,\"episode\") ,top_quantiles_to_drop_per_net=2, verbose=1, policy_kwargs=policy_kwargs, tensorboard_log=\"log/\")\n+    n_step = 3333 #1000\n+    total_timestep = 3333 * 1000\n+    # print(\"1\")\n+    # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = 100000, policy_kwargs=policy_kwargs, verbose=1)\n+    # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = n_step, verbose=1)\n+    model = TQC(\"MlpPolicy\", env, train_freq=(1,\"episode\") ,top_quantiles_to_drop_per_net=2, verbose=1, policy_kwargs=policy_kwargs, tensorboard_log=\"log/\")\n \n-    # # model = TQCsm(\"MultiInputPolicy\", env, top_quantiles_to_drop_per_net=2, verbose=1, policy_kwargs=policy_kwargs, save_path='/home/kist-robot2/catkin_ws/src/franka_emika_panda/py_src/saved_model/step_{}', save_interval=1e5, tensorboard_log=\"log/\", learning_starts=1000, gamma=1, target_update_interval=100)\n-    # # print(\"2\")\n-    # # model.to(device)\n-    # # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n-    # # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n-    # # eval_callback = EvalCallback(env, best_model_save_path=\"./logs/\",\n-    # #                          log_path=\"./logs/\", eval_freq=1, n_eval_episodes = 1,\n-    # #                          deterministic=False, verbose=10)\n+    # model = TQCsm(\"MultiInputPolicy\", env, top_quantiles_to_drop_per_net=2, verbose=1, policy_kwargs=policy_kwargs, save_path='/home/kist-robot2/catkin_ws/src/franka_emika_panda/py_src/saved_model/step_{}', save_interval=1e5, tensorboard_log=\"log/\", learning_starts=1000, gamma=1, target_update_interval=100)\n+    # print(\"2\")\n+    # model.to(device)\n+    # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n+    # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n+    # eval_callback = EvalCallback(env, best_model_save_path=\"./logs/\",\n+    #                          log_path=\"./logs/\", eval_freq=1, n_eval_episodes = 1,\n+    #                          deterministic=False, verbose=10)\n \n-    # # model.learn(total_timesteps=total_timestep)#, callback=eval_callback)\n+    # model.learn(total_timesteps=total_timestep)#, callback=eval_callback)\n \n-    # model.learn(total_timesteps=total_timestep)#, callback=eval_callback)\n-    # model.save(\"tqc_padna_mppi_0821\")\n+    model.learn(total_timesteps=total_timestep)#, callback=eval_callback)\n+    model.save(\"tqc_padna_mppi_0821\")\n     #############################################################################\n \n     # del model # remove to demonstrate saving and loading\n \n     ####################################### 확인 #############################\n-    model = TQC.load(\"./ppo_padna_mppi_0818_TQC.zip\")\n+    # model = TQC.load(\"./ppo_padna_mppi_0818_TQC.zip\")\n \n-    obs = env.reset() # reset 함수에서 return값을 return self.get_observation()#, {}로 바꿔주기\n-    while True:\n-        action, _states = model.predict(obs, deterministic=True)\n-        obs, rewards, terminated, truncated, info= env.step(action)\n+    # obs = env.reset() # reset 함수에서 return값을 return self.get_observation()#, {}로 바꿔주기\n+    # while True:\n+    #     action, _states = model.predict(obs, deterministic=True)\n+    #     obs, rewards, terminated, truncated, info= env.step(action)\n     #####################################################################\n         # env.render()\n     \n     \n"
                },
                {
                    "date": 1692582628295,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -36,9 +36,9 @@\n \n \n     ######################################### 학습 #############################\n     n_step = 3333 #1000\n-    total_timestep = 3333 * 1000\n+    total_timestep = 3333 * 4000\n     # print(\"1\")\n     # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = 100000, policy_kwargs=policy_kwargs, verbose=1)\n     # model = PPO(\"MlpPolicy\", env, n_steps=n_step, batch_size = n_step, verbose=1)\n     model = TQC(\"MlpPolicy\", env, train_freq=(1,\"episode\") ,top_quantiles_to_drop_per_net=2, verbose=1, policy_kwargs=policy_kwargs, tensorboard_log=\"log/\")\n"
                },
                {
                    "date": 1692582654751,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -60,9 +60,9 @@\n \n     # del model # remove to demonstrate saving and loading\n \n     ####################################### 확인 #############################\n-    # model = TQC.load(\"./ppo_padna_mppi_0818_TQC.zip\")\n+    # model = TQC.load(\"./tqc_padna_mppi_0821.zip\")\n \n     # obs = env.reset() # reset 함수에서 return값을 return self.get_observation()#, {}로 바꿔주기\n     # while True:\n     #     action, _states = model.predict(obs, deterministic=True)\n"
                }
            ],
            "date": 1691627759878,
            "name": "Commit-0",
            "content": "#!/usr/bin/env python3\nimport gym\nimport os\nimport torch as th\nimport numpy as np\n\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.env_util import make_vec_env\n# from stable_baselines3.common.callbacks import EvalCallback\nfrom stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\nimport frankaEnv\n# device = th.device('cuda:0')\ndevice = th.device('cuda:0')\n# th.cuda.is_available()\n# Parallel environments\n    \npolicy_kwargs = dict(activation_fn=th.nn.Tanh,net_arch=[128, dict(pi=[64,64], vf=[64,64])])\n# policy_kwargs = dict(activation_fn=th.nn.Tanh,net_arch=[128, 128])\n    \ndef main():\n    # Parallel environments\n    env = frankaEnv.FrankaEnv()\n\n    n_step = 8000 #1000\n    total_timestep = 8000 * 5000\n\n    model = PPO(\"MlpPolicy\", env, n_steps=n_step, policy_kwargs=policy_kwargs, verbose=1)\n    # model.to(device)\n    # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-60000, verbose=0)\n    eval_callback = EvalCallback(env, best_model_save_path=\"./logs/\",\n                             log_path=\"./logs/\", eval_freq=1,\n                             deterministic=False, verbose=8000)\n\n    model.learn(total_timesteps=total_timestep, callback=eval_callback)\n    model.save(\"ppo_padna_mppi_0810\")\n\n    # del model # remove to demonstrate saving and loading\n\n    # model = PPO.load(\"./logs/best_model.zip\")\n\n    # obs = env.reset()\n    # while True:\n    #     action, _states = model.predict(obs)\n    #     obs, rewards, dones, info = env.step(action)\n    #     env.render()\n    \n    \n    \nif __name__ == \"__main__\":\n  main()"
        }
    ]
}